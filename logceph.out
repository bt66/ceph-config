[WARNING]: Could not match supplied host pattern, ignoring: mdss
[WARNING]: Could not match supplied host pattern, ignoring: nfss
[WARNING]: Could not match supplied host pattern, ignoring: rbdmirrors
[WARNING]: Could not match supplied host pattern, ignoring: clients
[WARNING]: Could not match supplied host pattern, ignoring: iscsigws
[WARNING]: Could not match supplied host pattern, ignoring: iscsi-gws
[WARNING]: Could not match supplied host pattern, ignoring: rgwloadbalancers

PLAY [mons,osds,mdss,rgws,nfss,rbdmirrors,clients,mgrs,iscsigws,iscsi-gws,grafana-server,rgwloadbalancers] ***

TASK [check for python] ********************************************************
Saturday 05 June 2021  00:13:44 +0000 (0:00:00.019)       0:00:00.019 ********* 
ok: [node-5] => (item=/usr/bin/python)
ok: [node-3] => (item=/usr/bin/python)
ok: [node-4] => (item=/usr/bin/python)
ok: [node-1] => (item=/usr/bin/python)
ok: [node-2] => (item=/usr/bin/python)
ok: [node-6] => (item=/usr/bin/python)
ok: [node-1] => (item=/usr/bin/python3)
ok: [node-2] => (item=/usr/bin/python3)
ok: [node-6] => (item=/usr/bin/python3)
ok: [node-5] => (item=/usr/bin/python3)
ok: [node-2] => (item=/usr/libexec/platform-python)
ok: [node-1] => (item=/usr/libexec/platform-python)
ok: [node-6] => (item=/usr/libexec/platform-python)
ok: [node-5] => (item=/usr/libexec/platform-python)
ok: [node-3] => (item=/usr/bin/python3)
ok: [node-4] => (item=/usr/bin/python3)
ok: [node-3] => (item=/usr/libexec/platform-python)
ok: [node-4] => (item=/usr/libexec/platform-python)

TASK [check for dnf-3 package manager (RedHat/Fedora/CentOS)] ******************
Saturday 05 June 2021  00:13:45 +0000 (0:00:01.294)       0:00:01.314 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for yum package manager (RedHat/Fedora/CentOS)] ********************
Saturday 05 June 2021  00:13:45 +0000 (0:00:00.047)       0:00:01.361 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for apt package manager (Debian/Ubuntu)] ***************************
Saturday 05 June 2021  00:13:45 +0000 (0:00:00.045)       0:00:01.407 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for zypper package manager (SUSE/OpenSUSE)] ************************
Saturday 05 June 2021  00:13:45 +0000 (0:00:00.046)       0:00:01.453 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for RedHat based OS - dnf] ********************************
Saturday 05 June 2021  00:13:45 +0000 (0:00:00.049)       0:00:01.503 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for debian based OS] **************************************
Saturday 05 June 2021  00:13:46 +0000 (0:00:00.048)       0:00:01.551 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for SUSE/OpenSUSE] ****************************************
Saturday 05 June 2021  00:13:46 +0000 (0:00:00.044)       0:00:01.596 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python-xml for opensuse only if python2 is installed already] ****
Saturday 05 June 2021  00:13:46 +0000 (0:00:00.046)       0:00:01.642 ********* 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.9586957, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.958763, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.9587927, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622776621.1, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820802.4246573, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622776590.044, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 

TASK [gather facts] ************************************************************
Saturday 05 June 2021  00:13:46 +0000 (0:00:00.063)       0:00:01.706 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [gather and delegate facts] ***********************************************
Saturday 05 June 2021  00:13:46 +0000 (0:00:00.070)       0:00:01.777 ********* 
ok: [node-2 -> node-2] => (item=node-2)
ok: [node-2 -> node-3] => (item=node-3)
ok: [node-2 -> node-4] => (item=node-4)
ok: [node-2 -> node-1] => (item=node-1)
ok: [node-2 -> node-5] => (item=node-5)
ok: [node-2 -> node-6] => (item=node-6)

TASK [create filtered clients group] *******************************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:11.930)       0:00:13.707 ********* 

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.014)       0:00:13.722 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.099)       0:00:13.822 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-5]
ok: [node-1]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.164)       0:00:13.987 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.050)       0:00:14.037 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.163)       0:00:14.200 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.055)       0:00:14.255 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.050)       0:00:14.306 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.055)       0:00:14.361 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.058)       0:00:14.419 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.024)       0:00:14.444 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:13:58 +0000 (0:00:00.062)       0:00:14.507 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.023)       0:00:14.531 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.222)       0:00:14.754 ********* 
skipping: [node-2] => (item={'msg': 'non-zero return code', 'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '', 'stderr': "stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory", 'rc': 1, 'start': '2021-06-05 00:13:59.203375', 'end': '2021-06-05 00:13:59.205699', 'delta': '0:00:00.002324', 'changed': False, 'failed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory"], 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.024)       0:00:14.778 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'msg': 'non-zero return code', 'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '', 'stderr': "stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory", 'rc': 1, 'start': '2021-06-05 00:13:59.203375', 'end': '2021-06-05 00:13:59.205699', 'delta': '0:00:00.002324', 'changed': False, 'failed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory"], 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.021)       0:00:14.799 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.019)       0:00:14.818 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.057)       0:00:14.876 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.144)       0:00:15.021 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.054)       0:00:15.075 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.071)       0:00:15.147 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.052)       0:00:15.200 ********* 
skipping: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.018)       0:00:15.219 ********* 
changed: [node-2 -> node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.167)       0:00:15.387 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:13:59 +0000 (0:00:00.052)       0:00:15.439 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:14:01 +0000 (0:00:01.402)       0:00:16.842 ********* 
skipping: [node-2]
skipping: [node-6]
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:00.052981', 'end': '2021-06-05 00:14:00.162511', 'delta': '0:00:00.109530', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:00.297384', 'end': '2021-06-05 00:14:01.300819', 'delta': '0:00:01.003435', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:00.054431', 'end': '2021-06-05 00:14:00.162587', 'delta': '0:00:00.108156', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:00.289429', 'end': '2021-06-05 00:14:00.291674', 'delta': '0:00:00.002245', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.049)       0:00:16.891 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.049)       0:00:16.941 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.052)       0:00:16.994 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.052)       0:00:17.046 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.052)       0:00:17.099 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.051)       0:00:17.150 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.050)       0:00:17.201 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.052)       0:00:17.253 ********* 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.127)       0:00:17.380 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.016)       0:00:17.397 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.015)       0:00:17.413 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:14:01 +0000 (0:00:00.050)       0:00:17.464 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-5]
ok: [node-1]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.167)       0:00:17.632 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.047)       0:00:17.679 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.047)       0:00:17.727 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.052)       0:00:17.779 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.059)       0:00:17.838 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.048)       0:00:17.887 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.056)       0:00:17.943 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.056)       0:00:18.000 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.060)       0:00:18.060 ********* 
ok: [node-2] => (item=node-2)
ok: [node-3] => (item=node-2)
ok: [node-4] => (item=node-2)
ok: [node-6] => (item=node-2)
ok: [node-1] => (item=node-2)
ok: [node-5] => (item=node-2)

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.079)       0:00:18.140 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.056)       0:00:18.196 ********* 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.054)       0:00:18.251 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.059)       0:00:18.310 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.052)       0:00:18.363 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
ok: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.052)       0:00:18.415 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.050)       0:00:18.466 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:14:02 +0000 (0:00:00.050)       0:00:18.517 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.051)       0:00:18.568 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
ok: [node-6] => (item=0)
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.055)       0:00:18.624 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.050)       0:00:18.674 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-6] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.057)       0:00:18.731 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.055)       0:00:18.786 ********* 
skipping: [node-2] => (item=node-6) 
skipping: [node-3] => (item=node-6) 
skipping: [node-4] => (item=node-6) 
skipping: [node-6] => (item=node-6) 
skipping: [node-1] => (item=node-6) 
skipping: [node-5] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.053)       0:00:18.840 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.058)       0:00:18.898 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.027)       0:00:18.925 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-validate : include check_system.yml] ********************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.021)       0:00:18.947 ********* 
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_system.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-validate : fail on unsupported ansible version (1.X)] ***************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.071)       0:00:19.019 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported ansible version] *********************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.049)       0:00:19.069 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported system] ******************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.050)       0:00:19.119 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported architecture] ************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.050)       0:00:19.169 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution] ************************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.052)       0:00:19.221 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported CentOS release] **********************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.050)       0:00:19.272 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution for red hat ceph storage] ***
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.051)       0:00:19.324 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : determine if node is registered with subscription-manager] ***
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.048)       0:00:19.372 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unregistered red hat rhcs linux] *****************
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.053)       0:00:19.425 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution for ubuntu cloud archive] ***
Saturday 05 June 2021  00:14:03 +0000 (0:00:00.052)       0:00:19.478 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported SUSE/openSUSE distribution (only 15.x supported)] ***
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.052)       0:00:19.530 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if systemd is not present] **************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:19.581 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_origin] ************************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.051)       0:00:19.632 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository] ********************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.048)       0:00:19.681 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository_community] **********************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.051)       0:00:19.732 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository_type] ***************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.052)       0:00:19.785 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate osd_objectstore] ********************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.051)       0:00:19.837 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate monitor network configuration] ******************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.054)       0:00:19.892 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate radosgw network configuration] ******************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:19.942 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate lvm osd scenario] *******************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:19.992 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate filestore lvm osd scenario] *********************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.046)       0:00:20.039 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate bluestore lvm osd scenario] *********************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.047)       0:00:20.087 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if local scenario is enabled on debian] *************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.049)       0:00:20.136 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if rhcs repository is enabled on debian] ************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.052)       0:00:20.189 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : Check ceph_origin definition on SUSE/openSUSE Leap] ******
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.052)       0:00:20.241 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : Check ceph_repository definition on SUSE/openSUSE Leap] ***
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.051)       0:00:20.293 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ntp daemon type] ********************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.051)       0:00:20.344 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : abort if ntp_daemon_type is ntpd on Atomic] **************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:20.395 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : make sure journal_size configured] ***********************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:20.445 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_devices.yml] *******************************
Saturday 05 June 2021  00:14:04 +0000 (0:00:00.050)       0:00:20.496 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_devices.yml for node-3, node-4

TASK [ceph-validate : find device used for operating system] *******************
Saturday 05 June 2021  00:14:05 +0000 (0:00:00.128)       0:00:20.624 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-validate : resolve root_device] *************************************
Saturday 05 June 2021  00:14:05 +0000 (0:00:00.145)       0:00:20.770 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-validate : set_fact root_device] ************************************
Saturday 05 June 2021  00:14:06 +0000 (0:00:01.143)       0:00:21.913 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-validate : resolve devices in lvm_volumes] **************************
Saturday 05 June 2021  00:14:06 +0000 (0:00:00.081)       0:00:21.994 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : set_fact lvm_volumes_data_devices] ***********************
Saturday 05 June 2021  00:14:06 +0000 (0:00:00.021)       0:00:22.016 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : resolve devices in devices] ******************************
Saturday 05 June 2021  00:14:06 +0000 (0:00:00.021)       0:00:22.038 ********* 
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-validate : set_fact devices_resolved] *******************************
Saturday 05 June 2021  00:14:07 +0000 (0:00:01.276)       0:00:23.315 ********* 
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:06.641558', 'end': '2021-06-05 00:14:06.643372', 'delta': '0:00:00.001814', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:06.648516', 'end': '2021-06-05 00:14:06.650970', 'delta': '0:00:00.002454', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:06.770946', 'end': '2021-06-05 00:14:07.774150', 'delta': '0:00:01.003204', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:14:06.778973', 'end': '2021-06-05 00:14:06.781212', 'delta': '0:00:00.002239', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})

TASK [ceph-validate : fail if root_device is passed in lvm_volumes or devices] ***
Saturday 05 June 2021  00:14:07 +0000 (0:00:00.040)       0:00:23.355 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : read information about the devices] **********************
Saturday 05 June 2021  00:14:07 +0000 (0:00:00.021)       0:00:23.377 ********* 
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)
ok: [node-4] => (item=/dev/vdc)

TASK [ceph-validate : fail when gpt header found on osd devices] ***************
Saturday 05 June 2021  00:14:10 +0000 (0:00:02.340)       0:00:25.717 ********* 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 

TASK [ceph-validate : get devices information] *********************************
Saturday 05 June 2021  00:14:10 +0000 (0:00:00.030)       0:00:25.748 ********* 
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-validate : fail if one of the devices is not a device] **************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.980)       0:00:26.728 ********* 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 

TASK [ceph-validate : include check_eth_mon.yml] *******************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.028)       0:00:26.757 ********* 
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_eth_mon.yml for node-2

TASK [ceph-validate : fail if ens3 does not exist on node-2] *******************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.087)       0:00:26.844 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 is not active on node-2] ********************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.016)       0:00:26.861 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 does not have any ip v4 address on node-2] ***
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.020)       0:00:26.881 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 does not have any ip v6 address on node-2] ***
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.020)       0:00:26.902 ********* 
skipping: [node-2]

TASK [ceph-validate : include check_ipaddr_mon.yml] ****************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.013)       0:00:26.916 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_eth_rgw.yml] *******************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.070)       0:00:26.987 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rgw_pools.yml] *****************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.050)       0:00:27.037 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rgw_multisite.yml] *************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.050)       0:00:27.087 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_iscsi.yml] *********************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.049)       0:00:27.136 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : warn about radosgw_civetweb_num_threads option deprecation] ***
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.050)       0:00:27.187 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_nfs.yml] ***********************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.049)       0:00:27.236 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rbdmirror.yml] *****************************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.049)       0:00:27.286 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if [grafana-server] group doesn't exist] ************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.049)       0:00:27.336 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail when [grafana-server] doesn't contain at least one node.] ***
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.051)       0:00:27.387 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail when dashboard_admin_password and/or grafana_admin_password are not set] ***
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.050)       0:00:27.438 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate container registry credentials] *****************
Saturday 05 June 2021  00:14:11 +0000 (0:00:00.051)       0:00:27.489 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate openstack_keys key format] **********************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.050)       0:00:27.539 ********* 
skipping: [node-2] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 

TASK [ceph-validate : validate clients keys key format] ************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.093)       0:00:27.632 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate openstack_keys caps] ****************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.052)       0:00:27.685 ********* 
skipping: [node-2] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 

TASK [ceph-validate : validate clients keys caps] ******************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.099)       0:00:27.784 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : check virtual_ips is defined] ****************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.050)       0:00:27.834 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate virtual_ips length] *****************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.048)       0:00:27.883 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : update cache for Debian based OSs] **************************
Saturday 05 June 2021  00:14:12 +0000 (0:00:00.050)       0:00:27.933 ********* 
changed: [node-1]
changed: [node-6]
changed: [node-2]
changed: [node-5]
changed: [node-3]
changed: [node-4]

TASK [ceph-infra : include_tasks configure_firewall.yml] ***********************
Saturday 05 June 2021  00:14:55 +0000 (0:00:43.082)       0:01:11.015 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : include_tasks setup_ntp.yml] ********************************
Saturday 05 June 2021  00:14:55 +0000 (0:00:00.047)       0:01:11.062 ********* 
included: /opt/ceph-ansible/roles/ceph-infra/tasks/setup_ntp.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-infra : set ntp service and chrony daemon name for Debian family] ***
Saturday 05 June 2021  00:14:55 +0000 (0:00:00.069)       0:01:11.132 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-infra : set ntp service and chrony daemon name for RedHat and Suse family] ***
Saturday 05 June 2021  00:14:55 +0000 (0:00:00.053)       0:01:11.185 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : install ntpd] ***********************************************
Saturday 05 June 2021  00:14:55 +0000 (0:00:00.044)       0:01:11.230 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : install chrony] *********************************************
Saturday 05 June 2021  00:14:55 +0000 (0:00:00.044)       0:01:11.274 ********* 
ok: [node-6]
ok: [node-1]
ok: [node-2]
ok: [node-5]
ok: [node-4]
ok: [node-3]

TASK [ceph-infra : enable timesyncing on timesyncd] ****************************
Saturday 05 June 2021  00:15:01 +0000 (0:00:05.732)       0:01:17.007 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : disable time sync using timesyncd if we are not using it] ***
Saturday 05 June 2021  00:15:01 +0000 (0:00:00.047)       0:01:17.055 ********* 
changed: [node-1]
changed: [node-6]
changed: [node-5]
changed: [node-2]
changed: [node-3]
changed: [node-4]

TASK [ceph-infra : enable ntpd] ************************************************
Saturday 05 June 2021  00:15:04 +0000 (0:00:02.809)       0:01:19.864 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : enable chronyd] *********************************************
Saturday 05 June 2021  00:15:04 +0000 (0:00:00.049)       0:01:19.913 ********* 
changed: [node-6]
changed: [node-2]
changed: [node-1]
changed: [node-5]
changed: [node-4]
changed: [node-3]

TASK [ceph-infra : ensure logrotate is installed] ******************************
Saturday 05 June 2021  00:15:06 +0000 (0:00:02.068)       0:01:21.982 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : add logrotate configuration] ********************************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.050)       0:01:22.033 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_redhat.yml] **************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.049)       0:01:22.082 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_suse.yml] ****************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.049)       0:01:22.132 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include installs/install_on_debian.yml] ********************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.050)       0:01:22.182 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/install_on_debian.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : include configure_debian_repository_installation.yml] ******
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.081)       0:01:22.264 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/configure_debian_repository_installation.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : include debian_community_repository.yml] *******************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.079)       0:01:22.343 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/debian_community_repository.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : install dependencies for apt modules] **********************
Saturday 05 June 2021  00:15:06 +0000 (0:00:00.081)       0:01:22.425 ********* 
ok: [node-3]
ok: [node-5]
ok: [node-6]
ok: [node-4]
ok: [node-1]
ok: [node-2]

TASK [ceph-common : configure debian ceph community repository stable key] *****
Saturday 05 June 2021  00:15:09 +0000 (0:00:02.589)       0:01:25.015 ********* 
ok: [node-2]
ok: [node-1]
ok: [node-6]
ok: [node-4]
ok: [node-5]
ok: [node-3]

TASK [ceph-common : configure debian ceph stable community repository] *********
Saturday 05 June 2021  00:15:11 +0000 (0:00:02.205)       0:01:27.220 ********* 
ok: [node-2]
ok: [node-1]
ok: [node-6]
ok: [node-3]
ok: [node-5]
ok: [node-4]

TASK [ceph-common : include debian_dev_repository.yml] *************************
Saturday 05 June 2021  00:15:12 +0000 (0:00:00.815)       0:01:28.035 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include debian_custom_repository.yml] **********************
Saturday 05 June 2021  00:15:12 +0000 (0:00:00.053)       0:01:28.088 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include debian_uca_repository.yml] *************************
Saturday 05 June 2021  00:15:12 +0000 (0:00:00.052)       0:01:28.140 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : update apt cache if cache_valid_time has expired] **********
Saturday 05 June 2021  00:15:12 +0000 (0:00:00.053)       0:01:28.194 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-5]
ok: [node-6]
ok: [node-1]

TASK [ceph-common : install dependencies] **************************************
Saturday 05 June 2021  00:15:13 +0000 (0:00:00.515)       0:01:28.709 ********* 
ok: [node-5]
ok: [node-4]
ok: [node-3]
ok: [node-2]
ok: [node-6]
ok: [node-1]

TASK [ceph-common : include install_debian_packages.yml] ***********************
Saturday 05 June 2021  00:15:13 +0000 (0:00:00.502)       0:01:29.212 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/install_debian_packages.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : install ceph for debian] ***********************************
Saturday 05 June 2021  00:15:13 +0000 (0:00:00.089)       0:01:29.302 ********* 
changed: [node-5]
changed: [node-4]
changed: [node-2]
changed: [node-1]
changed: [node-6]
changed: [node-3]

TASK [ceph-common : include install_debian_rhcs_packages.yml] ******************
Saturday 05 June 2021  00:25:02 +0000 (0:09:48.811)       0:11:18.114 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_clear.yml] ***************
Saturday 05 June 2021  00:25:02 +0000 (0:00:00.054)       0:11:18.169 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : get ceph version] ******************************************
Saturday 05 June 2021  00:25:02 +0000 (0:00:00.053)       0:11:18.222 ********* 
ok: [node-2]
ok: [node-6]
ok: [node-1]
ok: [node-4]
ok: [node-3]
ok: [node-5]

TASK [ceph-common : set_fact ceph_version] *************************************
Saturday 05 June 2021  00:25:03 +0000 (0:00:01.124)       0:11:19.347 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-common : include release-rhcs.yml] **********************************
Saturday 05 June 2021  00:25:03 +0000 (0:00:00.059)       0:11:19.406 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : set_fact ceph_release - override ceph_release with ceph_stable_release] ***
Saturday 05 June 2021  00:25:03 +0000 (0:00:00.049)       0:11:19.455 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-common : include create_rbd_client_dir.yml] *************************
Saturday 05 June 2021  00:25:03 +0000 (0:00:00.051)       0:11:19.507 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/create_rbd_client_dir.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : create rbd client directory] *******************************
Saturday 05 June 2021  00:25:04 +0000 (0:00:00.089)       0:11:19.596 ********* 
ok: [node-5] => (item=/var/run/ceph)
ok: [node-2] => (item=/var/run/ceph)
ok: [node-6] => (item=/var/run/ceph)
ok: [node-3] => (item=/var/run/ceph)
ok: [node-1] => (item=/var/run/ceph)
ok: [node-4] => (item=/var/run/ceph)
changed: [node-5] => (item=/var/log/ceph)
changed: [node-1] => (item=/var/log/ceph)
changed: [node-2] => (item=/var/log/ceph)
changed: [node-6] => (item=/var/log/ceph)
changed: [node-4] => (item=/var/log/ceph)
changed: [node-3] => (item=/var/log/ceph)

TASK [ceph-common : include configure_cluster_name.yml] ************************
Saturday 05 June 2021  00:25:04 +0000 (0:00:00.378)       0:11:19.975 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/configure_cluster_name.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : configure cluster name] ************************************
Saturday 05 June 2021  00:25:04 +0000 (0:00:00.092)       0:11:20.067 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : check /etc/default/ceph exist] *****************************
Saturday 05 June 2021  00:25:04 +0000 (0:00:00.053)       0:11:20.121 ********* 
ok: [node-2]
ok: [node-6]
ok: [node-1]
ok: [node-4]
ok: [node-5]
ok: [node-3]

TASK [ceph-common : when /etc/default/ceph is not dir] *************************
Saturday 05 June 2021  00:25:06 +0000 (0:00:01.728)       0:11:21.850 ********* 
changed: [node-3]
changed: [node-4]
changed: [node-5]
changed: [node-6]
changed: [node-2]
changed: [node-1]

TASK [ceph-common : when /etc/default/ceph is dir] *****************************
Saturday 05 June 2021  00:25:06 +0000 (0:00:00.291)       0:11:22.141 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include configure_memory_allocator.yml] ********************
Saturday 05 June 2021  00:25:06 +0000 (0:00:00.056)       0:11:22.198 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include selinux.yml] ***************************************
Saturday 05 June 2021  00:25:06 +0000 (0:00:00.052)       0:11:22.250 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

RUNNING HANDLER [ceph-infra : disable ntpd] ************************************
Saturday 05 June 2021  00:25:06 +0000 (0:00:00.049)       0:11:22.300 ********* 
ok: [node-6]
ok: [node-5]
ok: [node-1]
ok: [node-2]
ok: [node-4]
ok: [node-3]

RUNNING HANDLER [ceph-infra : disable timesyncd] *******************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.263)       0:11:22.563 ********* 
ok: [node-6]
ok: [node-2]
ok: [node-5]
ok: [node-1]
ok: [node-4]
ok: [node-3]

PLAY [mons] ********************************************************************

TASK [set ceph monitor install 'In Progress'] **********************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.284)       0:11:22.848 ********* 
ok: [node-2]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.020)       0:11:22.868 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.043)       0:11:22.912 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.129)       0:11:23.041 ********* 
ok: [node-2]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.014)       0:11:23.056 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.211)       0:11:23.267 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.018)       0:11:23.286 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.013)       0:11:23.300 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.012)       0:11:23.312 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.019)       0:11:23.332 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.023)       0:11:23.355 ********* 
skipping: [node-2]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.018)       0:11:23.374 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:25:07 +0000 (0:00:00.020)       0:11:23.395 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.147)       0:11:23.542 ********* 
skipping: [node-2] => (item={'msg': 'non-zero return code', 'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '', 'stderr': "stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory", 'rc': 1, 'start': '2021-06-05 00:25:07.990848', 'end': '2021-06-05 00:25:07.993224', 'delta': '0:00:00.002376', 'changed': False, 'failed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory"], 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.021)       0:11:23.563 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'msg': 'non-zero return code', 'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '', 'stderr': "stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory", 'rc': 1, 'start': '2021-06-05 00:25:07.990848', 'end': '2021-06-05 00:25:07.993224', 'delta': '0:00:00.002376', 'changed': False, 'failed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["stat: cannot stat '/var/run/ceph/ceph-mon*.asok': No such file or directory"], 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.019)       0:11:23.582 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.017)       0:11:23.599 ********* 
skipping: [node-2]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.017)       0:11:23.617 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.199)       0:11:23.816 ********* 
skipping: [node-2]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.020)       0:11:23.837 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:23.853 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:23.869 ********* 
skipping: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.018)       0:11:23.887 ********* 
changed: [node-2 -> node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.169)       0:11:24.057 ********* 
ok: [node-2]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.020)       0:11:24.077 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.092 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.108 ********* 
skipping: [node-2]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.014)       0:11:24.123 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.014)       0:11:24.137 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.153 ********* 
skipping: [node-2]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.018)       0:11:24.171 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.186 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.202 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.014)       0:11:24.217 ********* 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.067)       0:11:24.284 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.019)       0:11:24.303 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.319 ********* 
skipping: [node-2]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.015)       0:11:24.334 ********* 
ok: [node-2]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.131)       0:11:24.466 ********* 
ok: [node-2]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.014)       0:11:24.481 ********* 
skipping: [node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.014)       0:11:24.495 ********* 
skipping: [node-2]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:25:08 +0000 (0:00:00.018)       0:11:24.514 ********* 
skipping: [node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.529 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.543 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.019)       0:11:24.562 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.019)       0:11:24.582 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.019)       0:11:24.601 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.018)       0:11:24.620 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.019)       0:11:24.639 ********* 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.021)       0:11:24.661 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:24.677 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.691 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:24.707 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.721 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.736 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:24.752 ********* 
skipping: [node-2] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.018)       0:11:24.770 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.785 ********* 
skipping: [node-2] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.020)       0:11:24.805 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:24.821 ********* 
skipping: [node-2] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.017)       0:11:24.839 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.022)       0:11:24.861 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.026)       0:11:24.888 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.021)       0:11:24.909 ********* 
skipping: [node-2]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:24.924 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-2

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.031)       0:11:24.955 ********* 
ok: [node-2]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.270)       0:11:25.226 ********* 

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.014)       0:11:25.240 ********* 

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.256 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.272 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.288 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.017)       0:11:25.305 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.322 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.017)       0:11:25.339 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.017)       0:11:25.356 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.372 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.388 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.404 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.420 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.436 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.452 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.467 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.016)       0:11:25.484 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.500 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:25:09 +0000 (0:00:00.015)       0:11:25.515 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:25.530 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:25.546 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.562 ********* 
ok: [node-2]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.151)       0:11:25.714 ********* 
ok: [node-2]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.019)       0:11:25.733 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.748 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:25.763 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:25.778 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.793 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.808 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.824 ********* 
ok: [node-2]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.021)       0:11:25.846 ********* 
skipping: [node-2]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.862 ********* 
skipping: [node-2]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:25.878 ********* 
skipping: [node-2]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.017)       0:11:25.895 ********* 
skipping: [node-2]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:25.911 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.017)       0:11:25.928 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.020)       0:11:25.948 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.018)       0:11:25.967 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.018)       0:11:25.985 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.017)       0:11:26.002 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.019 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.036 ********* 
skipping: [node-2]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.017)       0:11:26.053 ********* 
skipping: [node-2]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.018)       0:11:26.072 ********* 
skipping: [node-2]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.088 ********* 
skipping: [node-2]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.017)       0:11:26.106 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.123 ********* 
skipping: [node-2]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.139 ********* 
skipping: [node-2]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.155 ********* 
skipping: [node-2]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.172 ********* 
skipping: [node-2]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:26.186 ********* 
skipping: [node-2]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.202 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.217 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.013)       0:11:26.231 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.019)       0:11:26.250 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.266 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:26.280 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.016)       0:11:26.296 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.014)       0:11:26.311 ********* 
skipping: [node-2]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.015)       0:11:26.326 ********* 
changed: [node-2]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:25:10 +0000 (0:00:00.141)       0:11:26.468 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-2]

TASK [ceph-mon : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.357)       0:11:26.826 ********* 
skipping: [node-2]

TASK [ceph-mon : include deploy_monitors.yml] **********************************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.014)       0:11:26.841 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/deploy_monitors.yml for node-2

TASK [ceph-mon : check if monitor initial keyring already exists] **************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.029)       0:11:26.871 ********* 
skipping: [node-2]

TASK [ceph-mon : generate monitor initial keyring] *****************************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.016)       0:11:26.888 ********* 
changed: [node-2 -> localhost]

TASK [ceph-mon : get initial keyring when it already exists] *******************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.261)       0:11:27.149 ********* 
ok: [node-2]

TASK [ceph-mon : create monitor initial keyring] *******************************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.019)       0:11:27.168 ********* 
changed: [node-2]

TASK [ceph-mon : copy the initial key in /etc/ceph (for containers)] ***********
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.246)       0:11:27.415 ********* 
skipping: [node-2]

TASK [ceph-mon : create monitor directory] *************************************
Saturday 05 June 2021  00:25:11 +0000 (0:00:00.013)       0:11:27.429 ********* 
changed: [node-2]

TASK [ceph-mon : recursively fix ownership of monitor directory] ***************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.140)       0:11:27.569 ********* 
ok: [node-2]

TASK [ceph-mon : create custom admin keyring] **********************************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.143)       0:11:27.713 ********* 
skipping: [node-2]

TASK [ceph-mon : set_fact ceph-authtool container command] *********************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.018)       0:11:27.731 ********* 
ok: [node-2]

TASK [ceph-mon : import admin keyring into mon keyring] ************************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.017)       0:11:27.749 ********* 
skipping: [node-2]

TASK [ceph-mon : set_fact ceph-mon container command] **************************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.015)       0:11:27.764 ********* 
ok: [node-2]

TASK [ceph-mon : ceph monitor mkfs with keyring] *******************************
Saturday 05 June 2021  00:25:12 +0000 (0:00:00.018)       0:11:27.782 ********* 
changed: [node-2]

TASK [ceph-mon : ceph monitor mkfs without keyring] ****************************
Saturday 05 June 2021  00:25:24 +0000 (0:00:11.874)       0:11:39.656 ********* 
skipping: [node-2]

TASK [ceph-mon : include start_monitor.yml] ************************************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.014)       0:11:39.671 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/start_monitor.yml for node-2

TASK [ceph-mon : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.020)       0:11:39.691 ********* 
skipping: [node-2]

TASK [ceph-mon : add ceph-mon systemd service overrides] ***********************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.014)       0:11:39.706 ********* 
skipping: [node-2]

TASK [ceph-mon : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.013)       0:11:39.719 ********* 
skipping: [node-2]

TASK [ceph-mon : start the monitor service] ************************************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.012)       0:11:39.731 ********* 
changed: [node-2]

TASK [ceph-mon : include_tasks ceph_keys.yml] **********************************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.662)       0:11:40.394 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/ceph_keys.yml for node-2

TASK [ceph-mon : waiting for the monitor(s) to form the quorum...] *************
Saturday 05 June 2021  00:25:24 +0000 (0:00:00.026)       0:11:40.420 ********* 
FAILED - RETRYING: waiting for the monitor(s) to form the quorum... (10 retries left).
ok: [node-2]

TASK [ceph-mon : fetch ceph initial keys] **************************************
Saturday 05 June 2021  00:25:45 +0000 (0:00:20.452)       0:12:00.872 ********* 
ok: [node-2]

TASK [ceph-mon : include secure_cluster.yml] ***********************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:01.310)       0:12:02.183 ********* 
skipping: [node-2]

TASK [ceph-mgr : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.020)       0:12:02.204 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-mgr : include common.yml] *******************************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.017)       0:12:02.221 ********* 
skipping: [node-2]

TASK [ceph-mgr : include pre_requisite.yml] ************************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.015)       0:12:02.237 ********* 
skipping: [node-2]

TASK [ceph-mgr : include start_mgr.yml] ****************************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.017)       0:12:02.255 ********* 
skipping: [node-2]

TASK [ceph-mgr : include mgr_modules.yml] **************************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.016)       0:12:02.271 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : mons handler] **********************************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.015)       0:12:02.286 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/handler_mons.yml for node-2

RUNNING HANDLER [ceph-handler : set _mon_handler_called before restart] ********
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.022)       0:12:02.309 ********* 
ok: [node-2]

RUNNING HANDLER [ceph-handler : copy mon restart script] ***********************
Saturday 05 June 2021  00:25:46 +0000 (0:00:00.018)       0:12:02.327 ********* 
ok: [node-2]

RUNNING HANDLER [ceph-handler : restart ceph mon daemon(s)] ********************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.326)       0:12:02.654 ********* 
skipping: [node-2] => (item=node-2) 

RUNNING HANDLER [ceph-handler : set _mon_handler_called after restart] *********
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.030)       0:12:02.684 ********* 
ok: [node-2]

RUNNING HANDLER [ceph-handler : osds handler] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.017)       0:12:02.701 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : mdss handler] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.015)       0:12:02.717 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : rgws handler] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.015)       0:12:02.732 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : rbdmirrors handler] ****************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.015)       0:12:02.747 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : mgrs handler] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.014)       0:12:02.762 ********* 
skipping: [node-2]

RUNNING HANDLER [ceph-handler : rbd-target-api and rbd-target-gw handler] ******
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.015)       0:12:02.778 ********* 
skipping: [node-2]

TASK [set ceph monitor install 'Complete'] *************************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.018)       0:12:02.796 ********* 
ok: [node-2]

PLAY [mgrs] ********************************************************************

TASK [set ceph manager install 'In Progress'] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.025)       0:12:02.822 ********* 
ok: [node-1]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.021)       0:12:02.844 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-1

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.044)       0:12:02.889 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.135)       0:12:03.024 ********* 
ok: [node-1]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.015)       0:12:03.040 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.129)       0:12:03.169 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.020)       0:12:03.190 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.014)       0:12:03.204 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.014)       0:12:03.218 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.020)       0:12:03.239 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.024)       0:12:03.264 ********* 
skipping: [node-1]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.023)       0:12:03.287 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.024)       0:12:03.311 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:25:47 +0000 (0:00:00.154)       0:12:03.466 ********* 
ok: [node-1 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:25:47.913717', 'end': '2021-06-05 00:25:47.916191', 'delta': '0:00:00.002474', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.151)       0:12:03.618 ********* 
ok: [node-1] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:25:48.072146', 'end': '2021-06-05 00:25:48.074347', 'delta': '0:00:00.002201', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:25:47.913717', 'end': '2021-06-05 00:25:47.916191', 'delta': '0:00:00.002474', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.031)       0:12:03.649 ********* 
skipping: [node-1] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.019)       0:12:03.668 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.022)       0:12:03.691 ********* 
ok: [node-1 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.291)       0:12:03.983 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.020)       0:12:04.003 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.018 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.014)       0:12:04.033 ********* 
ok: [node-1]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.022)       0:12:04.055 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.016)       0:12:04.072 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.087 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.016)       0:12:04.104 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.019)       0:12:04.123 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.014)       0:12:04.137 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.153 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.169 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.014)       0:12:04.183 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.199 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.214 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.014)       0:12:04.229 ********* 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.067)       0:12:04.297 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.016)       0:12:04.313 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.329 ********* 
skipping: [node-1]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.344 ********* 
ok: [node-1]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.132)       0:12:04.476 ********* 
ok: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.015)       0:12:04.492 ********* 
skipping: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:25:48 +0000 (0:00:00.014)       0:12:04.506 ********* 
skipping: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.526 ********* 
ok: [node-1 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.140)       0:12:04.667 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.688 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.709 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.019)       0:12:04.728 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.749 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.770 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.021)       0:12:04.791 ********* 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.812 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.827 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.014)       0:12:04.842 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.857 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.873 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.888 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.904 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.019)       0:12:04.923 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:04.939 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.020)       0:12:04.959 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.016)       0:12:04.976 ********* 
skipping: [node-1] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.017)       0:12:04.993 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.023)       0:12:05.017 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.027)       0:12:05.044 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.025)       0:12:05.070 ********* 
skipping: [node-1]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.085 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-1

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.028)       0:12:05.114 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.130 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.016)       0:12:05.146 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.161 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.177 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.018)       0:12:05.195 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.017)       0:12:05.212 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.014)       0:12:05.227 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.242 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.016)       0:12:05.259 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.275 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.018)       0:12:05.293 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.016)       0:12:05.310 ********* 
ok: [node-1]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.135)       0:12:05.446 ********* 

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.014)       0:12:05.460 ********* 

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.017)       0:12:05.478 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.015)       0:12:05.494 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:25:49 +0000 (0:00:00.016)       0:12:05.510 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:05.527 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.543 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.559 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.574 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.590 ********* 
ok: [node-1]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.155)       0:12:05.746 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.762 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.777 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.793 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.809 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.824 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.840 ********* 
ok: [node-1]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.020)       0:12:05.860 ********* 
ok: [node-1]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.021)       0:12:05.882 ********* 
skipping: [node-1]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:05.899 ********* 
skipping: [node-1]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:05.916 ********* 
skipping: [node-1]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.018)       0:12:05.934 ********* 
skipping: [node-1]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.020)       0:12:05.955 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:05.972 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:05.988 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.018)       0:12:06.006 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.023 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.040 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:06.058 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.075 ********* 
skipping: [node-1]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.018)       0:12:06.093 ********* 
skipping: [node-1]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.019)       0:12:06.112 ********* 
skipping: [node-1]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:06.130 ********* 
skipping: [node-1]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:06.147 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.017)       0:12:06.164 ********* 
skipping: [node-1]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.181 ********* 
skipping: [node-1]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.014)       0:12:06.195 ********* 
skipping: [node-1]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:06.211 ********* 
skipping: [node-1]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.014)       0:12:06.226 ********* 
skipping: [node-1]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:06.241 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.019)       0:12:06.260 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.014)       0:12:06.274 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.018)       0:12:06.293 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:06.309 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.325 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:06.340 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.016)       0:12:06.356 ********* 
skipping: [node-1]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.015)       0:12:06.372 ********* 
changed: [node-1]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:25:50 +0000 (0:00:00.145)       0:12:06.517 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-1]

TASK [ceph-mgr : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:25:51 +0000 (0:00:00.229)       0:12:06.747 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-mgr : include common.yml] *******************************************
Saturday 05 June 2021  00:25:51 +0000 (0:00:00.016)       0:12:06.764 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/common.yml for node-1

TASK [ceph-mgr : create mgr directory] *****************************************
Saturday 05 June 2021  00:25:51 +0000 (0:00:00.022)       0:12:06.786 ********* 
changed: [node-1]

TASK [ceph-mgr : fetch ceph mgr keyring] ***************************************
Saturday 05 June 2021  00:25:51 +0000 (0:00:00.145)       0:12:06.932 ********* 
skipping: [node-1]

TASK [ceph-mgr : create ceph mgr keyring(s) on a mon node] *********************
Saturday 05 June 2021  00:25:51 +0000 (0:00:00.020)       0:12:06.953 ********* 
changed: [node-1 -> node-2] => (item=node-1)

TASK [ceph-mgr : set_fact _mgr_keys] *******************************************
Saturday 05 June 2021  00:25:52 +0000 (0:00:01.084)       0:12:08.037 ********* 
ok: [node-1]

TASK [ceph-mgr : get keys from monitors] ***************************************
Saturday 05 June 2021  00:25:52 +0000 (0:00:00.027)       0:12:08.065 ********* 
skipping: [node-1] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': 'copy_admin_key'}) 
changed: [node-1 -> node-2] => (item={'name': 'mgr.ceph-research-2', 'path': '/var/lib/ceph/mgr/ceph-ceph-research-2/keyring', 'copy_key': True})

TASK [ceph-mgr : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  00:25:52 +0000 (0:00:00.306)       0:12:08.371 ********* 
skipping: [node-1] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': 'copy_admin_key'}, 'ansible_loop_var': 'item'}) 
changed: [node-1] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'mgr.ceph-research-2'], 'stdout': '[mgr.ceph-research-2]\n\tkey = AQCPxLpgAAAAABAAlSxyFSComY9Kn7wTL84/hQ==\n\tcaps mds = "allow *"\n\tcaps mon = "allow profile mgr"\n\tcaps osd = "allow *"', 'stderr': 'exported keyring for mgr.ceph-research-2', 'rc': 0, 'start': '2021-06-05 00:25:52.682146', 'end': '2021-06-05 00:25:52.826222', 'delta': '0:00:00.144076', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get mgr.ceph-research-2', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[mgr.ceph-research-2]', '\tkey = AQCPxLpgAAAAABAAlSxyFSComY9Kn7wTL84/hQ==', '\tcaps mds = "allow *"', '\tcaps mon = "allow profile mgr"', '\tcaps osd = "allow *"'], 'stderr_lines': ['exported keyring for mgr.ceph-research-2'], 'failed': False, 'item': {'name': 'mgr.ceph-research-2', 'path': '/var/lib/ceph/mgr/ceph-ceph-research-2/keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})

TASK [ceph-mgr : set mgr key permissions] **************************************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.335)       0:12:08.707 ********* 
ok: [node-1]

TASK [ceph-mgr : include pre_requisite.yml] ************************************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.147)       0:12:08.854 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/pre_requisite.yml for node-1

TASK [ceph-mgr : set_fact ceph_mgr_packages for sso] ***************************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.025)       0:12:08.880 ********* 
skipping: [node-1]

TASK [ceph-mgr : set_fact ceph_mgr_packages for dashboard] *********************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.017)       0:12:08.898 ********* 
ok: [node-1]

TASK [ceph-mgr : set_fact ceph_mgr_packages for non el7 distribution] **********
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.019)       0:12:08.917 ********* 
ok: [node-1]

TASK [ceph-mgr : install ceph-mgr packages on RedHat or SUSE] ******************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.021)       0:12:08.939 ********* 
skipping: [node-1]

TASK [ceph-mgr : install ceph-mgr packages for debian] *************************
Saturday 05 June 2021  00:25:53 +0000 (0:00:00.015)       0:12:08.955 ********* 
ok: [node-1]

TASK [ceph-mgr : include start_mgr.yml] ****************************************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.796)       0:12:09.752 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/start_mgr.yml for node-1

TASK [ceph-mgr : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.023)       0:12:09.776 ********* 
skipping: [node-1]

TASK [ceph-mgr : add ceph-mgr systemd service overrides] ***********************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.018)       0:12:09.795 ********* 
skipping: [node-1]

TASK [ceph-mgr : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.016)       0:12:09.811 ********* 
skipping: [node-1]

TASK [ceph-mgr : systemd start mgr] ********************************************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.015)       0:12:09.826 ********* 
changed: [node-1]

TASK [ceph-mgr : include mgr_modules.yml] **************************************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.670)       0:12:10.497 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/mgr_modules.yml for node-1

TASK [ceph-mgr : append dashboard modules to ceph_mgr_modules] *****************
Saturday 05 June 2021  00:25:54 +0000 (0:00:00.028)       0:12:10.525 ********* 
ok: [node-1]

TASK [ceph-mgr : wait for all mgr to be up] ************************************
Saturday 05 June 2021  00:25:55 +0000 (0:00:00.020)       0:12:10.546 ********* 
FAILED - RETRYING: wait for all mgr to be up (30 retries left).
ok: [node-1 -> node-2]

TASK [ceph-mgr : get enabled modules from ceph-mgr] ****************************
Saturday 05 June 2021  00:26:00 +0000 (0:00:05.621)       0:12:16.168 ********* 
ok: [node-1 -> node-2]

TASK [ceph-mgr : set _ceph_mgr_modules fact (convert _ceph_mgr_modules.stdout to a dict)] ***
Saturday 05 June 2021  00:26:01 +0000 (0:00:00.406)       0:12:16.574 ********* 
ok: [node-1]

TASK [ceph-mgr : set _disabled_ceph_mgr_modules fact] **************************
Saturday 05 June 2021  00:26:01 +0000 (0:00:00.055)       0:12:16.630 ********* 
ok: [node-1]

TASK [ceph-mgr : disable ceph mgr enabled modules] *****************************
Saturday 05 June 2021  00:26:01 +0000 (0:00:00.028)       0:12:16.658 ********* 
changed: [node-1 -> node-2] => (item=iostat)
changed: [node-1 -> node-2] => (item=restful)

TASK [ceph-mgr : add modules to ceph-mgr] **************************************
Saturday 05 June 2021  00:26:04 +0000 (0:00:03.367)       0:12:20.026 ********* 
changed: [node-1 -> node-2] => (item=dashboard)
changed: [node-1 -> node-2] => (item=prometheus)

RUNNING HANDLER [ceph-handler : mons handler] **********************************
Saturday 05 June 2021  00:26:08 +0000 (0:00:04.448)       0:12:24.474 ********* 
skipping: [node-1]

RUNNING HANDLER [ceph-handler : osds handler] **********************************
Saturday 05 June 2021  00:26:08 +0000 (0:00:00.017)       0:12:24.492 ********* 
skipping: [node-1]

RUNNING HANDLER [ceph-handler : mdss handler] **********************************
Saturday 05 June 2021  00:26:08 +0000 (0:00:00.016)       0:12:24.508 ********* 
skipping: [node-1]

RUNNING HANDLER [ceph-handler : rgws handler] **********************************
Saturday 05 June 2021  00:26:08 +0000 (0:00:00.016)       0:12:24.525 ********* 
skipping: [node-1]

RUNNING HANDLER [ceph-handler : rbdmirrors handler] ****************************
Saturday 05 June 2021  00:26:09 +0000 (0:00:00.017)       0:12:24.543 ********* 
skipping: [node-1]

RUNNING HANDLER [ceph-handler : mgrs handler] **********************************
Saturday 05 June 2021  00:26:09 +0000 (0:00:00.017)       0:12:24.560 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/handler_mgrs.yml for node-1

RUNNING HANDLER [ceph-handler : set _mgr_handler_called before restart] ********
Saturday 05 June 2021  00:26:09 +0000 (0:00:00.023)       0:12:24.583 ********* 
ok: [node-1]

RUNNING HANDLER [ceph-handler : copy mgr restart script] ***********************
Saturday 05 June 2021  00:26:09 +0000 (0:00:00.017)       0:12:24.601 ********* 
ok: [node-1]

RUNNING HANDLER [ceph-handler : restart ceph mgr daemon(s)] ********************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.942)       0:12:25.544 ********* 
skipping: [node-1] => (item=node-1) 

RUNNING HANDLER [ceph-handler : set _mgr_handler_called after restart] *********
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.030)       0:12:25.574 ********* 
ok: [node-1]

RUNNING HANDLER [ceph-handler : rbd-target-api and rbd-target-gw handler] ******
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.018)       0:12:25.593 ********* 
skipping: [node-1]

TASK [set ceph manager install 'Complete'] *************************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.022)       0:12:25.616 ********* 
ok: [node-1]

PLAY [osds] ********************************************************************

TASK [set ceph osd install 'In Progress'] **************************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.029)       0:12:25.645 ********* 
ok: [node-3]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.024)       0:12:25.670 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-3, node-4

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.059)       0:12:25.729 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.219)       0:12:25.949 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.025)       0:12:25.974 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.282)       0:12:26.257 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.029)       0:12:26.286 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.022)       0:12:26.308 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.022)       0:12:26.331 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.028)       0:12:26.360 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.026)       0:12:26.386 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.029)       0:12:26.416 ********* 
skipping: [node-3] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:26:10 +0000 (0:00:00.024)       0:12:26.440 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.154)       0:12:26.594 ********* 
ok: [node-3 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:11.042532', 'end': '2021-06-05 00:26:11.045192', 'delta': '0:00:00.002660', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.186)       0:12:26.781 ********* 
ok: [node-3] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:11.206631', 'end': '2021-06-05 00:26:11.208623', 'delta': '0:00:00.001992', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:11.042532', 'end': '2021-06-05 00:26:11.045192', 'delta': '0:00:00.002660', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.034)       0:12:26.815 ********* 
skipping: [node-3] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.022)       0:12:26.838 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.029)       0:12:26.867 ********* 
ok: [node-3 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.427)       0:12:27.295 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.028)       0:12:27.323 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.029)       0:12:27.352 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.021)       0:12:27.374 ********* 
ok: [node-3]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.022)       0:12:27.397 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.018)       0:12:27.416 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:26:11 +0000 (0:00:00.023)       0:12:27.439 ********* 
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)
ok: [node-4] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.283)       0:12:27.723 ********* 
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:12.040976', 'end': '2021-06-05 00:26:12.042715', 'delta': '0:00:00.001739', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:12.051706', 'end': '2021-06-05 00:26:12.053909', 'delta': '0:00:00.002203', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:12.169119', 'end': '2021-06-05 00:26:12.170938', 'delta': '0:00:00.001819', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:26:12.180844', 'end': '2021-06-05 00:26:12.182634', 'delta': '0:00:00.001790', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.040)       0:12:27.764 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.029)       0:12:27.794 ********* 

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.020)       0:12:27.815 ********* 

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.022)       0:12:27.837 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.027)       0:12:27.865 ********* 

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.021)       0:12:27.887 ********* 

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.021)       0:12:27.908 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.027)       0:12:27.936 ********* 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.070)       0:12:28.006 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.018)       0:12:28.025 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.018)       0:12:28.043 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.023)       0:12:28.067 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.144)       0:12:28.212 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.021)       0:12:28.233 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.022)       0:12:28.256 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.026)       0:12:28.282 ********* 
ok: [node-3 -> node-2]
ok: [node-4 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.155)       0:12:28.438 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.026)       0:12:28.464 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.026)       0:12:28.491 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:26:12 +0000 (0:00:00.026)       0:12:28.518 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.029)       0:12:28.548 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.027)       0:12:28.576 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.028)       0:12:28.604 ********* 
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.027)       0:12:28.632 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.654 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.677 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.699 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.722 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.745 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.767 ********* 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.027)       0:12:28.795 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:28.818 ********* 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.026)       0:12:28.844 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.024)       0:12:28.868 ********* 
skipping: [node-3] => (item=node-6) 
skipping: [node-4] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.029)       0:12:28.898 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.031)       0:12:28.930 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.029)       0:12:28.960 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.023)       0:12:28.983 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.021)       0:12:29.005 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-3, node-4

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.039)       0:12:29.044 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:29.066 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.023)       0:12:29.090 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:29.112 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.147)       0:12:29.260 ********* 

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.021)       0:12:29.281 ********* 

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:29.304 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.021)       0:12:29.326 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.024)       0:12:29.350 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:29.372 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.022)       0:12:29.395 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.024)       0:12:29.419 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.023)       0:12:29.443 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.024)       0:12:29.467 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.024)       0:12:29.491 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:26:13 +0000 (0:00:00.023)       0:12:29.515 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:29.538 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:29.561 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:29.584 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:29.606 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:29.627 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:29.649 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:29.673 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.292)       0:12:29.966 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.020)       0:12:29.986 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.024)       0:12:30.011 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.020)       0:12:30.031 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.020)       0:12:30.052 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.020)       0:12:30.072 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.095 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:30.116 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.028)       0:12:30.145 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.167 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.190 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.213 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.025)       0:12:30.239 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:30.262 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:30.285 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:30.309 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.332 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:30.355 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.378 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.400 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:30.422 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.023)       0:12:30.446 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:30.467 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.022)       0:12:30.489 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:26:14 +0000 (0:00:00.021)       0:12:30.511 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:26:15 +0000 (0:00:00.021)       0:12:30.533 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:26:15 +0000 (0:00:00.023)       0:12:30.557 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:26:15 +0000 (0:00:00.022)       0:12:30.579 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:26:15 +0000 (0:00:00.024)       0:12:30.604 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:26:15 +0000 (0:00:00.023)       0:12:30.628 ********* 
changed: [node-3]
changed: [node-4]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:26:20 +0000 (0:00:05.026)       0:12:35.655 ********* 
skipping: [node-3] => (item={'path': '/dev/vdb', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdb', 'locked': 0}, 'lsm_data': {}, 'available': True, 'rejected_reasons': [], 'device_id': '132ca2a0-a39f-4f7a-8', 'lvs': []}) 
skipping: [node-4] => (item={'path': '/dev/vdb', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdb', 'locked': 0}, 'lsm_data': {}, 'available': True, 'rejected_reasons': [], 'device_id': 'a7783c46-16ee-4d7e-b', 'lvs': []}) 
skipping: [node-3] => (item={'path': '/dev/vdc', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdc', 'locked': 0}, 'lsm_data': {}, 'available': True, 'rejected_reasons': [], 'device_id': '9234817e-8651-4ccd-8', 'lvs': []}) 
skipping: [node-4] => (item={'path': '/dev/vdc', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdc', 'locked': 0}, 'lsm_data': {}, 'available': True, 'rejected_reasons': [], 'device_id': '45707e10-63d5-4285-9', 'lvs': []}) 
skipping: [node-3] => (item={'path': '/dev/vda', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {'vda15': {'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': 111149056.0, 'human_readable_size': '106.00 MB', 'holders': []}, 'vda1': {'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': 107257773568.0, 'human_readable_size': '99.89 GB', 'holders': []}, 'vda14': {'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': 4194304.0, 'human_readable_size': '4.00 MB', 'holders': []}}, 'sectors': 0, 'sectorsize': '512', 'size': 107374182400.0, 'human_readable_size': '100.00 GB', 'path': '/dev/vda', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['locked'], 'device_id': '7c3fe3ed-ccd7-486d-b', 'lvs': []}) 
skipping: [node-4] => (item={'path': '/dev/vda', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {'vda15': {'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': 111149056.0, 'human_readable_size': '106.00 MB', 'holders': []}, 'vda1': {'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': 107257773568.0, 'human_readable_size': '99.89 GB', 'holders': []}, 'vda14': {'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': 4194304.0, 'human_readable_size': '4.00 MB', 'holders': []}}, 'sectors': 0, 'sectorsize': '512', 'size': 107374182400.0, 'human_readable_size': '100.00 GB', 'path': '/dev/vda', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['locked'], 'device_id': '4403e9d7-14c7-4acf-a', 'lvs': []}) 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:26:20 +0000 (0:00:00.056)       0:12:35.712 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:26:20 +0000 (0:00:00.027)       0:12:35.740 ********* 
[WARNING]: The value 5120 (type int) in a string field was converted to '5120'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.
[WARNING]: The value -1 (type int) in a string field was converted to '-1'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.
changed: [node-4]
changed: [node-3]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:26:27 +0000 (0:00:07.160)       0:12:42.900 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:26:27 +0000 (0:00:00.025)       0:12:42.926 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:26:27 +0000 (0:00:00.030)       0:12:42.956 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:26:27 +0000 (0:00:00.440)       0:12:43.396 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:26:27 +0000 (0:00:00.027)       0:12:43.423 ********* 
changed: [node-3]
changed: [node-4]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.152)       0:12:43.576 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-4]
changed: [node-3]

TASK [ceph-osd : set_fact add_osd] *********************************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.713)       0:12:44.289 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.030)       0:12:44.320 ********* 
skipping: [node-3] => (item=node-2) 

TASK [ceph-osd : include_tasks system_tuning.yml] ******************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.017)       0:12:44.337 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/system_tuning.yml for node-3, node-4

TASK [ceph-osd : disable osd directory parsing by updatedb] ********************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.036)       0:12:44.374 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : disable osd directory path in updatedb.conf] ******************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.020)       0:12:44.394 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : create tmpfiles.d directory] **********************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.021)       0:12:44.416 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : disable transparent hugepage] *********************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.026)       0:12:44.443 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : get default vm.min_free_kbytes] *******************************
Saturday 05 June 2021  00:26:28 +0000 (0:00:00.021)       0:12:44.464 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact vm_min_free_kbytes] **********************************
Saturday 05 June 2021  00:26:29 +0000 (0:00:00.216)       0:12:44.681 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : apply operating system tuning] ********************************
Saturday 05 June 2021  00:26:29 +0000 (0:00:00.024)       0:12:44.706 ********* 
ok: [node-4] => (item={'name': 'fs.aio-max-nr', 'value': '1048576', 'enable': True})
ok: [node-3] => (item={'name': 'fs.aio-max-nr', 'value': '1048576', 'enable': True})
ok: [node-3] => (item={'name': 'fs.file-max', 'value': 26234859})
ok: [node-4] => (item={'name': 'fs.file-max', 'value': 26234859})
ok: [node-4] => (item={'name': 'vm.zone_reclaim_mode', 'value': 0})
ok: [node-3] => (item={'name': 'vm.zone_reclaim_mode', 'value': 0})
ok: [node-4] => (item={'name': 'vm.swappiness', 'value': 10})
ok: [node-4] => (item={'name': 'vm.min_free_kbytes', 'value': '67584'})
ok: [node-3] => (item={'name': 'vm.swappiness', 'value': 10})
ok: [node-3] => (item={'name': 'vm.min_free_kbytes', 'value': '67584'})

TASK [ceph-osd : install dependencies] *****************************************
Saturday 05 June 2021  00:26:32 +0000 (0:00:02.825)       0:12:47.531 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : install numactl when needed] **********************************
Saturday 05 June 2021  00:26:32 +0000 (0:00:00.742)       0:12:48.274 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks common.yml] *************************************
Saturday 05 June 2021  00:26:32 +0000 (0:00:00.031)       0:12:48.305 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/common.yml for node-3, node-4

TASK [ceph-osd : create bootstrap-osd and osd directories] *********************
Saturday 05 June 2021  00:26:32 +0000 (0:00:00.037)       0:12:48.343 ********* 
ok: [node-3] => (item=/var/lib/ceph/bootstrap-osd/)
ok: [node-4] => (item=/var/lib/ceph/bootstrap-osd/)
ok: [node-4] => (item=/var/lib/ceph/osd/)
ok: [node-3] => (item=/var/lib/ceph/osd/)

TASK [ceph-osd : get keys from monitors] ***************************************
Saturday 05 June 2021  00:26:33 +0000 (0:00:00.312)       0:12:48.656 ********* 
changed: [node-3 -> node-2] => (item={'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True})
skipping: [node-3] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}) 

TASK [ceph-osd : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  00:26:33 +0000 (0:00:00.434)       0:12:49.090 ********* 
changed: [node-3] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-osd'], 'stdout': '[client.bootstrap-osd]\n\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==\n\tcaps mon = "allow profile bootstrap-osd"', 'stderr': 'exported keyring for client.bootstrap-osd', 'rc': 0, 'start': '2021-06-05 00:26:33.260643', 'end': '2021-06-05 00:26:33.542275', 'delta': '0:00:00.281632', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-osd', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-osd]', '\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==', '\tcaps mon = "allow profile bootstrap-osd"'], 'stderr_lines': ['exported keyring for client.bootstrap-osd'], 'failed': False, 'item': {'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-3] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 
changed: [node-4] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-osd'], 'stdout': '[client.bootstrap-osd]\n\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==\n\tcaps mon = "allow profile bootstrap-osd"', 'stderr': 'exported keyring for client.bootstrap-osd', 'rc': 0, 'start': '2021-06-05 00:26:33.260643', 'end': '2021-06-05 00:26:33.542275', 'delta': '0:00:00.281632', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-osd', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-osd]', '\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==', '\tcaps mon = "allow profile bootstrap-osd"'], 'stderr_lines': ['exported keyring for client.bootstrap-osd'], 'failed': False, 'item': {'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-4] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 

TASK [ceph-osd : set noup flag] ************************************************
Saturday 05 June 2021  00:26:33 +0000 (0:00:00.404)       0:12:49.495 ********* 
changed: [node-3 -> node-2]

TASK [ceph-osd : include container_options_facts.yml] **************************
Saturday 05 June 2021  00:26:35 +0000 (0:00:01.488)       0:12:50.983 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks scenarios/lvm.yml] ******************************
Saturday 05 June 2021  00:26:35 +0000 (0:00:00.021)       0:12:51.004 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks scenarios/lvm-batch.yml] ************************
Saturday 05 June 2021  00:26:35 +0000 (0:00:00.022)       0:12:51.026 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/scenarios/lvm-batch.yml for node-3, node-4

TASK [ceph-osd : use ceph-volume lvm batch to create bluestore osds] ***********
Saturday 05 June 2021  00:26:35 +0000 (0:00:00.044)       0:12:51.071 ********* 
changed: [node-3]
changed: [node-4]

TASK [ceph-osd : include_tasks start_osds.yml] *********************************
Saturday 05 June 2021  00:27:29 +0000 (0:00:53.773)       0:13:44.845 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/start_osds.yml for node-3, node-4

TASK [ceph-osd : umount ceph disk (if on openstack)] ***************************
Saturday 05 June 2021  00:27:29 +0000 (0:00:00.041)       0:13:44.886 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : get osd ids] **************************************************
Saturday 05 June 2021  00:27:29 +0000 (0:00:00.020)       0:13:44.907 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact container_exec_start_osd] ****************************
Saturday 05 June 2021  00:27:29 +0000 (0:00:00.147)       0:13:45.055 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : collect osd ids] **********************************************
Saturday 05 June 2021  00:27:29 +0000 (0:00:00.026)       0:13:45.081 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-osd : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  00:27:30 +0000 (0:00:00.834)       0:13:45.916 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:27:30 +0000 (0:00:00.021)       0:13:45.937 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : add ceph-osd systemd service overrides] ***********************
Saturday 05 June 2021  00:27:30 +0000 (0:00:00.020)       0:13:45.958 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : ensure "/var/lib/ceph/osd/{{ cluster }}-{{ item }}" is present] ***
Saturday 05 June 2021  00:27:30 +0000 (0:00:00.082)       0:13:46.040 ********* 
changed: [node-3] => (item=1)
changed: [node-4] => (item=0)
changed: [node-3] => (item=3)
changed: [node-4] => (item=2)

TASK [ceph-osd : systemd start osd] ********************************************
Saturday 05 June 2021  00:27:30 +0000 (0:00:00.299)       0:13:46.340 ********* 
ok: [node-3] => (item=1)
ok: [node-4] => (item=0)
ok: [node-4] => (item=2)
ok: [node-3] => (item=3)

TASK [ceph-osd : unset noup flag] **********************************************
Saturday 05 June 2021  00:27:31 +0000 (0:00:00.918)       0:13:47.259 ********* 
skipping: [node-3]
changed: [node-4 -> node-2]

TASK [ceph-osd : wait for all osd to be up] ************************************
Saturday 05 June 2021  00:27:35 +0000 (0:00:03.586)       0:13:50.845 ********* 
skipping: [node-3]
FAILED - RETRYING: wait for all osd to be up (60 retries left).
ok: [node-4 -> node-2]

TASK [ceph-osd : include crush_rules.yml] **************************************
Saturday 05 June 2021  00:27:46 +0000 (0:00:10.831)       0:14:01.677 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include openstack_config.yml] *********************************
Saturday 05 June 2021  00:27:46 +0000 (0:00:00.025)       0:14:01.703 ********* 
skipping: [node-3]
skipping: [node-4]

RUNNING HANDLER [ceph-handler : mons handler] **********************************
Saturday 05 June 2021  00:27:46 +0000 (0:00:00.020)       0:14:01.723 ********* 
skipping: [node-4]
skipping: [node-3]

RUNNING HANDLER [ceph-handler : osds handler] **********************************
Saturday 05 June 2021  00:27:46 +0000 (0:00:00.025)       0:14:01.749 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/handler_osds.yml for node-4, node-3

RUNNING HANDLER [ceph-handler : set _osd_handler_called before restart] ********
Saturday 05 June 2021  00:27:46 +0000 (0:00:00.036)       0:14:01.785 ********* 
ok: [node-4]
ok: [node-3]

RUNNING HANDLER [ceph-handler : unset noup flag] *******************************
Saturday 05 June 2021  00:27:46 +0000 (0:00:00.026)       0:14:01.812 ********* 
changed: [node-4 -> node-2]

RUNNING HANDLER [ceph-handler : copy osd restart script] ***********************
Saturday 05 June 2021  00:27:48 +0000 (0:00:02.025)       0:14:03.837 ********* 
ok: [node-4]
ok: [node-3]

RUNNING HANDLER [ceph-handler : restart ceph osds daemon(s)] *******************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.445)       0:14:04.283 ********* 
skipping: [node-4] => (item=node-3) 
skipping: [node-4] => (item=node-4) 

RUNNING HANDLER [ceph-handler : set _osd_handler_called after restart] *********
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.042)       0:14:04.325 ********* 
ok: [node-4]
ok: [node-3]

RUNNING HANDLER [ceph-handler : mdss handler] **********************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.023)       0:14:04.348 ********* 
skipping: [node-4]
skipping: [node-3]

RUNNING HANDLER [ceph-handler : rgws handler] **********************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.022)       0:14:04.371 ********* 
skipping: [node-4]
skipping: [node-3]

RUNNING HANDLER [ceph-handler : rbdmirrors handler] ****************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.023)       0:14:04.395 ********* 
skipping: [node-4]
skipping: [node-3]

RUNNING HANDLER [ceph-handler : mgrs handler] **********************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.023)       0:14:04.419 ********* 
skipping: [node-4]
skipping: [node-3]

RUNNING HANDLER [ceph-handler : rbd-target-api and rbd-target-gw handler] ******
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.022)       0:14:04.441 ********* 
skipping: [node-4]
skipping: [node-3]

TASK [set ceph osd install 'Complete'] *****************************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.026)       0:14:04.468 ********* 
ok: [node-3]

PLAY [mdss] ********************************************************************
skipping: no hosts matched

PLAY [rgws] ********************************************************************

TASK [set ceph rgw install 'In Progress'] **************************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.030)       0:14:04.498 ********* 
ok: [node-6]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:27:48 +0000 (0:00:00.022)       0:14:04.521 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-6

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.047)       0:14:04.568 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.135)       0:14:04.703 ********* 
ok: [node-6]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.017)       0:14:04.721 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.134)       0:14:04.855 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.020)       0:14:04.875 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.016)       0:14:04.892 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.014)       0:14:04.907 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.022)       0:14:04.929 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.025)       0:14:04.954 ********* 
skipping: [node-6]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.021)       0:14:04.976 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.024)       0:14:05.001 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.232)       0:14:05.233 ********* 
ok: [node-6 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:49.679643', 'end': '2021-06-05 00:27:49.682832', 'delta': '0:00:00.003189', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:27:49 +0000 (0:00:00.289)       0:14:05.523 ********* 
ok: [node-6] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:49.840983', 'end': '2021-06-05 00:27:49.843056', 'delta': '0:00:00.002073', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:49.679643', 'end': '2021-06-05 00:27:49.682832', 'delta': '0:00:00.003189', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.030)       0:14:05.553 ********* 
skipping: [node-6] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.019)       0:14:05.573 ********* 
skipping: [node-6]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.020)       0:14:05.593 ********* 
ok: [node-6 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.619)       0:14:06.213 ********* 
skipping: [node-6]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.020)       0:14:06.234 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.015)       0:14:06.250 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.265 ********* 
ok: [node-6]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.023)       0:14:06.288 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.303 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.318 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.013)       0:14:06.331 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.013)       0:14:06.345 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.015)       0:14:06.360 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.015)       0:14:06.376 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.390 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.013)       0:14:06.404 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.418 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.014)       0:14:06.432 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.012)       0:14:06.445 ********* 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:27:50 +0000 (0:00:00.063)       0:14:06.508 ********* 
ok: [node-6 -> node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.375)       0:14:06.884 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.021)       0:14:06.906 ********* 
skipping: [node-6]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:06.926 ********* 
ok: [node-6]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.134)       0:14:07.060 ********* 
ok: [node-6]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.015)       0:14:07.076 ********* 
skipping: [node-6]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.015)       0:14:07.091 ********* 
skipping: [node-6]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.018)       0:14:07.110 ********* 
ok: [node-6 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.145)       0:14:07.255 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.275 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.295 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.315 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.020)       0:14:07.336 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.355 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.375 ********* 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.019)       0:14:07.394 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.017)       0:14:07.411 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.017)       0:14:07.429 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.020)       0:14:07.450 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.018)       0:14:07.468 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.017)       0:14:07.486 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.017)       0:14:07.503 ********* 
ok: [node-6] => (item=0)

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:27:51 +0000 (0:00:00.026)       0:14:07.529 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.017)       0:14:07.546 ********* 
skipping: [node-6] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.022)       0:14:07.569 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.020)       0:14:07.589 ********* 
skipping: [node-6] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.022)       0:14:07.611 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.024)       0:14:07.635 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.027)       0:14:07.663 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.022)       0:14:07.686 ********* 
skipping: [node-6]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:07.701 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-6

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.028)       0:14:07.729 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:07.744 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:07.760 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:07.776 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.017)       0:14:07.794 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.017)       0:14:07.812 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:07.828 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:07.843 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:07.860 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:07.877 ********* 
ok: [node-6]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.138)       0:14:08.015 ********* 

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.013)       0:14:08.029 ********* 

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.044 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.059 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.075 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:08.091 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:08.106 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.017)       0:14:08.124 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:08.141 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.156 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:08.171 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.186 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:08.201 ********* 
ok: [node-6]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.154)       0:14:08.355 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.371 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:08.385 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.400 ********* 
ok: [node-6]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.019)       0:14:08.420 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.014)       0:14:08.435 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.450 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.015)       0:14:08.466 ********* 
ok: [node-6]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.022)       0:14:08.489 ********* 
skipping: [node-6]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.017)       0:14:08.507 ********* 
skipping: [node-6]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:27:52 +0000 (0:00:00.016)       0:14:08.524 ********* 
skipping: [node-6]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.541 ********* 
skipping: [node-6]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.019)       0:14:08.561 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.578 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.013)       0:14:08.591 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.609 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.627 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.645 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.663 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.681 ********* 
skipping: [node-6]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.698 ********* 
skipping: [node-6]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.018)       0:14:08.717 ********* 
skipping: [node-6]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:08.734 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.020)       0:14:08.754 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.018)       0:14:08.773 ********* 
skipping: [node-6]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.018)       0:14:08.791 ********* 
skipping: [node-6]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.015)       0:14:08.807 ********* 
included: /opt/ceph-ansible/roles/ceph-config/tasks/rgw_systemd_environment_file.yml for node-6

TASK [ceph-config : create rados gateway instance directories] *****************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.019)       0:14:08.827 ********* 
changed: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-config : generate environment file] *********************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.152)       0:14:08.979 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.018)       0:14:08.997 ********* 
skipping: [node-6]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.011 ********* 
skipping: [node-6]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.015)       0:14:09.027 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.041 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.013)       0:14:09.055 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.069 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.084 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.017)       0:14:09.101 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.116 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.130 ********* 
skipping: [node-6]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.014)       0:14:09.145 ********* 
changed: [node-6]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:27:53 +0000 (0:00:00.143)       0:14:09.288 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-6]

TASK [ceph-rgw : include common.yml] *******************************************
Saturday 05 June 2021  00:27:54 +0000 (0:00:00.244)       0:14:09.533 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/common.yml for node-6

TASK [ceph-rgw : create rados gateway directories] *****************************
Saturday 05 June 2021  00:27:54 +0000 (0:00:00.020)       0:14:09.553 ********* 
changed: [node-6] => (item=/var/run/ceph)

TASK [ceph-rgw : get keys from monitors] ***************************************
Saturday 05 June 2021  00:27:54 +0000 (0:00:00.171)       0:14:09.724 ********* 
changed: [node-6 -> node-2] => (item={'name': 'client.bootstrap-rgw', 'path': '/var/lib/ceph/bootstrap-rgw/ceph.keyring', 'copy_key': True})
skipping: [node-6] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}) 

TASK [ceph-rgw : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  00:27:54 +0000 (0:00:00.492)       0:14:10.216 ********* 
changed: [node-6] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-rgw'], 'stdout': '[client.bootstrap-rgw]\n\tkey = AQB5xLpgat1nFBAACBG9jvBl5Gee0ePAlCEuEQ==\n\tcaps mon = "allow profile bootstrap-rgw"', 'stderr': 'exported keyring for client.bootstrap-rgw', 'rc': 0, 'start': '2021-06-05 00:27:54.328022', 'end': '2021-06-05 00:27:54.667176', 'delta': '0:00:00.339154', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-rgw', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-rgw]', '\tkey = AQB5xLpgat1nFBAACBG9jvBl5Gee0ePAlCEuEQ==', '\tcaps mon = "allow profile bootstrap-rgw"'], 'stderr_lines': ['exported keyring for client.bootstrap-rgw'], 'failed': False, 'item': {'name': 'client.bootstrap-rgw', 'path': '/var/lib/ceph/bootstrap-rgw/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-6] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 

TASK [ceph-rgw : copy SSL certificate & key data to certificate path] **********
Saturday 05 June 2021  00:27:55 +0000 (0:00:00.330)       0:14:10.546 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks pre_requisite.yml] ******************************
Saturday 05 June 2021  00:27:55 +0000 (0:00:00.015)       0:14:10.562 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/pre_requisite.yml for node-6

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:27:55 +0000 (0:00:00.022)       0:14:10.584 ********* 
changed: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : rgw pool creation tasks] **************************************
Saturday 05 June 2021  00:27:56 +0000 (0:00:01.217)       0:14:11.802 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks openstack-keystone.yml] *************************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.015)       0:14:11.817 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks start_radosgw.yml] ******************************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.014)       0:14:11.832 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/start_radosgw.yml for node-6

TASK [ceph-rgw : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.023)       0:14:11.856 ********* 
skipping: [node-6]

TASK [ceph-rgw : add ceph-rgw systemd service overrides] ***********************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.013)       0:14:11.869 ********* 
skipping: [node-6]

TASK [ceph-rgw : start rgw instance] *******************************************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.013)       0:14:11.883 ********* 
changed: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : enable the ceph-radosgw.target service] ***********************
Saturday 05 June 2021  00:27:56 +0000 (0:00:00.480)       0:14:12.363 ********* 
ok: [node-6]

TASK [ceph-rgw : include start_docker_rgw.yml] *********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.235)       0:14:12.599 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks multisite/main.yml] *****************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.016)       0:14:12.615 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : mons handler] **********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.012)       0:14:12.628 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : osds handler] **********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.018)       0:14:12.647 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : mdss handler] **********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.018)       0:14:12.665 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : rgws handler] **********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.016)       0:14:12.681 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/handler_rgws.yml for node-6

RUNNING HANDLER [ceph-handler : set _rgw_handler_called before restart] ********
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.022)       0:14:12.703 ********* 
ok: [node-6]

RUNNING HANDLER [ceph-handler : copy rgw restart script] ***********************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.018)       0:14:12.721 ********* 
ok: [node-6]

RUNNING HANDLER [ceph-handler : restart ceph rgw daemon(s)] ********************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.307)       0:14:13.029 ********* 
skipping: [node-6] => (item=node-6) 

RUNNING HANDLER [ceph-handler : set _rgw_handler_called after restart] *********
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.028)       0:14:13.058 ********* 
ok: [node-6]

RUNNING HANDLER [ceph-handler : rbdmirrors handler] ****************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.018)       0:14:13.077 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : mgrs handler] **********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.016)       0:14:13.093 ********* 
skipping: [node-6]

RUNNING HANDLER [ceph-handler : rbd-target-api and rbd-target-gw handler] ******
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.017)       0:14:13.111 ********* 
skipping: [node-6]

TASK [set ceph rgw install 'Complete'] *****************************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.020)       0:14:13.132 ********* 
ok: [node-6]

PLAY [clients] *****************************************************************
skipping: no hosts matched

PLAY [nfss] ********************************************************************
skipping: no hosts matched

PLAY [rbdmirrors] **************************************************************
skipping: no hosts matched

PLAY [iscsigws,iscsi-gws] ******************************************************
skipping: no hosts matched

PLAY [rgwloadbalancers] ********************************************************
skipping: no hosts matched

PLAY [mons,osds,mdss,rgws,mgrs,rbdmirrors,nfss,iscsigws,grafana-server] ********

TASK [set ceph node exporter install 'In Progress'] ****************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.036)       0:14:13.168 ********* 
ok: [node-2]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.025)       0:14:13.194 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.108)       0:14:13.302 ********* 
ok: [node-4]
ok: [node-6]
ok: [node-5]
ok: [node-3]
ok: [node-2]
ok: [node-1]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:27:57 +0000 (0:00:00.194)       0:14:13.496 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.069)       0:14:13.566 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-5]
ok: [node-6]
ok: [node-1]
ok: [node-2]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.223)       0:14:13.789 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-1]
ok: [node-6]
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.078)       0:14:13.868 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.061)       0:14:13.929 ********* 
skipping: [node-2]
skipping: [node-4]
skipping: [node-3]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.117)       0:14:14.047 ********* 
ok: [node-2]
ok: [node-4]
ok: [node-3]
ok: [node-1]
ok: [node-6]
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.070)       0:14:14.117 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.046)       0:14:14.163 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.070)       0:14:14.234 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.047)       0:14:14.281 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:27:58 +0000 (0:00:00.180)       0:14:14.462 ********* 
ok: [node-2 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:58.908225', 'end': '2021-06-05 00:27:58.911048', 'delta': '0:00:00.002823', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:27:59 +0000 (0:00:00.179)       0:14:14.642 ********* 
ok: [node-2] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:59.093019', 'end': '2021-06-05 00:27:59.095888', 'delta': '0:00:00.002869', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:27:58.908225', 'end': '2021-06-05 00:27:58.911048', 'delta': '0:00:00.002823', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:27:59 +0000 (0:00:00.055)       0:14:14.697 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:27:59 +0000 (0:00:00.044)       0:14:14.741 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:27:59 +0000 (0:00:00.073)       0:14:14.815 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:27:59 +0000 (0:00:00.670)       0:14:15.485 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.070)       0:14:15.555 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.089)       0:14:15.645 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.063)       0:14:15.708 ********* 
ok: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.034)       0:14:15.743 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.028)       0:14:15.772 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:28:00 +0000 (0:00:00.063)       0:14:15.835 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-4] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:28:01 +0000 (0:00:01.394)       0:14:17.230 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:28:00.467629', 'end': '2021-06-05 00:28:01.470989', 'delta': '0:00:01.003360', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
skipping: [node-5]
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:28:00.476747', 'end': '2021-06-05 00:28:00.478432', 'delta': '0:00:00.001685', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:28:01.684279', 'end': '2021-06-05 00:28:01.686690', 'delta': '0:00:00.002411', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:28:00.695258', 'end': '2021-06-05 00:28:00.697025', 'delta': '0:00:00.001767', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:28:01 +0000 (0:00:00.093)       0:14:17.323 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:28:01 +0000 (0:00:00.067)       0:14:17.391 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:28:01 +0000 (0:00:00.062)       0:14:17.454 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:28:01 +0000 (0:00:00.062)       0:14:17.516 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-5]
skipping: [node-1]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.064)       0:14:17.581 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.062)       0:14:17.643 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.063)       0:14:17.707 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.067)       0:14:17.775 ********* 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.517)       0:14:18.292 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.028)       0:14:18.320 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.028)       0:14:18.349 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:28:02 +0000 (0:00:00.066)       0:14:18.416 ********* 
ok: [node-2]
ok: [node-6]
ok: [node-1]
ok: [node-4]
ok: [node-5]
ok: [node-3]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.254)       0:14:18.670 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.067)       0:14:18.737 ********* 
skipping: [node-5]
ok: [node-3]
ok: [node-2]
ok: [node-4]
ok: [node-6]
ok: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.192)       0:14:18.930 ********* 
ok: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.127)       0:14:19.058 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.216)       0:14:19.274 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.072)       0:14:19.347 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:28:03 +0000 (0:00:00.104)       0:14:19.452 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.104)       0:14:19.556 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-5] => (item=node-2) 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.098)       0:14:19.655 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.102)       0:14:19.758 ********* 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.102)       0:14:19.861 ********* 
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.103)       0:14:19.964 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.069)       0:14:20.034 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.072)       0:14:20.107 ********* 
skipping: [node-2]
skipping: [node-4]
skipping: [node-3]
ok: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.073)       0:14:20.181 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.073)       0:14:20.254 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.076)       0:14:20.331 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.076)       0:14:20.408 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 
ok: [node-6] => (item=0)

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:28:04 +0000 (0:00:00.092)       0:14:20.501 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.073)       0:14:20.574 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 
skipping: [node-6] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.091)       0:14:20.666 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-5]
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.085)       0:14:20.751 ********* 
skipping: [node-3] => (item=node-6) 
skipping: [node-2] => (item=node-6) 
skipping: [node-4] => (item=node-6) 
skipping: [node-1] => (item=node-6) 
skipping: [node-5] => (item=node-6) 
skipping: [node-6] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.084)       0:14:20.836 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.072)       0:14:20.908 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.051)       0:14:20.959 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-container-engine : include pre_requisites/prerequisites.yml] ********
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.046)       0:14:21.005 ********* 
included: /opt/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/prerequisites.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-container-engine : include specific variables] **********************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.089)       0:14:21.095 ********* 
ok: [node-3] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-2] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-4] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-6] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-1] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-5] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)

TASK [ceph-container-engine : debian based systems tasks] **********************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.094)       0:14:21.189 ********* 
included: /opt/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/debian_prerequisites.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-container-engine : uninstall old docker versions] *******************
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.098)       0:14:21.288 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-container-engine : allow apt to use a repository over https (debian)] ***
Saturday 05 June 2021  00:28:05 +0000 (0:00:00.072)       0:14:21.360 ********* 
ok: [node-4]
ok: [node-6]
ok: [node-3]
ok: [node-2]
ok: [node-5]
ok: [node-1]

TASK [ceph-container-engine : add docker's gpg key] ****************************
Saturday 05 June 2021  00:28:09 +0000 (0:00:03.494)       0:14:24.854 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : add docker repository] ***************************
Saturday 05 June 2021  00:28:09 +0000 (0:00:00.071)       0:14:24.926 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-container-engine : add podman ppa repository] ***********************
Saturday 05 June 2021  00:28:09 +0000 (0:00:00.070)       0:14:24.996 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-container-engine : enable extras on centos] *************************
Saturday 05 June 2021  00:28:09 +0000 (0:00:00.070)       0:14:25.067 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : install container packages] **********************
Saturday 05 June 2021  00:28:09 +0000 (0:00:00.066)       0:14:25.134 ********* 
ok: [node-1]
ok: [node-5]
ok: [node-3]
ok: [node-6]
ok: [node-4]
ok: [node-2]

TASK [ceph-container-engine : install lvm2 package] ****************************
Saturday 05 June 2021  00:28:12 +0000 (0:00:02.618)       0:14:27.752 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-3]
ok: [node-4]

TASK [ceph-container-engine : create the systemd docker override directory] ****
Saturday 05 June 2021  00:28:12 +0000 (0:00:00.696)       0:14:28.448 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : create the systemd docker override file] *********
Saturday 05 June 2021  00:28:12 +0000 (0:00:00.065)       0:14:28.513 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : remove docker proxy configuration] ***************
Saturday 05 June 2021  00:28:13 +0000 (0:00:00.068)       0:14:28.582 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-5]
ok: [node-1]
ok: [node-6]

TASK [ceph-container-engine : restart docker] **********************************
Saturday 05 June 2021  00:28:16 +0000 (0:00:03.099)       0:14:31.682 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : start container service] *************************
Saturday 05 June 2021  00:28:16 +0000 (0:00:00.065)       0:14:31.748 ********* 
ok: [node-4]
ok: [node-3]
ok: [node-5]
ok: [node-6]
ok: [node-1]
ok: [node-2]

TASK [ceph-container-common : container registry authentication] ***************
Saturday 05 June 2021  00:28:17 +0000 (0:00:01.142)       0:14:32.890 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-node-exporter : include setup_container.yml] ************************
Saturday 05 June 2021  00:28:17 +0000 (0:00:00.061)       0:14:32.951 ********* 
included: /opt/ceph-ansible/roles/ceph-node-exporter/tasks/setup_container.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-node-exporter : include_tasks systemd.yml] **************************
Saturday 05 June 2021  00:28:17 +0000 (0:00:00.079)       0:14:33.031 ********* 
included: /opt/ceph-ansible/roles/ceph-node-exporter/tasks/systemd.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-node-exporter : ship systemd service] *******************************
Saturday 05 June 2021  00:28:17 +0000 (0:00:00.087)       0:14:33.118 ********* 
changed: [node-4]
changed: [node-3]
changed: [node-6]
changed: [node-1]
changed: [node-2]
changed: [node-5]

TASK [ceph-node-exporter : start the node_exporter service] ********************
Saturday 05 June 2021  00:28:18 +0000 (0:00:01.168)       0:14:34.287 ********* 
changed: [node-5]
changed: [node-6]
changed: [node-1]
changed: [node-2]
changed: [node-3]
changed: [node-4]

TASK [set ceph node exporter install 'Complete'] *******************************
Saturday 05 June 2021  00:29:15 +0000 (0:00:56.973)       0:15:31.260 ********* 
ok: [node-2]

PLAY [grafana-server] **********************************************************

TASK [set ceph grafana install 'In Progress'] **********************************
Saturday 05 June 2021  00:29:15 +0000 (0:00:00.032)       0:15:31.292 ********* 
ok: [node-5]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:29:15 +0000 (0:00:00.026)       0:15:31.319 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:29:15 +0000 (0:00:00.055)       0:15:31.374 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:29:15 +0000 (0:00:00.137)       0:15:31.512 ********* 
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.030)       0:15:31.542 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.146)       0:15:31.689 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.034)       0:15:31.724 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.025)       0:15:31.749 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.026)       0:15:31.776 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.032)       0:15:31.808 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.069)       0:15:31.878 ********* 
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.031)       0:15:31.909 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.046)       0:15:31.955 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.174)       0:15:32.130 ********* 
ok: [node-5 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:29:16.578334', 'end': '2021-06-05 00:29:16.580648', 'delta': '0:00:00.002314', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.178)       0:15:32.309 ********* 
ok: [node-5] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:29:16.762522', 'end': '2021-06-05 00:29:16.764804', 'delta': '0:00:00.002282', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:29:16.578334', 'end': '2021-06-05 00:29:16.580648', 'delta': '0:00:00.002314', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.053)       0:15:32.362 ********* 
skipping: [node-5] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.042)       0:15:32.405 ********* 
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:29:16 +0000 (0:00:00.032)       0:15:32.437 ********* 
ok: [node-5 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.878)       0:15:33.315 ********* 
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.029)       0:15:33.345 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.026)       0:15:33.371 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.025)       0:15:33.397 ********* 
ok: [node-5]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.033)       0:15:33.431 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.027)       0:15:33.458 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.029)       0:15:33.487 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:29:17 +0000 (0:00:00.026)       0:15:33.514 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.026)       0:15:33.540 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:33.566 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:33.591 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:33.617 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:33.642 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.026)       0:15:33.669 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.026)       0:15:33.696 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:33.721 ********* 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.292)       0:15:34.013 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.027)       0:15:34.041 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.026)       0:15:34.068 ********* 
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.026)       0:15:34.094 ********* 
ok: [node-5]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.139)       0:15:34.234 ********* 
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.027)       0:15:34.261 ********* 
skipping: [node-5]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.025)       0:15:34.287 ********* 
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.029)       0:15:34.316 ********* 
ok: [node-5 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.154)       0:15:34.470 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:29:18 +0000 (0:00:00.031)       0:15:34.501 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.050)       0:15:34.552 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.053)       0:15:34.606 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.052)       0:15:34.659 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.052)       0:15:34.711 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.051)       0:15:34.763 ********* 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.051)       0:15:34.814 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.031)       0:15:34.846 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.032)       0:15:34.878 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.032)       0:15:34.910 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.029)       0:15:34.940 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.033)       0:15:34.973 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.031)       0:15:35.004 ********* 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.034)       0:15:35.039 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.031)       0:15:35.070 ********* 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.034)       0:15:35.105 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.031)       0:15:35.136 ********* 
skipping: [node-5] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.034)       0:15:35.171 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.036)       0:15:35.207 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.050)       0:15:35.258 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set grafana_server_addr fact - ipv4] ************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.046)       0:15:35.304 ********* 
ok: [node-5]

TASK [ceph-facts : set grafana_server_addr fact - ipv6] ************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.032)       0:15:35.337 ********* 
skipping: [node-5]

TASK [ceph-facts : set grafana_server_addrs fact - ipv4] ***********************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.022)       0:15:35.359 ********* 
ok: [node-5] => (item=node-5)

TASK [ceph-facts : set grafana_server_addrs fact - ipv6] ***********************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.038)       0:15:35.398 ********* 
skipping: [node-5] => (item=node-5) 

TASK [ceph-prometheus : create prometheus directories] *************************
Saturday 05 June 2021  00:29:19 +0000 (0:00:00.031)       0:15:35.430 ********* 
changed: [node-5] => (item=/etc/prometheus)
changed: [node-5] => (item=/var/lib/prometheus)

TASK [ceph-prometheus : write prometheus config file] **************************
Saturday 05 June 2021  00:29:20 +0000 (0:00:00.284)       0:15:35.714 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-5]

TASK [ceph-prometheus : make sure the alerting rules directory exists] *********
Saturday 05 June 2021  00:29:20 +0000 (0:00:00.224)       0:15:35.938 ********* 
changed: [node-5]

TASK [ceph-prometheus : copy alerting rules] ***********************************
Saturday 05 June 2021  00:29:20 +0000 (0:00:00.142)       0:15:36.080 ********* 
changed: [node-5]

TASK [ceph-prometheus : create alertmanager directories] ***********************
Saturday 05 June 2021  00:29:20 +0000 (0:00:00.311)       0:15:36.392 ********* 
changed: [node-5] => (item=/etc/alertmanager)
changed: [node-5] => (item=/var/lib/alertmanager)

TASK [ceph-prometheus : write alertmanager config file] ************************
Saturday 05 June 2021  00:29:21 +0000 (0:00:01.095)       0:15:37.487 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-5]

TASK [ceph-prometheus : include setup_container.yml] ***************************
Saturday 05 June 2021  00:29:22 +0000 (0:00:00.197)       0:15:37.685 ********* 
included: /opt/ceph-ansible/roles/ceph-prometheus/tasks/setup_container.yml for node-5

TASK [ceph-prometheus : include_tasks systemd.yml] *****************************
Saturday 05 June 2021  00:29:22 +0000 (0:00:00.027)       0:15:37.712 ********* 
included: /opt/ceph-ansible/roles/ceph-prometheus/tasks/systemd.yml for node-5

TASK [ceph-prometheus : ship systemd services] *********************************
Saturday 05 June 2021  00:29:22 +0000 (0:00:00.030)       0:15:37.743 ********* 
changed: [node-5] => (item=alertmanager.service)
changed: [node-5] => (item=prometheus.service)

TASK [ceph-prometheus : start prometheus services] *****************************
Saturday 05 June 2021  00:29:22 +0000 (0:00:00.663)       0:15:38.406 ********* 
changed: [node-5] => (item=prometheus)
changed: [node-5] => (item=alertmanager)

TASK [ceph-grafana : include setup_container.yml] ******************************
Saturday 05 June 2021  00:29:24 +0000 (0:00:01.298)       0:15:39.705 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/setup_container.yml for node-5

TASK [ceph-grafana : create /etc/grafana and /var/lib/grafana] *****************
Saturday 05 June 2021  00:29:24 +0000 (0:00:00.027)       0:15:39.732 ********* 
ok: [node-5] => (item=/etc/grafana)
changed: [node-5] => (item=/var/lib/grafana)
[WARNING]: The value 472 (type int) in a string field was converted to '472'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.

TASK [ceph-grafana : include_tasks systemd.yml] ********************************
Saturday 05 June 2021  00:29:24 +0000 (0:00:00.328)       0:15:40.060 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/systemd.yml for node-5

TASK [ceph-grafana : ship systemd service] *************************************
Saturday 05 June 2021  00:29:24 +0000 (0:00:00.031)       0:15:40.091 ********* 
changed: [node-5]

TASK [ceph-grafana : start the grafana-server service] *************************
Saturday 05 June 2021  00:29:24 +0000 (0:00:00.314)       0:15:40.405 ********* 
changed: [node-5]

TASK [ceph-grafana : include configure_grafana.yml] ****************************
Saturday 05 June 2021  00:29:25 +0000 (0:00:00.667)       0:15:41.073 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/configure_grafana.yml for node-5

TASK [ceph-grafana : install ceph-grafana-dashboards package on RedHat or SUSE] ***
Saturday 05 June 2021  00:29:25 +0000 (0:00:00.032)       0:15:41.106 ********* 
skipping: [node-5]

TASK [ceph-grafana : make sure grafana is down] ********************************
Saturday 05 June 2021  00:29:25 +0000 (0:00:00.027)       0:15:41.134 ********* 
changed: [node-5]

TASK [ceph-grafana : wait for grafana to be stopped] ***************************
Saturday 05 June 2021  00:29:25 +0000 (0:00:00.263)       0:15:41.397 ********* 
ok: [node-5]

TASK [ceph-grafana : make sure grafana configuration directories exist] ********
Saturday 05 June 2021  00:29:26 +0000 (0:00:00.809)       0:15:42.207 ********* 
changed: [node-5] => (item=/etc/grafana/dashboards/ceph-dashboard)
changed: [node-5] => (item=/etc/grafana/provisioning/datasources)
changed: [node-5] => (item=/etc/grafana/provisioning/dashboards)
changed: [node-5] => (item=/etc/grafana/provisioning/notifiers)

TASK [ceph-grafana : download ceph grafana dashboards] *************************
Saturday 05 June 2021  00:29:27 +0000 (0:00:00.601)       0:15:42.809 ********* 
changed: [node-5] => (item=ceph-cluster.json)
changed: [node-5] => (item=cephfs-overview.json)
changed: [node-5] => (item=host-details.json)
changed: [node-5] => (item=hosts-overview.json)
changed: [node-5] => (item=osd-device-details.json)
changed: [node-5] => (item=osds-overview.json)
changed: [node-5] => (item=pool-detail.json)
changed: [node-5] => (item=pool-overview.json)
changed: [node-5] => (item=radosgw-detail.json)
changed: [node-5] => (item=radosgw-overview.json)
changed: [node-5] => (item=rbd-overview.json)

TASK [ceph-grafana : write grafana.ini] ****************************************
Saturday 05 June 2021  00:29:34 +0000 (0:00:06.885)       0:15:49.694 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
changed: [node-5]

TASK [ceph-grafana : write datasources provisioning config file] ***************
Saturday 05 June 2021  00:29:34 +0000 (0:00:00.202)       0:15:49.897 ********* 
changed: [node-5]

TASK [ceph-grafana : Write dashboards provisioning config file] ****************
Saturday 05 June 2021  00:29:34 +0000 (0:00:00.318)       0:15:50.216 ********* 
changed: [node-5]

TASK [ceph-grafana : copy grafana SSL certificate file] ************************
Saturday 05 June 2021  00:29:35 +0000 (0:00:00.316)       0:15:50.533 ********* 
skipping: [node-5]

TASK [ceph-grafana : copy grafana SSL certificate key] *************************
Saturday 05 June 2021  00:29:35 +0000 (0:00:00.026)       0:15:50.560 ********* 
skipping: [node-5]

TASK [ceph-grafana : generate a Self Signed OpenSSL certificate for dashboard] ***
Saturday 05 June 2021  00:29:35 +0000 (0:00:00.026)       0:15:50.586 ********* 
changed: [node-5]

TASK [ceph-grafana : enable and start grafana] *********************************
Saturday 05 June 2021  00:29:35 +0000 (0:00:00.146)       0:15:50.733 ********* 
changed: [node-5]

TASK [ceph-grafana : wait for grafana to start] ********************************
Saturday 05 June 2021  00:29:35 +0000 (0:00:00.304)       0:15:51.037 ********* 
fatal: [node-5]: FAILED! => changed=false 
  elapsed: 300
  msg: Timeout when waiting for 100.200.23.200:3000

RUNNING HANDLER [ceph-prometheus : service handler] ****************************
Saturday 05 June 2021  00:34:36 +0000 (0:05:00.542)       0:20:51.580 ********* 

PLAY RECAP *********************************************************************
node-1                     : ok=111  changed=17   unreachable=0    failed=0    skipped=280  rescued=0    ignored=0   
node-2                     : ok=123  changed=17   unreachable=0    failed=0    skipped=303  rescued=0    ignored=0   
node-3                     : ok=138  changed=17   unreachable=0    failed=0    skipped=271  rescued=0    ignored=0   
node-4                     : ok=129  changed=17   unreachable=0    failed=0    skipped=264  rescued=0    ignored=0   
node-5                     : ok=106  changed=27   unreachable=0    failed=1    skipped=213  rescued=0    ignored=0   
node-6                     : ok=109  changed=16   unreachable=0    failed=0    skipped=272  rescued=0    ignored=0   


INSTALLER STATUS ***************************************************************
Install Ceph Monitor           : Complete (0:00:40)
Install Ceph Manager           : Complete (0:00:23)
Install Ceph OSD               : Complete (0:01:38)
Install Ceph RGW               : Complete (0:00:09)
Install Ceph Grafana           : In Progress (0:05:21)
	This phase can be restarted by running: roles/ceph-grafana/tasks/main.yml
Install Ceph Node Exporter     : Complete (0:01:18)

Saturday 05 June 2021  00:34:36 +0000 (0:00:00.003)       0:20:51.583 ********* 
=============================================================================== 
ceph-common : install ceph for debian --------------------------------- 588.81s
ceph-grafana : wait for grafana to start ------------------------------ 300.54s
ceph-node-exporter : start the node_exporter service ------------------- 56.97s
ceph-osd : use ceph-volume lvm batch to create bluestore osds ---------- 53.77s
ceph-infra : update cache for Debian based OSs ------------------------- 43.08s
ceph-mon : waiting for the monitor(s) to form the quorum... ------------ 20.45s
gather and delegate facts ---------------------------------------------- 11.93s
ceph-mon : ceph monitor mkfs with keyring ------------------------------ 11.87s
ceph-osd : wait for all osd to be up ----------------------------------- 10.83s
ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created --- 7.16s
ceph-grafana : download ceph grafana dashboards ------------------------- 6.89s
ceph-infra : install chrony --------------------------------------------- 5.73s
ceph-mgr : wait for all mgr to be up ------------------------------------ 5.62s
ceph-config : look up for ceph-volume rejected devices ------------------ 5.03s
ceph-mgr : add modules to ceph-mgr -------------------------------------- 4.45s
ceph-osd : unset noup flag ---------------------------------------------- 3.59s
ceph-container-engine : allow apt to use a repository over https (debian) --- 3.49s
ceph-mgr : disable ceph mgr enabled modules ----------------------------- 3.37s
ceph-container-engine : remove docker proxy configuration --------------- 3.10s
ceph-osd : apply operating system tuning -------------------------------- 2.83s
[WARNING]: Could not match supplied host pattern, ignoring: mdss
[WARNING]: Could not match supplied host pattern, ignoring: nfss
[WARNING]: Could not match supplied host pattern, ignoring: rbdmirrors
[WARNING]: Could not match supplied host pattern, ignoring: clients
[WARNING]: Could not match supplied host pattern, ignoring: iscsigws
[WARNING]: Could not match supplied host pattern, ignoring: iscsi-gws
[WARNING]: Could not match supplied host pattern, ignoring: rgwloadbalancers

PLAY [mons,osds,mdss,rgws,nfss,rbdmirrors,clients,mgrs,iscsigws,iscsi-gws,grafana-server,rgwloadbalancers] ***

TASK [check for python] ********************************************************
Saturday 05 June 2021  00:57:28 +0000 (0:00:00.020)       0:00:00.020 ********* 
ok: [node-2] => (item=/usr/bin/python)
ok: [node-2] => (item=/usr/bin/python3)
ok: [node-2] => (item=/usr/libexec/platform-python)
ok: [node-5] => (item=/usr/bin/python)
ok: [node-5] => (item=/usr/bin/python3)
ok: [node-5] => (item=/usr/libexec/platform-python)
ok: [node-1] => (item=/usr/bin/python)
ok: [node-1] => (item=/usr/bin/python3)
ok: [node-1] => (item=/usr/libexec/platform-python)
ok: [node-6] => (item=/usr/bin/python)
ok: [node-6] => (item=/usr/bin/python3)
ok: [node-6] => (item=/usr/libexec/platform-python)
ok: [node-3] => (item=/usr/bin/python)
ok: [node-3] => (item=/usr/bin/python3)
ok: [node-3] => (item=/usr/libexec/platform-python)
ok: [node-4] => (item=/usr/bin/python)
ok: [node-4] => (item=/usr/bin/python3)
ok: [node-4] => (item=/usr/libexec/platform-python)

TASK [check for dnf-3 package manager (RedHat/Fedora/CentOS)] ******************
Saturday 05 June 2021  00:58:15 +0000 (0:00:47.421)       0:00:47.442 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for yum package manager (RedHat/Fedora/CentOS)] ********************
Saturday 05 June 2021  00:58:15 +0000 (0:00:00.047)       0:00:47.489 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for apt package manager (Debian/Ubuntu)] ***************************
Saturday 05 June 2021  00:58:15 +0000 (0:00:00.049)       0:00:47.538 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [check for zypper package manager (SUSE/OpenSUSE)] ************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.048)       0:00:47.586 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for RedHat based OS - dnf] ********************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.048)       0:00:47.635 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for debian based OS] **************************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.049)       0:00:47.684 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python for SUSE/OpenSUSE] ****************************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.048)       0:00:47.733 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [install python-xml for opensuse only if python2 is installed already] ****
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.050)       0:00:47.784 ********* 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.9586957, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-2] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.958763, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820958.9587927, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622776621.1, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-6] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622820802.4246573, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-1] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/bin/python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': True, 'path': '/usr/bin/python3', 'mode': '0777', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': True, 'issock': False, 'uid': 0, 'gid': 0, 'size': 9, 'inode': 1676, 'dev': 64513, 'nlink': 1, 'atime': 1622776590.044, 'mtime': 1584102020.0, 'ctime': 1615499155.816552, 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': True, 'rgrp': True, 'xgrp': True, 'woth': True, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False, 'blocks': 0, 'block_size': 4096, 'device_type': 0, 'readable': True, 'writeable': True, 'executable': True, 'lnk_source': '/usr/bin/python3.8', 'lnk_target': 'python3.8', 'pw_name': 'root', 'gr_name': 'root', 'mimetype': 'inode/symlink', 'charset': 'binary', 'version': None, 'attributes': [], 'attr_flags': ''}, 'invocation': {'module_args': {'path': '/usr/bin/python3', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/bin/python3', 'ansible_loop_var': 'item'}) 
skipping: [node-5] => (item={'changed': False, 'stat': {'exists': False}, 'invocation': {'module_args': {'path': '/usr/libexec/platform-python', 'follow': False, 'get_md5': False, 'get_checksum': True, 'get_mime': True, 'get_attributes': True, 'checksum_algorithm': 'sha1'}}, 'failed': False, 'failed_when_result': False, 'item': '/usr/libexec/platform-python', 'ansible_loop_var': 'item'}) 

TASK [gather facts] ************************************************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.064)       0:00:47.848 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [gather and delegate facts] ***********************************************
Saturday 05 June 2021  00:58:16 +0000 (0:00:00.070)       0:00:47.919 ********* 
ok: [node-2 -> node-2] => (item=node-2)
ok: [node-2 -> node-3] => (item=node-3)
ok: [node-2 -> node-4] => (item=node-4)
ok: [node-2 -> node-1] => (item=node-1)
ok: [node-2 -> node-5] => (item=node-5)
ok: [node-2 -> node-6] => (item=node-6)

TASK [create filtered clients group] *******************************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:05.747)       0:00:53.666 ********* 

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.014)       0:00:53.681 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.099)       0:00:53.781 ********* 
ok: [node-2]
ok: [node-5]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-3]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.388)       0:00:54.169 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.052)       0:00:54.221 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-5]
ok: [node-4]
ok: [node-1]
ok: [node-6]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.164)       0:00:54.385 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.055)       0:00:54.441 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.048)       0:00:54.489 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:58:22 +0000 (0:00:00.054)       0:00:54.544 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.057)       0:00:54.601 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.023)       0:00:54.624 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.054)       0:00:54.679 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.023)       0:00:54.703 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.288)       0:00:54.991 ********* 
ok: [node-2 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:23.393841', 'end': '2021-06-05 00:58:23.397435', 'delta': '0:00:00.003594', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.155)       0:00:55.147 ********* 
ok: [node-2] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:23.557010', 'end': '2021-06-05 00:58:23.559331', 'delta': '0:00:00.002321', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:23.393841', 'end': '2021-06-05 00:58:23.397435', 'delta': '0:00:00.003594', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.031)       0:00:55.179 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.020)       0:00:55.200 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:58:23 +0000 (0:00:00.057)       0:00:55.257 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.573)       0:00:55.830 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.056)       0:00:55.886 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.077)       0:00:55.964 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.054)       0:00:56.018 ********* 
ok: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.023)       0:00:56.042 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.017)       0:00:56.059 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.052)       0:00:56.111 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)
ok: [node-4] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.297)       0:00:56.409 ********* 
skipping: [node-2]
skipping: [node-6]
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:24.678231', 'end': '2021-06-05 00:58:24.680649', 'delta': '0:00:00.002418', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:24.812490', 'end': '2021-06-05 00:58:24.814155', 'delta': '0:00:00.001665', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:24.684991', 'end': '2021-06-05 00:58:24.687689', 'delta': '0:00:00.002698', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
skipping: [node-5]
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:24.821021', 'end': '2021-06-05 00:58:24.823109', 'delta': '0:00:00.002088', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
skipping: [node-1]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.053)       0:00:56.462 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.049)       0:00:56.512 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:58:24 +0000 (0:00:00.052)       0:00:56.565 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.052)       0:00:56.617 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.050)       0:00:56.667 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.048)       0:00:56.716 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.050)       0:00:56.766 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.051)       0:00:56.817 ********* 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f', 'dm-uuid-LVM-CYEC8gYUYt5QpQ4oUzfMuChDzu4C08SuRy3HxwPWTGzzSUwe2aIsTgJHCgRSM7yy'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-Z2dsTJ-RsHU-C6fc-xs0J-SWeX-Vsro-5R3XKz', 'virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b']}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1', 'dm-uuid-LVM-ODi5ZHZixQRub4roCEQf80hIkZvkXL19ETU6V1Iz0o9y8mX2PVP24caP5mKo3Mnp'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-20K1ns-E8SS-lCWr-dcq5-ZVEp-Ewqd-LSHxhM', 'virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89']}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b', 'dm-uuid-LVM-WAyknBsxMCdbeX0sR6xw65J36xZQesl7KZJqoEGujwFgVUavmYHV8kj6BEfUzDpz'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89', 'dm-uuid-LVM-4z7BLxcq21Gx1yBiuNOQYMSpCp1AowddWA7Hwq50FevkMbuVw32h4QCQgU33hN9e'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-K91yj5-Bejm-TEbe-XoUO-QzP6-U2VG-H51VvS', 'virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1']}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-43OiAt-Qti1-3oY8-yL5p-9hZA-lRhN-bO1c07', 'virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f']}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.130)       0:00:56.947 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.017)       0:00:56.965 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.016)       0:00:56.981 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.052)       0:00:57.033 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.186)       0:00:57.220 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.052)       0:00:57.273 ********* 
skipping: [node-5]
ok: [node-3]
ok: [node-2]
ok: [node-4]
ok: [node-6]
ok: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.168)       0:00:57.442 ********* 
ok: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:58:25 +0000 (0:00:00.054)       0:00:57.497 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.246)       0:00:57.743 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.057)       0:00:57.801 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.057)       0:00:57.858 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.063)       0:00:57.922 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.058)       0:00:57.981 ********* 
ok: [node-2] => (item=node-2)
ok: [node-3] => (item=node-2)
ok: [node-4] => (item=node-2)
ok: [node-1] => (item=node-2)
ok: [node-6] => (item=node-2)
ok: [node-5] => (item=node-2)

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.089)       0:00:58.071 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.053)       0:00:58.124 ********* 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.060)       0:00:58.185 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.053)       0:00:58.239 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.052)       0:00:58.291 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
ok: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.140)       0:00:58.432 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.052)       0:00:58.484 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:58:26 +0000 (0:00:00.052)       0:00:58.537 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.053)       0:00:58.590 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
ok: [node-6] => (item=0)
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.056)       0:00:58.647 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.054)       0:00:58.702 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-6] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.056)       0:00:58.758 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.056)       0:00:58.815 ********* 
skipping: [node-2] => (item=node-6) 
skipping: [node-3] => (item=node-6) 
skipping: [node-4] => (item=node-6) 
skipping: [node-6] => (item=node-6) 
skipping: [node-1] => (item=node-6) 
skipping: [node-5] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.060)       0:00:58.876 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.061)       0:00:58.937 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.028)       0:00:58.965 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-validate : include check_system.yml] ********************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.021)       0:00:58.987 ********* 
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_system.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-validate : fail on unsupported ansible version (1.X)] ***************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.073)       0:00:59.060 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported ansible version] *********************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.052)       0:00:59.112 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported system] ******************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.052)       0:00:59.165 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported architecture] ************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.053)       0:00:59.219 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution] ************************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.054)       0:00:59.273 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported CentOS release] **********************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.052)       0:00:59.326 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution for red hat ceph storage] ***
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.053)       0:00:59.379 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : determine if node is registered with subscription-manager] ***
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.053)       0:00:59.432 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unregistered red hat rhcs linux] *****************
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.053)       0:00:59.486 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported distribution for ubuntu cloud archive] ***
Saturday 05 June 2021  00:58:27 +0000 (0:00:00.054)       0:00:59.540 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail on unsupported SUSE/openSUSE distribution (only 15.x supported)] ***
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.053)       0:00:59.593 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if systemd is not present] **************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.054)       0:00:59.648 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_origin] ************************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.053)       0:00:59.702 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository] ********************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.053)       0:00:59.755 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository_community] **********************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.055)       0:00:59.811 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ceph_repository_type] ***************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.054)       0:00:59.865 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate osd_objectstore] ********************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.055)       0:00:59.921 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate monitor network configuration] ******************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.053)       0:00:59.974 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate radosgw network configuration] ******************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.052)       0:01:00.027 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate lvm osd scenario] *******************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.052)       0:01:00.079 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate filestore lvm osd scenario] *********************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.051)       0:01:00.130 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate bluestore lvm osd scenario] *********************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.050)       0:01:00.181 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if local scenario is enabled on debian] *************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.052)       0:01:00.233 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if rhcs repository is enabled on debian] ************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.052)       0:01:00.286 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : Check ceph_origin definition on SUSE/openSUSE Leap] ******
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.054)       0:01:00.341 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : Check ceph_repository definition on SUSE/openSUSE Leap] ***
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.051)       0:01:00.393 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate ntp daemon type] ********************************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.051)       0:01:00.444 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : abort if ntp_daemon_type is ntpd on Atomic] **************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.050)       0:01:00.495 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : make sure journal_size configured] ***********************
Saturday 05 June 2021  00:58:28 +0000 (0:00:00.052)       0:01:00.547 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_devices.yml] *******************************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.053)       0:01:00.601 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_devices.yml for node-3, node-4

TASK [ceph-validate : find device used for operating system] *******************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.080)       0:01:00.681 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-validate : resolve root_device] *************************************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.150)       0:01:00.832 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-validate : set_fact root_device] ************************************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.149)       0:01:00.981 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-validate : resolve devices in lvm_volumes] **************************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.023)       0:01:01.004 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : set_fact lvm_volumes_data_devices] ***********************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.021)       0:01:01.026 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : resolve devices in devices] ******************************
Saturday 05 June 2021  00:58:29 +0000 (0:00:00.022)       0:01:01.048 ********* 
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)
ok: [node-4] => (item=/dev/vdc)

TASK [ceph-validate : set_fact devices_resolved] *******************************
Saturday 05 June 2021  00:58:30 +0000 (0:00:01.350)       0:01:02.399 ********* 
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:29.604715', 'end': '2021-06-05 00:58:30.607474', 'delta': '0:00:01.002759', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:29.611543', 'end': '2021-06-05 00:58:29.613212', 'delta': '0:00:00.001669', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:30.733929', 'end': '2021-06-05 00:58:30.735642', 'delta': '0:00:00.001713', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:58:29.810211', 'end': '2021-06-05 00:58:30.813397', 'delta': '0:00:01.003186', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})

TASK [ceph-validate : fail if root_device is passed in lvm_volumes or devices] ***
Saturday 05 June 2021  00:58:30 +0000 (0:00:00.041)       0:01:02.440 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-validate : read information about the devices] **********************
Saturday 05 June 2021  00:58:30 +0000 (0:00:00.021)       0:01:02.462 ********* 
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-validate : fail when gpt header found on osd devices] ***************
Saturday 05 June 2021  00:58:32 +0000 (0:00:01.861)       0:01:04.324 ********* 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 

TASK [ceph-validate : get devices information] *********************************
Saturday 05 June 2021  00:58:32 +0000 (0:00:00.030)       0:01:04.354 ********* 
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-validate : fail if one of the devices is not a device] **************
Saturday 05 June 2021  00:58:34 +0000 (0:00:01.879)       0:01:06.234 ********* 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-3] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdb', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdb', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'}) 
skipping: [node-4] => (item={'changed': False, 'disk': {'dev': '/dev/vdc', 'size': 20480.0, 'unit': 'mib', 'table': 'unknown', 'model': 'Virtio Block Device', 'logical_block': 512, 'physical_block': 512}, 'partitions': [], 'script': "unit 'MiB' print", 'invocation': {'module_args': {'device': '/dev/vdc', 'unit': 'MiB', 'align': 'optimal', 'label': 'msdos', 'part_type': 'primary', 'part_start': '0%', 'part_end': '100%', 'state': 'info', 'number': None, 'name': None, 'flags': None}}, 'failed': False, 'failed_when_result': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'}) 

TASK [ceph-validate : include check_eth_mon.yml] *******************************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.029)       0:01:06.263 ********* 
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
included: /opt/ceph-ansible/roles/ceph-validate/tasks/check_eth_mon.yml for node-2

TASK [ceph-validate : fail if ens3 does not exist on node-2] *******************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.088)       0:01:06.352 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 is not active on node-2] ********************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.017)       0:01:06.369 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 does not have any ip v4 address on node-2] ***
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.023)       0:01:06.393 ********* 
skipping: [node-2]

TASK [ceph-validate : fail if ens3 does not have any ip v6 address on node-2] ***
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.023)       0:01:06.417 ********* 
skipping: [node-2]

TASK [ceph-validate : include check_ipaddr_mon.yml] ****************************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.015)       0:01:06.432 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_eth_rgw.yml] *******************************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.049)       0:01:06.482 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rgw_pools.yml] *****************************
Saturday 05 June 2021  00:58:34 +0000 (0:00:00.050)       0:01:06.532 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rgw_multisite.yml] *************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.052)       0:01:06.584 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_iscsi.yml] *********************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.051)       0:01:06.636 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : warn about radosgw_civetweb_num_threads option deprecation] ***
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.051)       0:01:06.688 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_nfs.yml] ***********************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.048)       0:01:06.736 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : include check_rbdmirror.yml] *****************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.054)       0:01:06.790 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail if [grafana-server] group doesn't exist] ************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.048)       0:01:06.839 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail when [grafana-server] doesn't contain at least one node.] ***
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.052)       0:01:06.891 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : fail when dashboard_admin_password and/or grafana_admin_password are not set] ***
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.052)       0:01:06.944 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate container registry credentials] *****************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.048)       0:01:06.993 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate openstack_keys key format] **********************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.050)       0:01:07.044 ********* 
skipping: [node-2] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 

TASK [ceph-validate : validate clients keys key format] ************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.100)       0:01:07.144 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate openstack_keys caps] ****************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.053)       0:01:07.198 ********* 
skipping: [node-2] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-2] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-1] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-6] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.glance', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.cinder-backup', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-5] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.gnocchi', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=metrics'}, 'mode': '0600'}) 
skipping: [node-4] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 
skipping: [node-3] => (item={'name': 'client.openstack', 'caps': {'mon': 'profile rbd', 'osd': 'profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups'}, 'mode': '0600'}) 

TASK [ceph-validate : validate clients keys caps] ******************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.100)       0:01:07.299 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : check virtual_ips is defined] ****************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.052)       0:01:07.352 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-validate : validate virtual_ips length] *****************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.051)       0:01:07.403 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : update cache for Debian based OSs] **************************
Saturday 05 June 2021  00:58:35 +0000 (0:00:00.050)       0:01:07.454 ********* 
changed: [node-1]
changed: [node-3]
changed: [node-6]
changed: [node-2]
changed: [node-5]
changed: [node-4]

TASK [ceph-infra : include_tasks configure_firewall.yml] ***********************
Saturday 05 June 2021  00:58:46 +0000 (0:00:10.273)       0:01:17.727 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : include_tasks setup_ntp.yml] ********************************
Saturday 05 June 2021  00:58:46 +0000 (0:00:00.053)       0:01:17.780 ********* 
included: /opt/ceph-ansible/roles/ceph-infra/tasks/setup_ntp.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-infra : set ntp service and chrony daemon name for Debian family] ***
Saturday 05 June 2021  00:58:46 +0000 (0:00:00.081)       0:01:17.862 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-infra : set ntp service and chrony daemon name for RedHat and Suse family] ***
Saturday 05 June 2021  00:58:46 +0000 (0:00:00.055)       0:01:17.917 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : install ntpd] ***********************************************
Saturday 05 June 2021  00:58:46 +0000 (0:00:00.054)       0:01:17.972 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : install chrony] *********************************************
Saturday 05 June 2021  00:58:46 +0000 (0:00:00.053)       0:01:18.025 ********* 
ok: [node-4]
ok: [node-1]
ok: [node-5]
ok: [node-2]
ok: [node-3]
ok: [node-6]

TASK [ceph-infra : enable timesyncing on timesyncd] ****************************
Saturday 05 June 2021  00:59:02 +0000 (0:00:15.744)       0:01:33.770 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : disable time sync using timesyncd if we are not using it] ***
Saturday 05 June 2021  00:59:02 +0000 (0:00:00.051)       0:01:33.821 ********* 
changed: [node-3]
changed: [node-5]
changed: [node-6]
changed: [node-1]
changed: [node-4]
changed: [node-2]

TASK [ceph-infra : enable ntpd] ************************************************
Saturday 05 June 2021  00:59:03 +0000 (0:00:01.510)       0:01:35.332 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : enable chronyd] *********************************************
Saturday 05 June 2021  00:59:03 +0000 (0:00:00.055)       0:01:35.388 ********* 
changed: [node-6]
changed: [node-4]
changed: [node-1]
changed: [node-5]
changed: [node-2]
changed: [node-3]

TASK [ceph-infra : ensure logrotate is installed] ******************************
Saturday 05 June 2021  00:59:20 +0000 (0:00:16.411)       0:01:51.799 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-infra : add logrotate configuration] ********************************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.053)       0:01:51.852 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_redhat.yml] **************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.052)       0:01:51.905 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_suse.yml] ****************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.053)       0:01:51.959 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include installs/install_on_debian.yml] ********************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.052)       0:01:52.011 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/install_on_debian.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : include configure_debian_repository_installation.yml] ******
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.081)       0:01:52.093 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/configure_debian_repository_installation.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : include debian_community_repository.yml] *******************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.080)       0:01:52.173 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/debian_community_repository.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : install dependencies for apt modules] **********************
Saturday 05 June 2021  00:59:20 +0000 (0:00:00.080)       0:01:52.254 ********* 
ok: [node-5]
ok: [node-6]
ok: [node-4]
ok: [node-3]
ok: [node-2]
ok: [node-1]

TASK [ceph-common : configure debian ceph community repository stable key] *****
Saturday 05 June 2021  00:59:26 +0000 (0:00:05.834)       0:01:58.089 ********* 
ok: [node-5]
ok: [node-3]
ok: [node-1]
ok: [node-6]
ok: [node-2]
ok: [node-4]

TASK [ceph-common : configure debian ceph stable community repository] *********
Saturday 05 June 2021  00:59:31 +0000 (0:00:05.274)       0:02:03.364 ********* 
ok: [node-3]
ok: [node-2]
ok: [node-1]
ok: [node-4]
ok: [node-5]
ok: [node-6]

TASK [ceph-common : include debian_dev_repository.yml] *************************
Saturday 05 June 2021  00:59:33 +0000 (0:00:01.504)       0:02:04.868 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include debian_custom_repository.yml] **********************
Saturday 05 June 2021  00:59:33 +0000 (0:00:00.052)       0:02:04.920 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include debian_uca_repository.yml] *************************
Saturday 05 June 2021  00:59:33 +0000 (0:00:00.048)       0:02:04.968 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : update apt cache if cache_valid_time has expired] **********
Saturday 05 June 2021  00:59:33 +0000 (0:00:00.051)       0:02:05.020 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-5]
ok: [node-6]
ok: [node-4]
ok: [node-1]

TASK [ceph-common : install dependencies] **************************************
Saturday 05 June 2021  00:59:33 +0000 (0:00:00.500)       0:02:05.520 ********* 
ok: [node-3]
ok: [node-5]
ok: [node-4]
ok: [node-2]
ok: [node-6]
ok: [node-1]

TASK [ceph-common : include install_debian_packages.yml] ***********************
Saturday 05 June 2021  00:59:34 +0000 (0:00:00.555)       0:02:06.076 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/installs/install_debian_packages.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : install ceph for debian] ***********************************
Saturday 05 June 2021  00:59:34 +0000 (0:00:00.087)       0:02:06.163 ********* 
ok: [node-5]
ok: [node-3]
ok: [node-6]
ok: [node-1]
ok: [node-2]
ok: [node-4]

TASK [ceph-common : include install_debian_rhcs_packages.yml] ******************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.747)       0:02:06.911 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include_tasks installs/install_on_clear.yml] ***************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.051)       0:02:06.962 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : get ceph version] ******************************************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.049)       0:02:07.012 ********* 
ok: [node-2]
ok: [node-6]
ok: [node-3]
ok: [node-5]
ok: [node-1]
ok: [node-4]

TASK [ceph-common : set_fact ceph_version] *************************************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.230)       0:02:07.243 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-common : include release-rhcs.yml] **********************************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.216)       0:02:07.459 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : set_fact ceph_release - override ceph_release with ceph_stable_release] ***
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.051)       0:02:07.511 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-common : include create_rbd_client_dir.yml] *************************
Saturday 05 June 2021  00:59:35 +0000 (0:00:00.053)       0:02:07.564 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/create_rbd_client_dir.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : create rbd client directory] *******************************
Saturday 05 June 2021  00:59:36 +0000 (0:00:00.089)       0:02:07.653 ********* 
ok: [node-4] => (item=/var/run/ceph)
changed: [node-6] => (item=/var/run/ceph)
ok: [node-5] => (item=/var/run/ceph)
ok: [node-1] => (item=/var/run/ceph)
ok: [node-3] => (item=/var/run/ceph)
ok: [node-5] => (item=/var/log/ceph)
ok: [node-6] => (item=/var/log/ceph)
ok: [node-1] => (item=/var/log/ceph)
ok: [node-4] => (item=/var/log/ceph)
ok: [node-3] => (item=/var/log/ceph)
ok: [node-2] => (item=/var/run/ceph)
ok: [node-2] => (item=/var/log/ceph)

TASK [ceph-common : include configure_cluster_name.yml] ************************
Saturday 05 June 2021  00:59:40 +0000 (0:00:04.079)       0:02:11.733 ********* 
included: /opt/ceph-ansible/roles/ceph-common/tasks/configure_cluster_name.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-common : configure cluster name] ************************************
Saturday 05 June 2021  00:59:40 +0000 (0:00:00.093)       0:02:11.827 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : check /etc/default/ceph exist] *****************************
Saturday 05 June 2021  00:59:40 +0000 (0:00:00.054)       0:02:11.882 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-1]
ok: [node-5]
ok: [node-2]
ok: [node-6]

TASK [ceph-common : when /etc/default/ceph is not dir] *************************
Saturday 05 June 2021  00:59:41 +0000 (0:00:01.609)       0:02:13.491 ********* 
ok: [node-5]
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-1]
ok: [node-6]

TASK [ceph-common : when /etc/default/ceph is dir] *****************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.278)       0:02:13.770 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include configure_memory_allocator.yml] ********************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.055)       0:02:13.826 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-common : include selinux.yml] ***************************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.053)       0:02:13.879 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

RUNNING HANDLER [ceph-infra : disable ntpd] ************************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.050)       0:02:13.930 ********* 
ok: [node-5]
ok: [node-4]
ok: [node-1]
ok: [node-6]
ok: [node-2]
ok: [node-3]

RUNNING HANDLER [ceph-infra : disable timesyncd] *******************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.280)       0:02:14.210 ********* 
ok: [node-4]
ok: [node-6]
ok: [node-5]
ok: [node-1]
ok: [node-2]
ok: [node-3]

PLAY [mons] ********************************************************************

TASK [set ceph monitor install 'In Progress'] **********************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.289)       0:02:14.499 ********* 
ok: [node-2]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.023)       0:02:14.522 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:59:42 +0000 (0:00:00.045)       0:02:14.568 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.132)       0:02:14.700 ********* 
ok: [node-2]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.016)       0:02:14.717 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.152)       0:02:14.870 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.020)       0:02:14.890 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.015)       0:02:14.905 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.014)       0:02:14.920 ********* 
ok: [node-2]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.021)       0:02:14.941 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.024)       0:02:14.966 ********* 
skipping: [node-2]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.020)       0:02:14.987 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.022)       0:02:15.010 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.158)       0:02:15.168 ********* 
ok: [node-2 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:43.570308', 'end': '2021-06-05 00:59:43.573622', 'delta': '0:00:00.003314', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.167)       0:02:15.335 ********* 
ok: [node-2] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:43.746679', 'end': '2021-06-05 00:59:43.748771', 'delta': '0:00:00.002092', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:43.570308', 'end': '2021-06-05 00:59:43.573622', 'delta': '0:00:00.003314', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.030)       0:02:15.366 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.019)       0:02:15.385 ********* 
skipping: [node-2]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:59:43 +0000 (0:00:00.023)       0:02:15.409 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.557)       0:02:15.966 ********* 
skipping: [node-2]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.020)       0:02:15.987 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.016)       0:02:16.003 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.014)       0:02:16.017 ********* 
ok: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.022)       0:02:16.040 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.018)       0:02:16.058 ********* 
skipping: [node-2]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.016)       0:02:16.074 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.090 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.017)       0:02:16.107 ********* 
skipping: [node-2]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.123 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.139 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.016)       0:02:16.155 ********* 
skipping: [node-2]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.016)       0:02:16.171 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.187 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.203 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.014)       0:02:16.218 ********* 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.077)       0:02:16.295 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.017)       0:02:16.313 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.020)       0:02:16.333 ********* 
skipping: [node-2]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.015)       0:02:16.349 ********* 
ok: [node-2]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:59:44 +0000 (0:00:00.203)       0:02:16.552 ********* 
ok: [node-2]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.018)       0:02:16.570 ********* 
ok: [node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.142)       0:02:16.713 ********* 
ok: [node-2]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.024)       0:02:16.738 ********* 
skipping: [node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.017)       0:02:16.756 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.016)       0:02:16.772 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.020)       0:02:16.793 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.021)       0:02:16.814 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.020)       0:02:16.835 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.019)       0:02:16.854 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.021)       0:02:16.875 ********* 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.023)       0:02:16.899 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:16.915 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:16.930 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:16.946 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:16.962 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.014)       0:02:16.977 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:16.992 ********* 
skipping: [node-2] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.019)       0:02:17.011 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.014)       0:02:17.026 ********* 
skipping: [node-2] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.022)       0:02:17.049 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.017)       0:02:17.066 ********* 
skipping: [node-2] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.018)       0:02:17.085 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.024)       0:02:17.109 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.027)       0:02:17.137 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.022)       0:02:17.159 ********* 
skipping: [node-2]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.015)       0:02:17.175 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-2

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.031)       0:02:17.206 ********* 
ok: [node-2]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:59:45 +0000 (0:00:00.276)       0:02:17.483 ********* 
ok: [node-2] => (item={'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.149)       0:02:17.632 ********* 
skipping: [node-2] => (item=[{'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:46.043381', 'end': '2021-06-05 00:59:46.045872', 'delta': '0:00:00.002491', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.026)       0:02:17.659 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.675 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.032)       0:02:17.707 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.724 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.739 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.755 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.018)       0:02:17.773 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.789 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:17.806 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.823 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.838 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.855 ********* 
skipping: [node-2]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.872 ********* 
skipping: [node-2]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.888 ********* 
skipping: [node-2]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:17.904 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.020)       0:02:17.925 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.941 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.956 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.972 ********* 
skipping: [node-2]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:17.988 ********* 
ok: [node-2]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.158)       0:02:18.147 ********* 
ok: [node-2]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.027)       0:02:18.174 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:18.190 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:18.206 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:18.221 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:18.238 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:18.254 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.271 ********* 
ok: [node-2]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.022)       0:02:18.293 ********* 
skipping: [node-2]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.311 ********* 
skipping: [node-2]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.328 ********* 
skipping: [node-2]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.346 ********* 
skipping: [node-2]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.364 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.016)       0:02:18.380 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.015)       0:02:18.395 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.413 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.430 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.448 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.466 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.018)       0:02:18.484 ********* 
skipping: [node-2]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.018)       0:02:18.502 ********* 
skipping: [node-2]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.020)       0:02:18.522 ********* 
skipping: [node-2]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.017)       0:02:18.540 ********* 
skipping: [node-2]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:59:46 +0000 (0:00:00.020)       0:02:18.561 ********* 
skipping: [node-2]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.019)       0:02:18.581 ********* 
skipping: [node-2]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.084)       0:02:18.666 ********* 
skipping: [node-2]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.682 ********* 
skipping: [node-2]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.698 ********* 
skipping: [node-2]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.014)       0:02:18.712 ********* 
skipping: [node-2]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.728 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.744 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.013)       0:02:18.758 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.016)       0:02:18.774 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.016)       0:02:18.791 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.806 ********* 
skipping: [node-2]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.016)       0:02:18.822 ********* 
skipping: [node-2]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.016)       0:02:18.839 ********* 
skipping: [node-2]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:18.854 ********* 
ok: [node-2]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.144)       0:02:18.999 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-2]

TASK [ceph-mon : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.393)       0:02:19.392 ********* 
skipping: [node-2]

TASK [ceph-mon : include deploy_monitors.yml] **********************************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.015)       0:02:19.408 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/deploy_monitors.yml for node-2

TASK [ceph-mon : check if monitor initial keyring already exists] **************
Saturday 05 June 2021  00:59:47 +0000 (0:00:00.032)       0:02:19.440 ********* 
ok: [node-2 -> node-2]

TASK [ceph-mon : generate monitor initial keyring] *****************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.510)       0:02:19.950 ********* 
skipping: [node-2]

TASK [ceph-mon : get initial keyring when it already exists] *******************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.017)       0:02:19.968 ********* 
ok: [node-2]

TASK [ceph-mon : create monitor initial keyring] *******************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.021)       0:02:19.989 ********* 
changed: [node-2]

TASK [ceph-mon : copy the initial key in /etc/ceph (for containers)] ***********
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.169)       0:02:20.158 ********* 
skipping: [node-2]

TASK [ceph-mon : create monitor directory] *************************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.017)       0:02:20.176 ********* 
ok: [node-2]

TASK [ceph-mon : recursively fix ownership of monitor directory] ***************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.151)       0:02:20.327 ********* 
ok: [node-2]

TASK [ceph-mon : create custom admin keyring] **********************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.147)       0:02:20.475 ********* 
skipping: [node-2]

TASK [ceph-mon : set_fact ceph-authtool container command] *********************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.016)       0:02:20.492 ********* 
ok: [node-2]

TASK [ceph-mon : import admin keyring into mon keyring] ************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.018)       0:02:20.510 ********* 
skipping: [node-2]

TASK [ceph-mon : set_fact ceph-mon container command] **************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.015)       0:02:20.526 ********* 
ok: [node-2]

TASK [ceph-mon : ceph monitor mkfs with keyring] *******************************
Saturday 05 June 2021  00:59:48 +0000 (0:00:00.018)       0:02:20.544 ********* 
ok: [node-2]

TASK [ceph-mon : ceph monitor mkfs without keyring] ****************************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.205)       0:02:20.750 ********* 
skipping: [node-2]

TASK [ceph-mon : include start_monitor.yml] ************************************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.016)       0:02:20.766 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/start_monitor.yml for node-2

TASK [ceph-mon : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.022)       0:02:20.789 ********* 
skipping: [node-2]

TASK [ceph-mon : add ceph-mon systemd service overrides] ***********************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.017)       0:02:20.807 ********* 
skipping: [node-2]

TASK [ceph-mon : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.016)       0:02:20.823 ********* 
skipping: [node-2]

TASK [ceph-mon : start the monitor service] ************************************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.015)       0:02:20.839 ********* 
ok: [node-2]

TASK [ceph-mon : include_tasks ceph_keys.yml] **********************************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.439)       0:02:21.278 ********* 
included: /opt/ceph-ansible/roles/ceph-mon/tasks/ceph_keys.yml for node-2

TASK [ceph-mon : waiting for the monitor(s) to form the quorum...] *************
Saturday 05 June 2021  00:59:49 +0000 (0:00:00.028)       0:02:21.306 ********* 
ok: [node-2]

TASK [ceph-mon : fetch ceph initial keys] **************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.472)       0:02:21.778 ********* 
ok: [node-2]

TASK [ceph-mon : include secure_cluster.yml] ***********************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.675)       0:02:22.454 ********* 
skipping: [node-2]

TASK [ceph-mgr : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.019)       0:02:22.473 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-mgr : include common.yml] *******************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.017)       0:02:22.491 ********* 
skipping: [node-2]

TASK [ceph-mgr : include pre_requisite.yml] ************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.015)       0:02:22.507 ********* 
skipping: [node-2]

TASK [ceph-mgr : include start_mgr.yml] ****************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.016)       0:02:22.524 ********* 
skipping: [node-2]

TASK [ceph-mgr : include mgr_modules.yml] **************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.015)       0:02:22.539 ********* 
skipping: [node-2]

TASK [set ceph monitor install 'Complete'] *************************************
Saturday 05 June 2021  00:59:50 +0000 (0:00:00.015)       0:02:22.555 ********* 
ok: [node-2]

PLAY [mgrs] ********************************************************************

TASK [set ceph manager install 'In Progress'] **********************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.029)       0:02:22.585 ********* 
ok: [node-1]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.022)       0:02:22.607 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-1

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.045)       0:02:22.653 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.137)       0:02:22.791 ********* 
ok: [node-1]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.019)       0:02:22.810 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.135)       0:02:22.946 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.020)       0:02:22.966 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.016)       0:02:22.982 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.016)       0:02:22.999 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.020)       0:02:23.020 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.027)       0:02:23.048 ********* 
skipping: [node-1]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.022)       0:02:23.070 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.022)       0:02:23.093 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.212)       0:02:23.306 ********* 
ok: [node-1 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:51.651801', 'end': '2021-06-05 00:59:51.654238', 'delta': '0:00:00.002437', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.192)       0:02:23.498 ********* 
ok: [node-1] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:51.870950', 'end': '2021-06-05 00:59:51.873498', 'delta': '0:00:00.002548', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:51.651801', 'end': '2021-06-05 00:59:51.654238', 'delta': '0:00:00.002437', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  00:59:51 +0000 (0:00:00.030)       0:02:23.529 ********* 
skipping: [node-1] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  00:59:52 +0000 (0:00:00.354)       0:02:23.883 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  00:59:52 +0000 (0:00:00.023)       0:02:23.906 ********* 
ok: [node-1 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:01.169)       0:02:25.075 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.019)       0:02:25.095 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.111 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.016)       0:02:25.127 ********* 
ok: [node-1]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.022)       0:02:25.149 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.017)       0:02:25.166 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.016)       0:02:25.183 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.014)       0:02:25.198 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.214 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.014)       0:02:25.229 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.245 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.261 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.014)       0:02:25.275 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.014)       0:02:25.290 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.306 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.014)       0:02:25.320 ********* 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.068)       0:02:25.389 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.018)       0:02:25.407 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.423 ********* 
skipping: [node-1]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  00:59:53 +0000 (0:00:00.015)       0:02:25.438 ********* 
ok: [node-1]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.145)       0:02:25.583 ********* 
ok: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:25.598 ********* 
ok: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.141)       0:02:25.740 ********* 
skipping: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:25.760 ********* 
skipping: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.017)       0:02:25.778 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.017)       0:02:25.795 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:25.815 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:25.836 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:25.856 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.019)       0:02:25.876 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:25.896 ********* 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.019)       0:02:25.916 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:25.931 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:25.946 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:25.962 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.016)       0:02:25.978 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:25.993 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:26.008 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.018)       0:02:26.027 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:26.042 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:26.063 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.020)       0:02:26.083 ********* 
skipping: [node-1] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.017)       0:02:26.101 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.023)       0:02:26.124 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.028)       0:02:26.153 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.022)       0:02:26.175 ********* 
skipping: [node-1]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.016)       0:02:26.191 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-1

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.028)       0:02:26.219 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.013)       0:02:26.233 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:26.249 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.016)       0:02:26.266 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:26.280 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.018)       0:02:26.299 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.017)       0:02:26.316 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.013)       0:02:26.330 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.016)       0:02:26.346 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:26.361 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.014)       0:02:26.376 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.015)       0:02:26.391 ********* 
skipping: [node-1]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.016)       0:02:26.408 ********* 
ok: [node-1]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  00:59:54 +0000 (0:00:00.139)       0:02:26.547 ********* 
ok: [node-1] => (item={'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 679, 'dev': 26, 'nlink': 1, 'atime': 1622852769.8499513, 'mtime': 1622852769.8499513, 'ctime': 1622852769.8499513, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.147)       0:02:26.695 ********* 
skipping: [node-1] => (item=[{'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 679, 'dev': 26, 'nlink': 1, 'atime': 1622852769.8499513, 'mtime': 1622852769.8499513, 'ctime': 1622852769.8499513, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-mgr.ceph-research-2.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 00:59:55.104219', 'end': '2021-06-05 00:59:55.106894', 'delta': '0:00:00.002675', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mgr.ceph-research-2.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 679, 'dev': 26, 'nlink': 1, 'atime': 1622852769.8499513, 'mtime': 1622852769.8499513, 'ctime': 1622852769.8499513, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.026)       0:02:26.722 ********* 
skipping: [node-1]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:26.737 ********* 
skipping: [node-1]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:26.754 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:26.772 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:26.787 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:26.803 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:26.819 ********* 
skipping: [node-1]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:26.835 ********* 
ok: [node-1]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.162)       0:02:26.998 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.014 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.031 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.046 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.014)       0:02:27.061 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.076 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.092 ********* 
ok: [node-1]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.019)       0:02:27.112 ********* 
ok: [node-1]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.022)       0:02:27.134 ********* 
skipping: [node-1]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.150 ********* 
skipping: [node-1]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.167 ********* 
skipping: [node-1]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.018)       0:02:27.185 ********* 
skipping: [node-1]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.202 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.219 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.014)       0:02:27.234 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.251 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.268 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.285 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.019)       0:02:27.305 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.322 ********* 
skipping: [node-1]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.339 ********* 
skipping: [node-1]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.018)       0:02:27.357 ********* 
skipping: [node-1]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.375 ********* 
skipping: [node-1]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.393 ********* 
skipping: [node-1]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.016)       0:02:27.410 ********* 
skipping: [node-1]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.017)       0:02:27.427 ********* 
skipping: [node-1]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.443 ********* 
skipping: [node-1]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.458 ********* 
skipping: [node-1]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.474 ********* 
skipping: [node-1]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.490 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.505 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.014)       0:02:27.520 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.535 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.551 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  00:59:55 +0000 (0:00:00.015)       0:02:27.567 ********* 
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.019)       0:02:27.586 ********* 
skipping: [node-1]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.015)       0:02:27.601 ********* 
skipping: [node-1]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.016)       0:02:27.618 ********* 
ok: [node-1]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.144)       0:02:27.762 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-1]

TASK [ceph-mgr : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.255)       0:02:28.018 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-mgr : include common.yml] *******************************************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.017)       0:02:28.035 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/common.yml for node-1

TASK [ceph-mgr : create mgr directory] *****************************************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.022)       0:02:28.057 ********* 
ok: [node-1]

TASK [ceph-mgr : fetch ceph mgr keyring] ***************************************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.146)       0:02:28.203 ********* 
skipping: [node-1]

TASK [ceph-mgr : create ceph mgr keyring(s) on a mon node] *********************
Saturday 05 June 2021  00:59:56 +0000 (0:00:00.022)       0:02:28.226 ********* 
ok: [node-1 -> node-2] => (item=node-1)

TASK [ceph-mgr : set_fact _mgr_keys] *******************************************
Saturday 05 June 2021  00:59:57 +0000 (0:00:00.832)       0:02:29.058 ********* 
ok: [node-1]

TASK [ceph-mgr : get keys from monitors] ***************************************
Saturday 05 June 2021  00:59:57 +0000 (0:00:00.028)       0:02:29.086 ********* 
skipping: [node-1] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': 'copy_admin_key'}) 
changed: [node-1 -> node-2] => (item={'name': 'mgr.ceph-research-2', 'path': '/var/lib/ceph/mgr/ceph-ceph-research-2/keyring', 'copy_key': True})

TASK [ceph-mgr : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  00:59:57 +0000 (0:00:00.436)       0:02:29.522 ********* 
skipping: [node-1] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': 'copy_admin_key'}, 'ansible_loop_var': 'item'}) 
ok: [node-1] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'mgr.ceph-research-2'], 'stdout': '[mgr.ceph-research-2]\n\tkey = AQCPxLpgAAAAABAAlSxyFSComY9Kn7wTL84/hQ==\n\tcaps mds = "allow *"\n\tcaps mon = "allow profile mgr"\n\tcaps osd = "allow *"', 'stderr': 'exported keyring for mgr.ceph-research-2', 'rc': 0, 'start': '2021-06-05 00:59:57.653083', 'end': '2021-06-05 00:59:57.936236', 'delta': '0:00:00.283153', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get mgr.ceph-research-2', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[mgr.ceph-research-2]', '\tkey = AQCPxLpgAAAAABAAlSxyFSComY9Kn7wTL84/hQ==', '\tcaps mds = "allow *"', '\tcaps mon = "allow profile mgr"', '\tcaps osd = "allow *"'], 'stderr_lines': ['exported keyring for mgr.ceph-research-2'], 'failed': False, 'item': {'name': 'mgr.ceph-research-2', 'path': '/var/lib/ceph/mgr/ceph-ceph-research-2/keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})

TASK [ceph-mgr : set mgr key permissions] **************************************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.340)       0:02:29.863 ********* 
ok: [node-1]

TASK [ceph-mgr : include pre_requisite.yml] ************************************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.152)       0:02:30.016 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/pre_requisite.yml for node-1

TASK [ceph-mgr : set_fact ceph_mgr_packages for sso] ***************************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.025)       0:02:30.041 ********* 
skipping: [node-1]

TASK [ceph-mgr : set_fact ceph_mgr_packages for dashboard] *********************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.017)       0:02:30.058 ********* 
ok: [node-1]

TASK [ceph-mgr : set_fact ceph_mgr_packages for non el7 distribution] **********
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.018)       0:02:30.077 ********* 
ok: [node-1]

TASK [ceph-mgr : install ceph-mgr packages on RedHat or SUSE] ******************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.022)       0:02:30.099 ********* 
skipping: [node-1]

TASK [ceph-mgr : install ceph-mgr packages for debian] *************************
Saturday 05 June 2021  00:59:58 +0000 (0:00:00.016)       0:02:30.115 ********* 
ok: [node-1]

TASK [ceph-mgr : include start_mgr.yml] ****************************************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.665)       0:02:30.780 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/start_mgr.yml for node-1

TASK [ceph-mgr : ensure systemd service override directory exists] *************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.023)       0:02:30.803 ********* 
skipping: [node-1]

TASK [ceph-mgr : add ceph-mgr systemd service overrides] ***********************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.015)       0:02:30.819 ********* 
skipping: [node-1]

TASK [ceph-mgr : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.014)       0:02:30.834 ********* 
skipping: [node-1]

TASK [ceph-mgr : systemd start mgr] ********************************************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.014)       0:02:30.848 ********* 
ok: [node-1]

TASK [ceph-mgr : include mgr_modules.yml] **************************************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.457)       0:02:31.306 ********* 
included: /opt/ceph-ansible/roles/ceph-mgr/tasks/mgr_modules.yml for node-1

TASK [ceph-mgr : append dashboard modules to ceph_mgr_modules] *****************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.031)       0:02:31.338 ********* 
ok: [node-1]

TASK [ceph-mgr : wait for all mgr to be up] ************************************
Saturday 05 June 2021  00:59:59 +0000 (0:00:00.023)       0:02:31.361 ********* 
ok: [node-1 -> node-2]

TASK [ceph-mgr : get enabled modules from ceph-mgr] ****************************
Saturday 05 June 2021  01:00:00 +0000 (0:00:00.418)       0:02:31.779 ********* 
ok: [node-1 -> node-2]

TASK [ceph-mgr : set _ceph_mgr_modules fact (convert _ceph_mgr_modules.stdout to a dict)] ***
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.971)       0:02:32.750 ********* 
ok: [node-1]

TASK [ceph-mgr : set _disabled_ceph_mgr_modules fact] **************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.044)       0:02:32.795 ********* 
ok: [node-1]

TASK [ceph-mgr : disable ceph mgr enabled modules] *****************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.027)       0:02:32.823 ********* 
skipping: [node-1] => (item=dashboard) 
skipping: [node-1] => (item=prometheus) 

TASK [ceph-mgr : add modules to ceph-mgr] **************************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.026)       0:02:32.849 ********* 
skipping: [node-1] => (item=dashboard) 
skipping: [node-1] => (item=prometheus) 

TASK [set ceph manager install 'Complete'] *************************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.020)       0:02:32.870 ********* 
ok: [node-1]

PLAY [osds] ********************************************************************

TASK [set ceph osd install 'In Progress'] **************************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.027)       0:02:32.898 ********* 
ok: [node-3]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.020)       0:02:32.919 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-3, node-4

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  01:00:01 +0000 (0:00:00.055)       0:02:32.974 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:02.170)       0:02:35.145 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.022)       0:02:35.167 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.147)       0:02:35.315 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.027)       0:02:35.342 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.023)       0:02:35.366 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.094)       0:02:35.461 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.028)       0:02:35.489 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.026)       0:02:35.516 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.028)       0:02:35.544 ********* 
skipping: [node-3] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  01:00:03 +0000 (0:00:00.024)       0:02:35.568 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  01:00:04 +0000 (0:00:00.157)       0:02:35.726 ********* 
ok: [node-3 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:04.130122', 'end': '2021-06-05 01:00:04.132961', 'delta': '0:00:00.002839', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  01:00:04 +0000 (0:00:00.154)       0:02:35.881 ********* 
ok: [node-3] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:04.292098', 'end': '2021-06-05 01:00:04.294505', 'delta': '0:00:00.002407', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:04.130122', 'end': '2021-06-05 01:00:04.132961', 'delta': '0:00:00.002839', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  01:00:04 +0000 (0:00:00.031)       0:02:35.913 ********* 
skipping: [node-3] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  01:00:04 +0000 (0:00:00.020)       0:02:35.933 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  01:00:04 +0000 (0:00:00.028)       0:02:35.962 ********* 
ok: [node-3 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.843)       0:02:36.806 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.025)       0:02:36.831 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.025)       0:02:36.857 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.020)       0:02:36.878 ********* 
ok: [node-3]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.022)       0:02:36.900 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.017)       0:02:36.918 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  01:00:05 +0000 (0:00:00.025)       0:02:36.943 ********* 
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdc)
ok: [node-3] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  01:00:06 +0000 (0:00:01.319)       0:02:38.263 ********* 
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:05.546421', 'end': '2021-06-05 01:00:05.548541', 'delta': '0:00:00.002120', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:05.511535', 'end': '2021-06-05 01:00:05.513663', 'delta': '0:00:00.002128', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:05.673348', 'end': '2021-06-05 01:00:06.676750', 'delta': '0:00:01.003402', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:05.644782', 'end': '2021-06-05 01:00:05.646884', 'delta': '0:00:00.002102', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.038)       0:02:38.301 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.026)       0:02:38.328 ********* 

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.021)       0:02:38.350 ********* 

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.020)       0:02:38.370 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.025)       0:02:38.396 ********* 

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.019)       0:02:38.415 ********* 

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.020)       0:02:38.436 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.025)       0:02:38.462 ********* 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f', 'dm-uuid-LVM-CYEC8gYUYt5QpQ4oUzfMuChDzu4C08SuRy3HxwPWTGzzSUwe2aIsTgJHCgRSM7yy'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1', 'dm-uuid-LVM-ODi5ZHZixQRub4roCEQf80hIkZvkXL19ETU6V1Iz0o9y8mX2PVP24caP5mKo3Mnp'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-Z2dsTJ-RsHU-C6fc-xs0J-SWeX-Vsro-5R3XKz', 'virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b']}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-20K1ns-E8SS-lCWr-dcq5-ZVEp-Ewqd-LSHxhM', 'virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89']}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b', 'dm-uuid-LVM-WAyknBsxMCdbeX0sR6xw65J36xZQesl7KZJqoEGujwFgVUavmYHV8kj6BEfUzDpz'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-43OiAt-Qti1-3oY8-yL5p-9hZA-lRhN-bO1c07', 'virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f']}}) 
skipping: [node-4] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89', 'dm-uuid-LVM-4z7BLxcq21Gx1yBiuNOQYMSpCp1AowddWA7Hwq50FevkMbuVw32h4QCQgU33hN9e'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-K91yj5-Bejm-TEbe-XoUO-QzP6-U2VG-H51VvS', 'virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1']}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.078)       0:02:38.540 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  01:00:06 +0000 (0:00:00.017)       0:02:38.557 ********* 
skipping: [node-3]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.018)       0:02:38.576 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.021)       0:02:38.597 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.161)       0:02:38.759 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.019)       0:02:38.778 ********* 
ok: [node-4]
ok: [node-3]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.145)       0:02:38.923 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.028)       0:02:38.952 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.055)       0:02:39.008 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.021)       0:02:39.029 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.025)       0:02:39.054 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.024)       0:02:39.079 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.025)       0:02:39.105 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.026)       0:02:39.132 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.026)       0:02:39.158 ********* 
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.026)       0:02:39.184 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.021)       0:02:39.205 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.020)       0:02:39.226 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.020)       0:02:39.247 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.022)       0:02:39.269 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.215)       0:02:39.485 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.021)       0:02:39.507 ********* 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.025)       0:02:39.532 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  01:00:07 +0000 (0:00:00.020)       0:02:39.553 ********* 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.024)       0:02:39.577 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.021)       0:02:39.599 ********* 
skipping: [node-3] => (item=node-6) 
skipping: [node-4] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:39.622 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.029)       0:02:39.651 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.027)       0:02:39.679 ********* 
ok: [node-3 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.021)       0:02:39.701 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.020)       0:02:39.721 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-3, node-4

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.039)       0:02:39.761 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.021)       0:02:39.782 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.022)       0:02:39.805 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.031)       0:02:39.836 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.148)       0:02:39.985 ********* 
ok: [node-3] => (item={'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-4] => (item={'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-3] => (item={'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-4] => (item={'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.284)       0:02:40.270 ********* 
skipping: [node-3] => (item=[{'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:08.551824', 'end': '2021-06-05 01:00:08.554706', 'delta': '0:00:00.002882', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-3] => (item=[{'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.1.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:08.681947', 'end': '2021-06-05 01:00:08.684646', 'delta': '0:00:00.002699', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.1.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-4] => (item=[{'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.2.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:08.550140', 'end': '2021-06-05 01:00:08.552868', 'delta': '0:00:00.002728', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.2.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-4] => (item=[{'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.0.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:00:08.680785', 'end': '2021-06-05 01:00:08.682901', 'delta': '0:00:00.002116', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.0.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.040)       0:02:40.310 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.022)       0:02:40.333 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.022)       0:02:40.356 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.022)       0:02:40.378 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.021)       0:02:40.400 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:40.424 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:40.447 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.021)       0:02:40.469 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:40.492 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:40.516 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.022)       0:02:40.538 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  01:00:08 +0000 (0:00:00.023)       0:02:40.562 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:40.585 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.607 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.629 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.650 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.672 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.162)       0:02:40.835 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.856 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.027)       0:02:40.884 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.022)       0:02:40.906 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.928 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.022)       0:02:40.950 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:40.972 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:40.995 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.028)       0:02:41.023 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.024)       0:02:41.047 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.022)       0:02:41.070 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.027)       0:02:41.097 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.121 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.144 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.020)       0:02:41.164 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.188 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.211 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.234 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.022)       0:02:41.257 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.280 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.303 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.024)       0:02:41.327 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.351 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.023)       0:02:41.375 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.025)       0:02:41.400 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.025)       0:02:41.426 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:41.447 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.021)       0:02:41.469 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.024)       0:02:41.493 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  01:00:09 +0000 (0:00:00.025)       0:02:41.519 ********* 
changed: [node-4]
changed: [node-3]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  01:00:16 +0000 (0:00:06.681)       0:02:48.200 ********* 
skipping: [node-3] => (item={'path': '/dev/vda', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {'vda15': {'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': 111149056.0, 'human_readable_size': '106.00 MB', 'holders': []}, 'vda1': {'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': 107257773568.0, 'human_readable_size': '99.89 GB', 'holders': []}, 'vda14': {'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': 4194304.0, 'human_readable_size': '4.00 MB', 'holders': []}}, 'sectors': 0, 'sectorsize': '512', 'size': 107374182400.0, 'human_readable_size': '100.00 GB', 'path': '/dev/vda', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['locked'], 'device_id': '7c3fe3ed-ccd7-486d-b', 'lvs': []}) 
skipping: [node-4] => (item={'path': '/dev/vda', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {'vda15': {'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': 111149056.0, 'human_readable_size': '106.00 MB', 'holders': []}, 'vda1': {'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': 107257773568.0, 'human_readable_size': '99.89 GB', 'holders': []}, 'vda14': {'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': 4194304.0, 'human_readable_size': '4.00 MB', 'holders': []}}, 'sectors': 0, 'sectorsize': '512', 'size': 107374182400.0, 'human_readable_size': '100.00 GB', 'path': '/dev/vda', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['locked'], 'device_id': '4403e9d7-14c7-4acf-a', 'lvs': []}) 
skipping: [node-3] => (item={'path': '/dev/vdb', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdb', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['Insufficient space (<10 extents) on vgs', 'locked', 'LVM detected'], 'device_id': '132ca2a0-a39f-4f7a-8', 'lvs': [{'name': 'osd-block-44b70d25-7b56-49bb-b2e6-313018c8310b', 'osd_id': '1', 'cluster_name': 'ceph', 'type': 'block', 'osd_fsid': '44b70d25-7b56-49bb-b2e6-313018c8310b', 'cluster_fsid': '83b8c8b9-24e1-4842-a961-e82a6cdd24a3', 'osdspec_affinity': '', 'block_uuid': 'KZJqoE-Gujw-FgVU-avmY-HV8k-j6BE-fUzDpz'}]}) 
skipping: [node-4] => (item={'path': '/dev/vdb', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdb', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['LVM detected', 'Insufficient space (<10 extents) on vgs', 'locked'], 'device_id': 'a7783c46-16ee-4d7e-b', 'lvs': [{'name': 'osd-block-d18a2842-847e-4997-97e9-333f59ed6c89', 'osd_id': '0', 'cluster_name': 'ceph', 'type': 'block', 'osd_fsid': 'd18a2842-847e-4997-97e9-333f59ed6c89', 'cluster_fsid': '83b8c8b9-24e1-4842-a961-e82a6cdd24a3', 'osdspec_affinity': '', 'block_uuid': 'WA7Hwq-50Fe-vkMb-uVw3-2h4Q-CQgU-33hN9e'}]}) 
skipping: [node-3] => (item={'path': '/dev/vdc', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdc', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['Insufficient space (<10 extents) on vgs', 'locked', 'LVM detected'], 'device_id': '9234817e-8651-4ccd-8', 'lvs': [{'name': 'osd-block-e9986d2a-2d5b-4c11-b006-6c3ddc2fa31f', 'osd_id': '3', 'cluster_name': 'ceph', 'type': 'block', 'osd_fsid': 'e9986d2a-2d5b-4c11-b006-6c3ddc2fa31f', 'cluster_fsid': '83b8c8b9-24e1-4842-a961-e82a6cdd24a3', 'osdspec_affinity': '', 'block_uuid': 'Ry3Hxw-PWTG-zzSU-we2a-IsTg-JHCg-RSM7yy'}]}) 
skipping: [node-4] => (item={'path': '/dev/vdc', 'sys_api': {'removable': '0', 'ro': '0', 'vendor': '0x1af4', 'model': '', 'rev': '', 'sas_address': '', 'sas_device_handle': '', 'support_discard': '0', 'rotational': '1', 'nr_requests': '256', 'scheduler_mode': 'mq-deadline', 'partitions': {}, 'sectors': 0, 'sectorsize': '512', 'size': 21474836480.0, 'human_readable_size': '20.00 GB', 'path': '/dev/vdc', 'locked': 1}, 'lsm_data': {}, 'available': False, 'rejected_reasons': ['LVM detected', 'Insufficient space (<10 extents) on vgs', 'locked'], 'device_id': '45707e10-63d5-4285-9', 'lvs': [{'name': 'osd-block-eb271dd4-44bc-418a-b5f3-6a69ea9d0bc1', 'osd_id': '2', 'cluster_name': 'ceph', 'type': 'block', 'osd_fsid': 'eb271dd4-44bc-418a-b5f3-6a69ea9d0bc1', 'cluster_fsid': '83b8c8b9-24e1-4842-a961-e82a6cdd24a3', 'osdspec_affinity': '', 'block_uuid': 'ETU6V1-Iz0o-9y8m-X2PV-P24c-aP5m-Ko3Mnp'}]}) 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  01:00:16 +0000 (0:00:00.054)       0:02:48.254 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  01:00:16 +0000 (0:00:00.029)       0:02:48.284 ********* 
[WARNING]: The value 5120 (type int) in a string field was converted to '5120'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.
[WARNING]: The value -1 (type int) in a string field was converted to '-1'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.
changed: [node-3]
changed: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  01:00:18 +0000 (0:00:01.363)       0:02:49.647 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  01:00:18 +0000 (0:00:00.026)       0:02:49.674 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  01:00:18 +0000 (0:00:00.032)       0:02:49.706 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.892)       0:02:50.599 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.028)       0:02:50.627 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.155)       0:02:50.782 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact add_osd] *********************************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.240)       0:02:51.023 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact container_exec_cmd] **********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.029)       0:02:51.052 ********* 
skipping: [node-3] => (item=node-2) 

TASK [ceph-osd : include_tasks system_tuning.yml] ******************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.019)       0:02:51.072 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/system_tuning.yml for node-3, node-4

TASK [ceph-osd : disable osd directory parsing by updatedb] ********************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.036)       0:02:51.108 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : disable osd directory path in updatedb.conf] ******************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.025)       0:02:51.133 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : create tmpfiles.d directory] **********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.021)       0:02:51.155 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : disable transparent hugepage] *********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.023)       0:02:51.178 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : get default vm.min_free_kbytes] *******************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.021)       0:02:51.200 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact vm_min_free_kbytes] **********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.240)       0:02:51.440 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : apply operating system tuning] ********************************
Saturday 05 June 2021  01:00:19 +0000 (0:00:00.025)       0:02:51.466 ********* 
ok: [node-4] => (item={'name': 'fs.aio-max-nr', 'value': '1048576', 'enable': True})
ok: [node-3] => (item={'name': 'fs.aio-max-nr', 'value': '1048576', 'enable': True})
ok: [node-4] => (item={'name': 'fs.file-max', 'value': 26234859})
ok: [node-3] => (item={'name': 'fs.file-max', 'value': 26234859})
ok: [node-4] => (item={'name': 'vm.zone_reclaim_mode', 'value': 0})
ok: [node-3] => (item={'name': 'vm.zone_reclaim_mode', 'value': 0})
ok: [node-4] => (item={'name': 'vm.swappiness', 'value': 10})
ok: [node-3] => (item={'name': 'vm.swappiness', 'value': 10})
ok: [node-4] => (item={'name': 'vm.min_free_kbytes', 'value': '67584'})
ok: [node-3] => (item={'name': 'vm.min_free_kbytes', 'value': '67584'})

TASK [ceph-osd : install dependencies] *****************************************
Saturday 05 June 2021  01:00:20 +0000 (0:00:00.807)       0:02:52.274 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : install numactl when needed] **********************************
Saturday 05 June 2021  01:00:21 +0000 (0:00:00.670)       0:02:52.944 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks common.yml] *************************************
Saturday 05 June 2021  01:00:21 +0000 (0:00:00.022)       0:02:52.966 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/common.yml for node-3, node-4

TASK [ceph-osd : create bootstrap-osd and osd directories] *********************
Saturday 05 June 2021  01:00:21 +0000 (0:00:00.037)       0:02:53.003 ********* 
ok: [node-4] => (item=/var/lib/ceph/bootstrap-osd/)
ok: [node-3] => (item=/var/lib/ceph/bootstrap-osd/)
ok: [node-4] => (item=/var/lib/ceph/osd/)
ok: [node-3] => (item=/var/lib/ceph/osd/)

TASK [ceph-osd : get keys from monitors] ***************************************
Saturday 05 June 2021  01:00:21 +0000 (0:00:00.300)       0:02:53.304 ********* 
changed: [node-3 -> node-2] => (item={'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True})
skipping: [node-3] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}) 

TASK [ceph-osd : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  01:00:33 +0000 (0:00:11.836)       0:03:05.140 ********* 
ok: [node-3] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-osd'], 'stdout': '[client.bootstrap-osd]\n\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==\n\tcaps mon = "allow profile bootstrap-osd"', 'stderr': 'exported keyring for client.bootstrap-osd', 'rc': 0, 'start': '2021-06-05 01:00:21.864012', 'end': '2021-06-05 01:00:33.547419', 'delta': '0:00:11.683407', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-osd', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-osd]', '\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==', '\tcaps mon = "allow profile bootstrap-osd"'], 'stderr_lines': ['exported keyring for client.bootstrap-osd'], 'failed': False, 'item': {'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-3] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 
ok: [node-4] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-osd'], 'stdout': '[client.bootstrap-osd]\n\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==\n\tcaps mon = "allow profile bootstrap-osd"', 'stderr': 'exported keyring for client.bootstrap-osd', 'rc': 0, 'start': '2021-06-05 01:00:21.864012', 'end': '2021-06-05 01:00:33.547419', 'delta': '0:00:11.683407', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-osd', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-osd]', '\tkey = AQB5xLpghLRnFBAA7nDlJMsODUwT3bHR0Kt1WQ==', '\tcaps mon = "allow profile bootstrap-osd"'], 'stderr_lines': ['exported keyring for client.bootstrap-osd'], 'failed': False, 'item': {'name': 'client.bootstrap-osd', 'path': '/var/lib/ceph/bootstrap-osd/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-4] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 

TASK [ceph-osd : set noup flag] ************************************************
Saturday 05 June 2021  01:00:39 +0000 (0:00:06.334)       0:03:11.474 ********* 
changed: [node-3 -> node-2]

TASK [ceph-osd : include container_options_facts.yml] **************************
Saturday 05 June 2021  01:00:45 +0000 (0:00:05.497)       0:03:16.972 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks scenarios/lvm.yml] ******************************
Saturday 05 June 2021  01:00:45 +0000 (0:00:00.021)       0:03:16.994 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include_tasks scenarios/lvm-batch.yml] ************************
Saturday 05 June 2021  01:00:45 +0000 (0:00:00.021)       0:03:17.015 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/scenarios/lvm-batch.yml for node-3, node-4

TASK [ceph-osd : use ceph-volume lvm batch to create bluestore osds] ***********
Saturday 05 June 2021  01:00:45 +0000 (0:00:00.043)       0:03:17.059 ********* 
changed: [node-4]
changed: [node-3]

TASK [ceph-osd : include_tasks start_osds.yml] *********************************
Saturday 05 June 2021  01:00:56 +0000 (0:00:11.037)       0:03:28.096 ********* 
included: /opt/ceph-ansible/roles/ceph-osd/tasks/start_osds.yml for node-3, node-4

TASK [ceph-osd : umount ceph disk (if on openstack)] ***************************
Saturday 05 June 2021  01:00:56 +0000 (0:00:00.039)       0:03:28.136 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : get osd ids] **************************************************
Saturday 05 June 2021  01:00:56 +0000 (0:00:00.020)       0:03:28.156 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : set_fact container_exec_start_osd] ****************************
Saturday 05 June 2021  01:00:56 +0000 (0:00:00.143)       0:03:28.300 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : collect osd ids] **********************************************
Saturday 05 June 2021  01:00:56 +0000 (0:00:00.025)       0:03:28.326 ********* 
ok: [node-3]
ok: [node-4]

TASK [ceph-osd : include_tasks systemd.yml] ************************************
Saturday 05 June 2021  01:00:57 +0000 (0:00:00.912)       0:03:29.238 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : ensure systemd service override directory exists] *************
Saturday 05 June 2021  01:00:57 +0000 (0:00:00.021)       0:03:29.260 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : add ceph-osd systemd service overrides] ***********************
Saturday 05 June 2021  01:00:57 +0000 (0:00:00.022)       0:03:29.282 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : ensure "/var/lib/ceph/osd/{{ cluster }}-{{ item }}" is present] ***
Saturday 05 June 2021  01:00:57 +0000 (0:00:00.022)       0:03:29.305 ********* 
ok: [node-3] => (item=1)
ok: [node-4] => (item=0)
ok: [node-3] => (item=3)
ok: [node-4] => (item=2)

TASK [ceph-osd : systemd start osd] ********************************************
Saturday 05 June 2021  01:00:58 +0000 (0:00:00.297)       0:03:29.602 ********* 
ok: [node-3] => (item=1)
ok: [node-4] => (item=0)
ok: [node-3] => (item=3)
ok: [node-4] => (item=2)

TASK [ceph-osd : unset noup flag] **********************************************
Saturday 05 June 2021  01:00:58 +0000 (0:00:00.920)       0:03:30.523 ********* 
skipping: [node-3]
changed: [node-4 -> node-2]

TASK [ceph-osd : wait for all osd to be up] ************************************
Saturday 05 June 2021  01:01:02 +0000 (0:00:03.377)       0:03:33.900 ********* 
skipping: [node-3]
ok: [node-4 -> node-2]

TASK [ceph-osd : include crush_rules.yml] **************************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.819)       0:03:34.720 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [ceph-osd : include openstack_config.yml] *********************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.025)       0:03:34.745 ********* 
skipping: [node-3]
skipping: [node-4]

TASK [set ceph osd install 'Complete'] *****************************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.024)       0:03:34.770 ********* 
ok: [node-3]

PLAY [mdss] ********************************************************************
skipping: no hosts matched

PLAY [rgws] ********************************************************************

TASK [set ceph rgw install 'In Progress'] **************************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.030)       0:03:34.800 ********* 
ok: [node-6]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.022)       0:03:34.823 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-6

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.046)       0:03:34.870 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.224)       0:03:35.094 ********* 
ok: [node-6]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.017)       0:03:35.111 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.128)       0:03:35.240 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.019)       0:03:35.260 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.014)       0:03:35.274 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.016)       0:03:35.291 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.021)       0:03:35.313 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.025)       0:03:35.338 ********* 
skipping: [node-6]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.021)       0:03:35.360 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.022)       0:03:35.382 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  01:01:03 +0000 (0:00:00.153)       0:03:35.536 ********* 
ok: [node-6 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:03.940504', 'end': '2021-06-05 01:01:03.942977', 'delta': '0:00:00.002473', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.149)       0:03:35.685 ********* 
ok: [node-6] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:04.097400', 'end': '2021-06-05 01:01:04.099323', 'delta': '0:00:00.001923', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:03.940504', 'end': '2021-06-05 01:01:03.942977', 'delta': '0:00:00.002473', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.031)       0:03:35.716 ********* 
skipping: [node-6] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.020)       0:03:35.737 ********* 
skipping: [node-6]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.020)       0:03:35.757 ********* 
ok: [node-6 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.618)       0:03:36.376 ********* 
skipping: [node-6]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.020)       0:03:36.396 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.412 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.427 ********* 
ok: [node-6]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.021)       0:03:36.449 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.465 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.480 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.016)       0:03:36.497 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.512 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.014)       0:03:36.527 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.543 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  01:01:04 +0000 (0:00:00.015)       0:03:36.558 ********* 
skipping: [node-6]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.014)       0:03:36.573 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.014)       0:03:36.588 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.015)       0:03:36.603 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.015)       0:03:36.619 ********* 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.059)       0:03:36.679 ********* 
ok: [node-6 -> node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.758)       0:03:37.437 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.021)       0:03:37.459 ********* 
skipping: [node-6]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  01:01:05 +0000 (0:00:00.019)       0:03:37.478 ********* 
ok: [node-6]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.142)       0:03:37.621 ********* 
ok: [node-6]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.014)       0:03:37.635 ********* 
ok: [node-6]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.138)       0:03:37.773 ********* 
skipping: [node-6]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:37.794 ********* 
skipping: [node-6]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.016)       0:03:37.811 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:37.828 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.019)       0:03:37.848 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:37.868 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.019)       0:03:37.888 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:37.908 ********* 
skipping: [node-6] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.018)       0:03:37.927 ********* 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:37.947 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:37.965 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:37.983 ********* 
ok: [node-6]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:38.003 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.018)       0:03:38.021 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.021)       0:03:38.043 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:38.061 ********* 
ok: [node-6] => (item=0)

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.025)       0:03:38.087 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.016)       0:03:38.103 ********* 
skipping: [node-6] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.020)       0:03:38.124 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.019)       0:03:38.144 ********* 
skipping: [node-6] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.021)       0:03:38.165 ********* 
skipping: [node-6]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.023)       0:03:38.189 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.027)       0:03:38.216 ********* 
ok: [node-6 -> node-2] => (item=node-2)

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.022)       0:03:38.238 ********* 
skipping: [node-6]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.015)       0:03:38.254 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-6

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.029)       0:03:38.284 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.018)       0:03:38.302 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.016)       0:03:38.318 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:38.336 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.015)       0:03:38.352 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.016)       0:03:38.369 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:38.386 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.015)       0:03:38.402 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:38.419 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  01:01:06 +0000 (0:00:00.017)       0:03:38.437 ********* 
ok: [node-6]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.140)       0:03:38.578 ********* 
ok: [node-6] => (item={'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.89111.94252982106616.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622854833.3290498, 'mtime': 1622854833.3290498, 'ctime': 1622854833.3290498, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.144)       0:03:38.722 ********* 
skipping: [node-6] => (item=[{'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.89111.94252982106616.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622854833.3290498, 'mtime': 1622854833.3290498, 'ctime': 1622854833.3290498, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.89111.94252982106616.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:07.135469', 'end': '2021-06-05 01:01:07.137747', 'delta': '0:00:00.002278', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.89111.94252982106616.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.89111.94252982106616.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622854833.3290498, 'mtime': 1622854833.3290498, 'ctime': 1622854833.3290498, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.027)       0:03:38.750 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:38.767 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.017)       0:03:38.785 ********* 
skipping: [node-6]

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:38.801 ********* 
skipping: [node-6]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:38.818 ********* 
skipping: [node-6]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:38.835 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.017)       0:03:38.852 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:38.868 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:38.883 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:38.899 ********* 
skipping: [node-6]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:38.915 ********* 
ok: [node-6]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.158)       0:03:39.073 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:39.089 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:39.105 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:39.120 ********* 
ok: [node-6]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.140 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:39.156 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.016)       0:03:39.172 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.014)       0:03:39.187 ********* 
ok: [node-6]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.022)       0:03:39.210 ********* 
skipping: [node-6]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.018)       0:03:39.228 ********* 
included: /opt/ceph-ansible/roles/ceph-config/tasks/rgw_systemd_environment_file.yml for node-6

TASK [ceph-config : create rados gateway instance directories] *****************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.024)       0:03:39.253 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-config : generate environment file] *********************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.158)       0:03:39.412 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.026)       0:03:39.438 ********* 
skipping: [node-6]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.458 ********* 
skipping: [node-6]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.478 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.499 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.015)       0:03:39.514 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.534 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  01:01:07 +0000 (0:00:00.020)       0:03:39.554 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.020)       0:03:39.574 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.018)       0:03:39.593 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.020)       0:03:39.613 ********* 
skipping: [node-6]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.018)       0:03:39.632 ********* 
ok: [node-6]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.148)       0:03:39.781 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-6]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.333)       0:03:40.114 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.152)       0:03:40.267 ********* 
skipping: [node-6]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.019)       0:03:40.287 ********* 
ok: [node-6]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.019)       0:03:40.306 ********* 
skipping: [node-6]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.014)       0:03:40.320 ********* 
included: /opt/ceph-ansible/roles/ceph-config/tasks/rgw_systemd_environment_file.yml for node-6

TASK [ceph-config : create rados gateway instance directories] *****************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.020)       0:03:40.341 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-config : generate environment file] *********************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.145)       0:03:40.486 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.018)       0:03:40.505 ********* 
skipping: [node-6]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.016)       0:03:40.521 ********* 
skipping: [node-6]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.015)       0:03:40.537 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.016)       0:03:40.553 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  01:01:08 +0000 (0:00:00.014)       0:03:40.567 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.015)       0:03:40.583 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.017)       0:03:40.600 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.014)       0:03:40.615 ********* 
skipping: [node-6]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.016)       0:03:40.631 ********* 
skipping: [node-6]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.015)       0:03:40.647 ********* 
skipping: [node-6]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.015)       0:03:40.663 ********* 
ok: [node-6]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.141)       0:03:40.804 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-6]

TASK [ceph-rgw : include common.yml] *******************************************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.230)       0:03:41.035 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/common.yml for node-6

TASK [ceph-rgw : create rados gateway directories] *****************************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.019)       0:03:41.054 ********* 
changed: [node-6] => (item=/var/run/ceph)

TASK [ceph-rgw : get keys from monitors] ***************************************
Saturday 05 June 2021  01:01:09 +0000 (0:00:00.153)       0:03:41.208 ********* 
changed: [node-6 -> node-2] => (item={'name': 'client.bootstrap-rgw', 'path': '/var/lib/ceph/bootstrap-rgw/ceph.keyring', 'copy_key': True})
skipping: [node-6] => (item={'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}) 

TASK [ceph-rgw : copy ceph key(s) if needed] ***********************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.715)       0:03:41.923 ********* 
ok: [node-6] => (item={'cmd': ['ceph', '--cluster', 'ceph', 'auth', 'get', 'client.bootstrap-rgw'], 'stdout': '[client.bootstrap-rgw]\n\tkey = AQB5xLpgat1nFBAACBG9jvBl5Gee0ePAlCEuEQ==\n\tcaps mon = "allow profile bootstrap-rgw"', 'stderr': 'exported keyring for client.bootstrap-rgw', 'rc': 0, 'start': '2021-06-05 01:01:09.766443', 'end': '2021-06-05 01:01:10.333636', 'delta': '0:00:00.567193', 'changed': True, 'invocation': {'module_args': {'_raw_params': ' ceph --cluster ceph auth get client.bootstrap-rgw', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['[client.bootstrap-rgw]', '\tkey = AQB5xLpgat1nFBAACBG9jvBl5Gee0ePAlCEuEQ==', '\tcaps mon = "allow profile bootstrap-rgw"'], 'stderr_lines': ['exported keyring for client.bootstrap-rgw'], 'failed': False, 'item': {'name': 'client.bootstrap-rgw', 'path': '/var/lib/ceph/bootstrap-rgw/ceph.keyring', 'copy_key': True}, 'ansible_loop_var': 'item'})
skipping: [node-6] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': {'name': 'client.admin', 'path': '/etc/ceph/ceph.client.admin.keyring', 'copy_key': False}, 'ansible_loop_var': 'item'}) 

TASK [ceph-rgw : copy SSL certificate & key data to certificate path] **********
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.312)       0:03:42.236 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks pre_requisite.yml] ******************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.015)       0:03:42.251 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/pre_requisite.yml for node-6

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.020)       0:03:42.272 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : rgw pool creation tasks] **************************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.188)       0:03:42.461 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks openstack-keystone.yml] *************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.014)       0:03:42.475 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks start_radosgw.yml] ******************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.013)       0:03:42.489 ********* 
included: /opt/ceph-ansible/roles/ceph-rgw/tasks/start_radosgw.yml for node-6

TASK [ceph-rgw : ensure systemd service override directory exists] *************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.024)       0:03:42.513 ********* 
skipping: [node-6]

TASK [ceph-rgw : add ceph-rgw systemd service overrides] ***********************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.013)       0:03:42.527 ********* 
skipping: [node-6]

TASK [ceph-rgw : start rgw instance] *******************************************
Saturday 05 June 2021  01:01:10 +0000 (0:00:00.012)       0:03:42.540 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : enable the ceph-radosgw.target service] ***********************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.253)       0:03:42.794 ********* 
ok: [node-6]

TASK [ceph-rgw : include start_docker_rgw.yml] *********************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.234)       0:03:43.028 ********* 
skipping: [node-6]

TASK [ceph-rgw : include_tasks multisite/main.yml] *****************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.015)       0:03:43.044 ********* 
skipping: [node-6]

TASK [set ceph rgw install 'Complete'] *****************************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.016)       0:03:43.060 ********* 
ok: [node-6]

PLAY [clients] *****************************************************************
skipping: no hosts matched

PLAY [nfss] ********************************************************************
skipping: no hosts matched

PLAY [rbdmirrors] **************************************************************
skipping: no hosts matched

PLAY [iscsigws,iscsi-gws] ******************************************************
skipping: no hosts matched

PLAY [rgwloadbalancers] ********************************************************
skipping: no hosts matched

PLAY [mons,osds,mdss,rgws,mgrs,rbdmirrors,nfss,iscsigws,grafana-server] ********

TASK [set ceph node exporter install 'In Progress'] ****************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.039)       0:03:43.099 ********* 
ok: [node-2]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.027)       0:03:43.127 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.115)       0:03:43.243 ********* 
ok: [node-3]
ok: [node-2]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  01:01:11 +0000 (0:00:00.261)       0:03:43.505 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-1]
ok: [node-6]
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.066)       0:03:43.571 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-5]
ok: [node-1]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.192)       0:03:43.764 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-1]
ok: [node-6]
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.080)       0:03:43.844 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.068)       0:03:43.913 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.063)       0:03:43.976 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.073)       0:03:44.050 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.046)       0:03:44.097 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.069)       0:03:44.166 ********* 
skipping: [node-2] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.043)       0:03:44.209 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  01:01:12 +0000 (0:00:00.244)       0:03:44.454 ********* 
ok: [node-2 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:12.857614', 'end': '2021-06-05 01:01:12.860261', 'delta': '0:00:00.002647', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.192)       0:03:44.646 ********* 
ok: [node-2] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:13.037618', 'end': '2021-06-05 01:01:13.040804', 'delta': '0:00:00.003186', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:12.857614', 'end': '2021-06-05 01:01:12.860261', 'delta': '0:00:00.002647', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.050)       0:03:44.697 ********* 
skipping: [node-2] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.041)       0:03:44.739 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.069)       0:03:44.808 ********* 
ok: [node-2 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.433)       0:03:45.242 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.069)       0:03:45.311 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.082)       0:03:45.394 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.058)       0:03:45.453 ********* 
ok: [node-2]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.031)       0:03:45.484 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:13 +0000 (0:00:00.028)       0:03:45.513 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.064)       0:03:45.577 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-3] => (item=/dev/vdb)
ok: [node-4] => (item=/dev/vdb)
ok: [node-3] => (item=/dev/vdc)
ok: [node-4] => (item=/dev/vdc)

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.330)       0:03:45.908 ********* 
skipping: [node-2]
skipping: [node-6]
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:14.166668', 'end': '2021-06-05 01:01:14.169205', 'delta': '0:00:00.002537', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
ok: [node-3] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:14.318050', 'end': '2021-06-05 01:01:14.320267', 'delta': '0:00:00.002217', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdb'], 'stdout': '/dev/vdb', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:14.170647', 'end': '2021-06-05 01:01:14.172310', 'delta': '0:00:00.001663', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdb', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdb'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdb', 'ansible_loop_var': 'item'})
skipping: [node-5]
ok: [node-4] => (item={'cmd': ['readlink', '-f', '/dev/vdc'], 'stdout': '/dev/vdc', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:14.322115', 'end': '2021-06-05 01:01:14.323891', 'delta': '0:00:00.001776', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'readlink -f /dev/vdc', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/dev/vdc'], 'stderr_lines': [], 'failed': False, 'item': '/dev/vdc', 'ansible_loop_var': 'item'})
skipping: [node-1]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.192)       0:03:46.101 ********* 
skipping: [node-2]
ok: [node-3]
skipping: [node-6]
ok: [node-4]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.066)       0:03:46.168 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.060)       0:03:46.228 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.062)       0:03:46.291 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.065)       0:03:46.357 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.061)       0:03:46.419 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.066)       0:03:46.485 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-5]
skipping: [node-1]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  01:01:14 +0000 (0:00:00.063)       0:03:46.548 ********* 
skipping: [node-2] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f', 'dm-uuid-LVM-CYEC8gYUYt5QpQ4oUzfMuChDzu4C08SuRy3HxwPWTGzzSUwe2aIsTgJHCgRSM7yy'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e', 'dm-uuid-LVM-ydDorItiXO1OeTfyo80lF4qP3HA5QTeKfOc2MZiErIWusGn1vycVp3SeYTYLxoE2'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1', 'dm-uuid-LVM-ODi5ZHZixQRub4roCEQf80hIkZvkXL19ETU6V1Iz0o9y8mX2PVP24caP5mKo3Mnp'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-Z2dsTJ-RsHU-C6fc-xs0J-SWeX-Vsro-5R3XKz', 'virtio-132ca2a0-a39f-4f7a-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b']}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-4] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-20K1ns-E8SS-lCWr-dcq5-ZVEp-Ewqd-LSHxhM', 'virtio-a7783c46-16ee-4d7e-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89']}}) 
skipping: [node-2] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-chBfSx-QbKF-dWNj-aCyQ-3ffX-2B1g-wnKZZi', 'virtio-bba6b9f6-af2e-4051-8'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d']}}) 
skipping: [node-6] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b34241f5-e429-4344-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--e43ceb28--b9db--4a82--9bcd--1c178f588da3-osd--block--44b70d25--7b56--49bb--b2e6--313018c8310b', 'dm-uuid-LVM-WAyknBsxMCdbeX0sR6xw65J36xZQesl7KZJqoEGujwFgVUavmYHV8kj6BEfUzDpz'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-43OiAt-Qti1-3oY8-yL5p-9hZA-lRhN-bO1c07', 'virtio-9234817e-8651-4ccd-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--433dd112--84d0--4bc9--b2bf--a2544a207657-osd--block--e9986d2a--2d5b--4c11--b006--6c3ddc2fa31f']}}) 
skipping: [node-6] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--1c7bdf68--df61--4c83--a46e--69415cabb4d0-osd--block--d18a2842--847e--4997--97e9--333f59ed6c89', 'dm-uuid-LVM-4z7BLxcq21Gx1yBiuNOQYMSpCp1AowddWA7Hwq50FevkMbuVw32h4QCQgU33hN9e'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-K91yj5-Bejm-TEbe-XoUO-QzP6-U2VG-H51VvS', 'virtio-45707e10-63d5-4285-9'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a23b1d24--d5af--4df7--a08b--32084410cecc-osd--block--eb271dd4--44bc--418a--b5f3--6a69ea9d0bc1']}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-4] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-7c3fe3ed-ccd7-486d-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--cd441f78--de14--474a--906c--b7905793ef23-osd--block--b2329ac8--7ed3--4200--b620--1e74bf92ab2d', 'dm-uuid-LVM-fgaxyJ3y3nG0p2aKf9r7eteU0moTWoOV7Od52AzjJ9dqNav01AoBFZ04PdJefgeJ'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-4403e9d7-14c7-4acf-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-df69e1d5-6559-45f4-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-3] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-4] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-1db925ad-6276-464a-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-1db925ad-6276-464a-b-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-qVzQfr-2HRe-lldj-mXAV-1KDO-UsL2-B00ANY', 'virtio-75fa5c96-3ebc-44c6-b'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--49b0396f--b624--407d--bebc--aeb57009ade6-osd--block--76a38e25--6429--46ad--8978--0add49bf6d4e']}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-6] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-db27ec09-e4ed-4820-8'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-db27ec09-e4ed-4820-8-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-2] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  01:01:15 +0000 (0:00:00.555)       0:03:47.104 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  01:01:15 +0000 (0:00:00.028)       0:03:47.132 ********* 
skipping: [node-2]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  01:01:15 +0000 (0:00:00.028)       0:03:47.161 ********* 
skipping: [node-2]
skipping: [node-4]
skipping: [node-3]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  01:01:15 +0000 (0:00:00.064)       0:03:47.225 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-6]
ok: [node-5]
ok: [node-1]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  01:01:15 +0000 (0:00:00.281)       0:03:47.507 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.068)       0:03:47.576 ********* 
skipping: [node-5]
ok: [node-3]
ok: [node-6]
ok: [node-4]
ok: [node-1]
ok: [node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.217)       0:03:47.793 ********* 
ok: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.068)       0:03:47.861 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.238)       0:03:48.100 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.074)       0:03:48.174 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.105)       0:03:48.280 ********* 
skipping: [node-2] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-3] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.100)       0:03:48.380 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  01:01:16 +0000 (0:00:00.102)       0:03:48.482 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.106)       0:03:48.589 ********* 
skipping: [node-3] => (item=node-2) 
skipping: [node-4] => (item=node-2) 
skipping: [node-2] => (item=node-2) 
skipping: [node-6] => (item=node-2) 
skipping: [node-1] => (item=node-2) 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.101)       0:03:48.691 ********* 
ok: [node-2] => (item={'name': 'node-2', 'addr': '100.200.23.57'})
skipping: [node-4] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-3] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-6] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.099)       0:03:48.791 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.071)       0:03:48.862 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.071)       0:03:48.933 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
ok: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.073)       0:03:49.007 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.072)       0:03:49.079 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.071)       0:03:49.151 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.071)       0:03:49.223 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-5] => (item=0) 
ok: [node-6] => (item=0)

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.087)       0:03:49.310 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.072)       0:03:49.383 ********* 
skipping: [node-2] => (item=0) 
skipping: [node-3] => (item=0) 
skipping: [node-4] => (item=0) 
skipping: [node-1] => (item=0) 
skipping: [node-6] => (item=0) 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.085)       0:03:49.468 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-5]
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  01:01:17 +0000 (0:00:00.082)       0:03:49.550 ********* 
skipping: [node-2] => (item=node-6) 
skipping: [node-3] => (item=node-6) 
skipping: [node-4] => (item=node-6) 
skipping: [node-1] => (item=node-6) 
skipping: [node-5] => (item=node-6) 
skipping: [node-6] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.088)       0:03:49.639 ********* 
skipping: [node-3]
skipping: [node-2]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.074)       0:03:49.714 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.051)       0:03:49.765 ********* 
ok: [node-2 -> node-2] => (item=node-2)

TASK [ceph-container-engine : include pre_requisites/prerequisites.yml] ********
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.046)       0:03:49.812 ********* 
included: /opt/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/prerequisites.yml for node-3, node-2, node-4, node-1, node-6, node-5

TASK [ceph-container-engine : include specific variables] **********************
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.084)       0:03:49.896 ********* 
ok: [node-2] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-3] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-4] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-6] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-1] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)
ok: [node-5] => (item=/opt/ceph-ansible/roles/ceph-container-engine/vars/Ubuntu-20.yml)

TASK [ceph-container-engine : debian based systems tasks] **********************
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.094)       0:03:49.990 ********* 
included: /opt/ceph-ansible/roles/ceph-container-engine/tasks/pre_requisites/debian_prerequisites.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-container-engine : uninstall old docker versions] *******************
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.097)       0:03:50.088 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : allow apt to use a repository over https (debian)] ***
Saturday 05 June 2021  01:01:18 +0000 (0:00:00.071)       0:03:50.159 ********* 
ok: [node-3]
ok: [node-6]
ok: [node-5]
ok: [node-4]
ok: [node-2]
ok: [node-1]

TASK [ceph-container-engine : add docker's gpg key] ****************************
Saturday 05 June 2021  01:01:29 +0000 (0:00:11.165)       0:04:01.324 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : add docker repository] ***************************
Saturday 05 June 2021  01:01:29 +0000 (0:00:00.070)       0:04:01.394 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : add podman ppa repository] ***********************
Saturday 05 June 2021  01:01:29 +0000 (0:00:00.066)       0:04:01.461 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : enable extras on centos] *************************
Saturday 05 June 2021  01:01:29 +0000 (0:00:00.069)       0:04:01.530 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : install container packages] **********************
Saturday 05 June 2021  01:01:30 +0000 (0:00:00.064)       0:04:01.594 ********* 
ok: [node-5]
ok: [node-1]
ok: [node-6]
ok: [node-4]
ok: [node-3]
ok: [node-2]

TASK [ceph-container-engine : install lvm2 package] ****************************
Saturday 05 June 2021  01:01:35 +0000 (0:00:05.040)       0:04:06.634 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]
ok: [node-4]
ok: [node-3]

TASK [ceph-container-engine : create the systemd docker override directory] ****
Saturday 05 June 2021  01:01:35 +0000 (0:00:00.734)       0:04:07.369 ********* 
skipping: [node-2]
skipping: [node-4]
skipping: [node-3]
skipping: [node-1]
skipping: [node-6]
skipping: [node-5]

TASK [ceph-container-engine : create the systemd docker override file] *********
Saturday 05 June 2021  01:01:35 +0000 (0:00:00.065)       0:04:07.434 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : remove docker proxy configuration] ***************
Saturday 05 June 2021  01:01:35 +0000 (0:00:00.061)       0:04:07.496 ********* 
ok: [node-2]
ok: [node-6]
ok: [node-3]
ok: [node-4]
ok: [node-5]
ok: [node-1]

TASK [ceph-container-engine : restart docker] **********************************
Saturday 05 June 2021  01:01:38 +0000 (0:00:02.279)       0:04:09.775 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-container-engine : start container service] *************************
Saturday 05 June 2021  01:01:38 +0000 (0:00:00.060)       0:04:09.836 ********* 
ok: [node-5]
ok: [node-2]
ok: [node-6]
ok: [node-3]
ok: [node-4]
ok: [node-1]

TASK [ceph-container-common : container registry authentication] ***************
Saturday 05 June 2021  01:01:38 +0000 (0:00:00.296)       0:04:10.133 ********* 
skipping: [node-3]
skipping: [node-2]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
skipping: [node-5]

TASK [ceph-node-exporter : include setup_container.yml] ************************
Saturday 05 June 2021  01:01:38 +0000 (0:00:00.057)       0:04:10.191 ********* 
included: /opt/ceph-ansible/roles/ceph-node-exporter/tasks/setup_container.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-node-exporter : include_tasks systemd.yml] **************************
Saturday 05 June 2021  01:01:38 +0000 (0:00:00.082)       0:04:10.273 ********* 
included: /opt/ceph-ansible/roles/ceph-node-exporter/tasks/systemd.yml for node-2, node-3, node-4, node-6, node-1, node-5

TASK [ceph-node-exporter : ship systemd service] *******************************
Saturday 05 June 2021  01:01:38 +0000 (0:00:00.085)       0:04:10.359 ********* 
ok: [node-5]
ok: [node-4]
ok: [node-6]
ok: [node-2]
ok: [node-1]
ok: [node-3]

TASK [ceph-node-exporter : start the node_exporter service] ********************
Saturday 05 June 2021  01:01:39 +0000 (0:00:00.386)       0:04:10.745 ********* 
ok: [node-5]
ok: [node-6]
ok: [node-3]
ok: [node-1]
ok: [node-4]
ok: [node-2]

TASK [set ceph node exporter install 'Complete'] *******************************
Saturday 05 June 2021  01:01:39 +0000 (0:00:00.589)       0:04:11.335 ********* 
ok: [node-2]

PLAY [grafana-server] **********************************************************

TASK [set ceph grafana install 'In Progress'] **********************************
Saturday 05 June 2021  01:01:39 +0000 (0:00:00.031)       0:04:11.366 ********* 
ok: [node-5]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  01:01:39 +0000 (0:00:00.029)       0:04:11.396 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-5

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  01:01:39 +0000 (0:00:00.056)       0:04:11.453 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.136)       0:04:11.589 ********* 
ok: [node-5]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.032)       0:04:11.622 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.146)       0:04:11.768 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.037)       0:04:11.806 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.028)       0:04:11.834 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.028)       0:04:11.862 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.034)       0:04:11.897 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.047)       0:04:11.944 ********* 
skipping: [node-5]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.032)       0:04:11.977 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.049)       0:04:12.027 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.180)       0:04:12.208 ********* 
ok: [node-5 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:40.611727', 'end': '2021-06-05 01:01:40.615304', 'delta': '0:00:00.003577', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.213)       0:04:12.421 ********* 
ok: [node-5] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:40.831032', 'end': '2021-06-05 01:01:40.833958', 'delta': '0:00:00.002926', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:01:40.611727', 'end': '2021-06-05 01:01:40.615304', 'delta': '0:00:00.003577', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.055)       0:04:12.477 ********* 
skipping: [node-5] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.044)       0:04:12.521 ********* 
skipping: [node-5]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  01:01:40 +0000 (0:00:00.032)       0:04:12.554 ********* 
ok: [node-5 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.737)       0:04:13.291 ********* 
skipping: [node-5]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.031)       0:04:13.323 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.026)       0:04:13.350 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.025)       0:04:13.375 ********* 
ok: [node-5]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.032)       0:04:13.408 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.028)       0:04:13.437 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.028)       0:04:13.466 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.027)       0:04:13.493 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.029)       0:04:13.522 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  01:01:41 +0000 (0:00:00.029)       0:04:13.552 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.027)       0:04:13.579 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.026)       0:04:13.606 ********* 
skipping: [node-5]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.026)       0:04:13.633 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.027)       0:04:13.661 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.028)       0:04:13.689 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.026)       0:04:13.715 ********* 
skipping: [node-5] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['virtio-72986ef8-05ca-476b-b'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['virtio-4dd33f44-b169-493c-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-120db002-5687-4f40-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-120db002-5687-4f40-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-5] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.291)       0:04:14.007 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.028)       0:04:14.035 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.069)       0:04:14.105 ********* 
skipping: [node-5]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.027)       0:04:14.133 ********* 
ok: [node-5]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.135)       0:04:14.268 ********* 
ok: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.028)       0:04:14.297 ********* 
skipping: [node-5]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.029)       0:04:14.327 ********* 
skipping: [node-5]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.031)       0:04:14.358 ********* 
ok: [node-5 -> node-2]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.157)       0:04:14.515 ********* 
ok: [node-5]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  01:01:42 +0000 (0:00:00.031)       0:04:14.546 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.050)       0:04:14.597 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.051)       0:04:14.648 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.054)       0:04:14.703 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.052)       0:04:14.755 ********* 
skipping: [node-5] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.051)       0:04:14.806 ********* 
skipping: [node-5] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.049)       0:04:14.856 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.031)       0:04:14.887 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.030)       0:04:14.918 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.029)       0:04:14.948 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.030)       0:04:14.979 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.029)       0:04:15.009 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.030)       0:04:15.039 ********* 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.036)       0:04:15.075 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.030)       0:04:15.105 ********* 
skipping: [node-5] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.034)       0:04:15.140 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.031)       0:04:15.172 ********* 
skipping: [node-5] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.033)       0:04:15.206 ********* 
skipping: [node-5]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.034)       0:04:15.240 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.050)       0:04:15.291 ********* 
ok: [node-5 -> node-2] => (item=node-2)

TASK [ceph-facts : set grafana_server_addr fact - ipv4] ************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.044)       0:04:15.336 ********* 
ok: [node-5]

TASK [ceph-facts : set grafana_server_addr fact - ipv6] ************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.030)       0:04:15.366 ********* 
skipping: [node-5]

TASK [ceph-facts : set grafana_server_addrs fact - ipv4] ***********************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.022)       0:04:15.388 ********* 
ok: [node-5] => (item=node-5)

TASK [ceph-facts : set grafana_server_addrs fact - ipv6] ***********************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.040)       0:04:15.428 ********* 
skipping: [node-5] => (item=node-5) 

TASK [ceph-prometheus : create prometheus directories] *************************
Saturday 05 June 2021  01:01:43 +0000 (0:00:00.031)       0:04:15.460 ********* 
ok: [node-5] => (item=/etc/prometheus)
ok: [node-5] => (item=/var/lib/prometheus)

TASK [ceph-prometheus : write prometheus config file] **************************
Saturday 05 June 2021  01:01:44 +0000 (0:00:00.276)       0:04:15.736 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-5]

TASK [ceph-prometheus : make sure the alerting rules directory exists] *********
Saturday 05 June 2021  01:01:44 +0000 (0:00:00.216)       0:04:15.953 ********* 
ok: [node-5]

TASK [ceph-prometheus : copy alerting rules] ***********************************
Saturday 05 June 2021  01:01:44 +0000 (0:00:00.136)       0:04:16.089 ********* 
ok: [node-5]

TASK [ceph-prometheus : create alertmanager directories] ***********************
Saturday 05 June 2021  01:01:44 +0000 (0:00:00.282)       0:04:16.371 ********* 
ok: [node-5] => (item=/etc/alertmanager)
ok: [node-5] => (item=/var/lib/alertmanager)

TASK [ceph-prometheus : write alertmanager config file] ************************
Saturday 05 June 2021  01:01:45 +0000 (0:00:00.275)       0:04:16.647 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-5]

TASK [ceph-prometheus : include setup_container.yml] ***************************
Saturday 05 June 2021  01:01:45 +0000 (0:00:00.191)       0:04:16.838 ********* 
included: /opt/ceph-ansible/roles/ceph-prometheus/tasks/setup_container.yml for node-5

TASK [ceph-prometheus : include_tasks systemd.yml] *****************************
Saturday 05 June 2021  01:01:45 +0000 (0:00:00.025)       0:04:16.864 ********* 
included: /opt/ceph-ansible/roles/ceph-prometheus/tasks/systemd.yml for node-5

TASK [ceph-prometheus : ship systemd services] *********************************
Saturday 05 June 2021  01:01:45 +0000 (0:00:00.029)       0:04:16.894 ********* 
ok: [node-5] => (item=alertmanager.service)
ok: [node-5] => (item=prometheus.service)

TASK [ceph-prometheus : start prometheus services] *****************************
Saturday 05 June 2021  01:01:45 +0000 (0:00:00.616)       0:04:17.510 ********* 
ok: [node-5] => (item=prometheus)
ok: [node-5] => (item=alertmanager)

TASK [ceph-grafana : include setup_container.yml] ******************************
Saturday 05 June 2021  01:01:46 +0000 (0:00:00.853)       0:04:18.364 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/setup_container.yml for node-5

TASK [ceph-grafana : create /etc/grafana and /var/lib/grafana] *****************
Saturday 05 June 2021  01:01:46 +0000 (0:00:00.026)       0:04:18.390 ********* 
changed: [node-5] => (item=/etc/grafana)
ok: [node-5] => (item=/var/lib/grafana)
[WARNING]: The value 472 (type int) in a string field was converted to '472'
(type string). If this does not look like what you expect, quote the entire
value to ensure it does not change.

TASK [ceph-grafana : include_tasks systemd.yml] ********************************
Saturday 05 June 2021  01:01:47 +0000 (0:00:00.298)       0:04:18.688 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/systemd.yml for node-5

TASK [ceph-grafana : ship systemd service] *************************************
Saturday 05 June 2021  01:01:47 +0000 (0:00:00.032)       0:04:18.721 ********* 
ok: [node-5]

TASK [ceph-grafana : start the grafana-server service] *************************
Saturday 05 June 2021  01:01:47 +0000 (0:00:00.296)       0:04:19.017 ********* 
ok: [node-5]

TASK [ceph-grafana : include configure_grafana.yml] ****************************
Saturday 05 June 2021  01:01:47 +0000 (0:00:00.491)       0:04:19.508 ********* 
included: /opt/ceph-ansible/roles/ceph-grafana/tasks/configure_grafana.yml for node-5

TASK [ceph-grafana : install ceph-grafana-dashboards package on RedHat or SUSE] ***
Saturday 05 June 2021  01:01:47 +0000 (0:00:00.033)       0:04:19.542 ********* 
skipping: [node-5]

TASK [ceph-grafana : make sure grafana is down] ********************************
Saturday 05 June 2021  01:01:48 +0000 (0:00:00.028)       0:04:19.570 ********* 
changed: [node-5]

TASK [ceph-grafana : wait for grafana to be stopped] ***************************
Saturday 05 June 2021  01:02:03 +0000 (0:00:15.596)       0:04:35.167 ********* 
ok: [node-5]

TASK [ceph-grafana : make sure grafana configuration directories exist] ********
Saturday 05 June 2021  01:02:03 +0000 (0:00:00.289)       0:04:35.456 ********* 
ok: [node-5] => (item=/etc/grafana/dashboards/ceph-dashboard)
ok: [node-5] => (item=/etc/grafana/provisioning/datasources)
ok: [node-5] => (item=/etc/grafana/provisioning/dashboards)
ok: [node-5] => (item=/etc/grafana/provisioning/notifiers)

TASK [ceph-grafana : download ceph grafana dashboards] *************************
Saturday 05 June 2021  01:02:04 +0000 (0:00:00.585)       0:04:36.041 ********* 
ok: [node-5] => (item=ceph-cluster.json)
ok: [node-5] => (item=cephfs-overview.json)
ok: [node-5] => (item=host-details.json)
ok: [node-5] => (item=hosts-overview.json)
ok: [node-5] => (item=osd-device-details.json)
ok: [node-5] => (item=osds-overview.json)
ok: [node-5] => (item=pool-detail.json)
ok: [node-5] => (item=pool-overview.json)
ok: [node-5] => (item=radosgw-detail.json)
ok: [node-5] => (item=radosgw-overview.json)
ok: [node-5] => (item=rbd-overview.json)

TASK [ceph-grafana : write grafana.ini] ****************************************
Saturday 05 June 2021  01:02:10 +0000 (0:00:06.519)       0:04:42.560 ********* 
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-5]

TASK [ceph-grafana : write datasources provisioning config file] ***************
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.193)       0:04:42.754 ********* 
ok: [node-5]

TASK [ceph-grafana : Write dashboards provisioning config file] ****************
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.298)       0:04:43.053 ********* 
ok: [node-5]

TASK [ceph-grafana : copy grafana SSL certificate file] ************************
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.304)       0:04:43.357 ********* 
skipping: [node-5]

TASK [ceph-grafana : copy grafana SSL certificate key] *************************
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.026)       0:04:43.384 ********* 
skipping: [node-5]

TASK [ceph-grafana : generate a Self Signed OpenSSL certificate for dashboard] ***
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.026)       0:04:43.410 ********* 
changed: [node-5]

TASK [ceph-grafana : enable and start grafana] *********************************
Saturday 05 June 2021  01:02:11 +0000 (0:00:00.144)       0:04:43.555 ********* 
changed: [node-5]

TASK [ceph-grafana : wait for grafana to start] ********************************
Saturday 05 June 2021  01:02:16 +0000 (0:00:04.667)       0:04:48.222 ********* 
ok: [node-5]

TASK [set ceph grafana install 'Complete'] *************************************
Saturday 05 June 2021  01:02:43 +0000 (0:00:27.187)       0:05:15.409 ********* 
ok: [node-5]

PLAY [['node-1']] **************************************************************

TASK [set ceph dashboard install 'In Progress'] ********************************
Saturday 05 June 2021  01:02:43 +0000 (0:00:00.029)       0:05:15.438 ********* 
ok: [node-1]

TASK [ceph-facts : include facts.yml] ******************************************
Saturday 05 June 2021  01:02:43 +0000 (0:00:00.021)       0:05:15.460 ********* 
included: /opt/ceph-ansible/roles/ceph-facts/tasks/facts.yml for node-1

TASK [ceph-facts : check if it is atomic host] *********************************
Saturday 05 June 2021  01:02:43 +0000 (0:00:00.053)       0:05:15.514 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact is_atomic] *****************************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.144)       0:05:15.659 ********* 
ok: [node-1]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.025)       0:05:15.684 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.147)       0:05:15.831 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python] *********************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.031)       0:05:15.863 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact discovered_interpreter_python if not previously set] ***
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.025)       0:05:15.888 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_release ceph_stable_release] ******************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.027)       0:05:15.916 ********* 
ok: [node-1]

TASK [ceph-facts : set_fact monitor_name ansible_facts['hostname']] ************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.033)       0:05:15.950 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.045)       0:05:15.995 ********* 
skipping: [node-1]

TASK [ceph-facts : find a running mon container] *******************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.032)       0:05:16.028 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : check for a ceph mon socket] ********************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.046)       0:05:16.074 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : check if the ceph mon socket is in-use] *********************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.181)       0:05:16.255 ********* 
ok: [node-1 -> node-2] => (item={'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:02:44.657688', 'end': '2021-06-05 01:02:44.660104', 'delta': '0:00:00.002416', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - non_container] ***********************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.202)       0:05:16.458 ********* 
ok: [node-1] => (item={'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:02:44.841736', 'end': '2021-06-05 01:02:44.844201', 'delta': '0:00:00.002465', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'cmd': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', 'stdout': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:02:44.657688', 'end': '2021-06-05 01:02:44.660104', 'delta': '0:00:00.002416', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'stat --printf=%n /var/run/ceph/ceph-mon*.asok', '_uses_shell': True, 'warn': True, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['/var/run/ceph/ceph-mon.ceph-research-3.asok'], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': 'node-2', 'ansible_loop_var': 'item'}, 'ansible_loop_var': 'item'})

TASK [ceph-facts : set_fact running_mon - container] ***************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.053)       0:05:16.512 ********* 
skipping: [node-1] => (item={'changed': False, 'skipped': True, 'skip_reason': 'Conditional result was False', 'item': 'node-2', 'ansible_loop_var': 'item'}) 

TASK [ceph-facts : set_fact _container_exec_cmd] *******************************
Saturday 05 June 2021  01:02:44 +0000 (0:00:00.042)       0:05:16.554 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid if cluster is already running] *************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.032)       0:05:16.587 ********* 
ok: [node-1 -> node-2]

TASK [ceph-facts : set_fact current_fsid rc 1] *********************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.551)       0:05:17.138 ********* 
skipping: [node-1]

TASK [ceph-facts : get current fsid] *******************************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.030)       0:05:17.168 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.026)       0:05:17.195 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid from current_fsid] ****************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.026)       0:05:17.221 ********* 
ok: [node-1]

TASK [ceph-facts : generate cluster fsid] **************************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.033)       0:05:17.255 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact fsid] **********************************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.282 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve device link(s)] *************************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.310 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build devices from resolved symlinks] **************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.028)       0:05:17.338 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final devices list] **************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.365 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve dedicated_device link(s)] ***************************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.025)       0:05:17.391 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build dedicated_devices from resolved symlinks] ****
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.029)       0:05:17.421 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final dedicated_devices list] ****************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.448 ********* 
skipping: [node-1]

TASK [ceph-facts : resolve bluestore_wal_device link(s)] ***********************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.026)       0:05:17.474 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build bluestore_wal_devices from resolved symlinks] ***
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.502 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact build final bluestore_wal_devices list] ************
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.027)       0:05:17.529 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact devices generate device list when osd_auto_discovery] ***
Saturday 05 June 2021  01:02:45 +0000 (0:00:00.026)       0:05:17.556 ********* 
skipping: [node-1] => (item={'key': 'loop1', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '143120', 'sectorsize': '512', 'size': '69.88 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-1', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300', 'dm-uuid-LVM-wgWMqdbfQdGC8GroSYVNTfyWAZgLNcCrNcMjTUULQMG1KRQYi8Uft4YrU2MIqacu'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop6', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdb', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-EXifKY-q7c5-32qb-hyqR-cFzw-xS81-g7YAk7', 'virtio-c5f9558d-2605-4811-b'], 'uuids': [], 'labels': [], 'masters': ['dm-0']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a']}}) 
skipping: [node-1] => (item={'key': 'loop4', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '65744', 'sectorsize': '512', 'size': '32.10 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop2', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '66104', 'sectorsize': '512', 'size': '32.28 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop0', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113592', 'sectorsize': '512', 'size': '55.46 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'dm-0', 'value': {'virtual': 1, 'links': {'ids': ['dm-name-ceph--b01be05b--4153--45fa--bb8c--d600ebc08c69-osd--block--1940e407--6621--4557--abf4--225550129e7a', 'dm-uuid-LVM-AOE9884cinmeZUShh4MKCK52mkK0qf1Su6TTI6kz3H6OFAMvKfsbraILpKbXOtK3'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': '', 'sectors': '41934848', 'sectorsize': '512', 'size': '20.00 GB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop7', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '0', 'sectorsize': '512', 'size': '0.00 Bytes', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vdc', 'value': {'virtual': 1, 'links': {'ids': ['lvm-pv-uuid-dI5Ywj-ygtE-FHR2-aaTB-dSHp-rYTb-GQiXs2', 'virtio-38fadbeb-1428-4074-8'], 'uuids': [], 'labels': [], 'masters': ['dm-1']}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '41943040', 'sectorsize': '512', 'size': '20.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': ['ceph--a1dd2ec2--8608--490a--bc9b--a07b93cc46de-osd--block--7002b55d--1337--4d77--8ca9--b47103b58300']}}) 
skipping: [node-1] => (item={'key': 'loop5', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '138376', 'sectorsize': '512', 'size': '67.57 MB', 'host': '', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'vda', 'value': {'virtual': 1, 'links': {'ids': ['virtio-b8707316-9d3d-4662-a'], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': '0x1af4', 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '0', 'partitions': {'vda15': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part15'], 'uuids': ['F9F8-C1B8'], 'labels': ['UEFI'], 'masters': []}, 'start': '10240', 'sectors': '217088', 'sectorsize': 512, 'size': '106.00 MB', 'uuid': 'F9F8-C1B8', 'holders': []}, 'vda1': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part1'], 'uuids': ['72674ca3-7e52-48dd-8a9d-79b1b9639662'], 'labels': ['cloudimg-rootfs'], 'masters': []}, 'start': '227328', 'sectors': '209487839', 'sectorsize': 512, 'size': '99.89 GB', 'uuid': '72674ca3-7e52-48dd-8a9d-79b1b9639662', 'holders': []}, 'vda14': {'links': {'ids': ['virtio-b8707316-9d3d-4662-a-part14'], 'uuids': [], 'labels': [], 'masters': []}, 'start': '2048', 'sectors': '8192', 'sectorsize': 512, 'size': '4.00 MB', 'uuid': None, 'holders': []}}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '209715200', 'sectorsize': '512', 'size': '100.00 GB', 'host': 'SCSI storage controller: Red Hat, Inc. Virtio block device', 'holders': []}}) 
skipping: [node-1] => (item={'key': 'loop3', 'value': {'virtual': 1, 'links': {'ids': [], 'uuids': [], 'labels': [], 'masters': []}, 'vendor': None, 'model': None, 'sas_address': None, 'sas_device_handle': None, 'removable': '0', 'support_discard': '4096', 'partitions': {}, 'rotational': '1', 'scheduler_mode': 'mq-deadline', 'sectors': '113504', 'sectorsize': '512', 'size': '55.42 MB', 'host': '', 'holders': []}}) 

TASK [ceph-facts : get ceph current status] ************************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.341)       0:05:17.897 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_current_status] *******************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.028)       0:05:17.926 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_hostname] **************************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.029)       0:05:17.956 ********* 
skipping: [node-1]

TASK [ceph-facts : check if the ceph conf exists] ******************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.027)       0:05:17.983 ********* 
ok: [node-1]

TASK [ceph-facts : set default osd_pool_default_crush_rule fact] ***************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.158)       0:05:18.141 ********* 
ok: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.031)       0:05:18.173 ********* 
ok: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.150)       0:05:18.324 ********* 
skipping: [node-1]

TASK [ceph-facts : read osd pool default crush rule] ***************************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.031)       0:05:18.356 ********* 
skipping: [node-1]

TASK [ceph-facts : set osd_pool_default_crush_rule fact] ***********************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.028)       0:05:18.384 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv4] ***
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.028)       0:05:18.413 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address_block ipv6] ***
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.053)       0:05:18.466 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_address] *************
Saturday 05 June 2021  01:02:46 +0000 (0:00:00.053)       0:05:18.520 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv4] ****
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.053)       0:05:18.573 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _monitor_addresses to monitor_interface - ipv6] ****
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.051)       0:05:18.624 ********* 
skipping: [node-1] => (item=node-2) 

TASK [ceph-facts : set_fact _current_monitor_address] **************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.051)       0:05:18.676 ********* 
skipping: [node-1] => (item={'name': 'node-2', 'addr': '100.200.23.57'}) 

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv4] ****
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.052)       0:05:18.728 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address_block ipv6] ****
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.031)       0:05:18.760 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_address] ***************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.030)       0:05:18.791 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _interface] ****************************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.030)       0:05:18.821 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv4] ******
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.031)       0:05:18.853 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact _radosgw_address to radosgw_interface - ipv6] ******
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.032)       0:05:18.885 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances without rgw multisite] ***************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.034)       0:05:18.919 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact is_rgw_instances_defined] **************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.036)       0:05:18.956 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances with rgw multisite] ******************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.031)       0:05:18.988 ********* 
skipping: [node-1] => (item=0) 

TASK [ceph-facts : set_fact rgw_instances_host] ********************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.035)       0:05:19.023 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact rgw_instances_all] *********************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.032)       0:05:19.055 ********* 
skipping: [node-1] => (item=node-6) 

TASK [ceph-facts : set_fact use_new_ceph_iscsi package or old ceph-iscsi-config/cli] ***
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.033)       0:05:19.089 ********* 
skipping: [node-1]

TASK [ceph-facts : set_fact ceph_run_cmd] **************************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.035)       0:05:19.124 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set_fact ceph_admin_command] ********************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.050)       0:05:19.175 ********* 
ok: [node-1 -> node-2] => (item=node-2)

TASK [ceph-facts : set grafana_server_addr fact - ipv4] ************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.046)       0:05:19.221 ********* 
skipping: [node-1]

TASK [ceph-facts : set grafana_server_addr fact - ipv6] ************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.027)       0:05:19.248 ********* 
skipping: [node-1]

TASK [ceph-facts : set grafana_server_addrs fact - ipv4] ***********************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.023)       0:05:19.271 ********* 
ok: [node-1] => (item=node-5)

TASK [ceph-facts : set grafana_server_addrs fact - ipv6] ***********************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.042)       0:05:19.313 ********* 
skipping: [node-1] => (item=node-5) 

TASK [ceph-dashboard : include configure_dashboard.yml] ************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.031)       0:05:19.345 ********* 
included: /opt/ceph-ansible/roles/ceph-dashboard/tasks/configure_dashboard.yml for node-1

TASK [ceph-dashboard : set_fact container_exec_cmd] ****************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.051)       0:05:19.397 ********* 
skipping: [node-1]

TASK [ceph-dashboard : set_fact container_run_cmd] *****************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.026)       0:05:19.423 ********* 
ok: [node-1]

TASK [ceph-dashboard : get SSL status for dashboard] ***************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.035)       0:05:19.458 ********* 
skipping: [node-1]

TASK [ceph-dashboard : disable SSL for dashboard] ******************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.032)       0:05:19.490 ********* 
skipping: [node-1]

TASK [ceph-dashboard : enable SSL for dashboard] *******************************
Saturday 05 June 2021  01:02:47 +0000 (0:00:00.031)       0:05:19.522 ********* 
changed: [node-1 -> node-2]

TASK [ceph-dashboard : copy dashboard SSL certificate file] ********************
Saturday 05 June 2021  01:02:49 +0000 (0:00:01.094)       0:05:20.616 ********* 
skipping: [node-1]

TASK [ceph-dashboard : copy dashboard SSL certificate key] *********************
Saturday 05 June 2021  01:02:49 +0000 (0:00:00.031)       0:05:20.647 ********* 
skipping: [node-1]

TASK [ceph-dashboard : generate a Self Signed OpenSSL certificate for dashboard] ***
Saturday 05 June 2021  01:02:49 +0000 (0:00:00.025)       0:05:20.673 ********* 
changed: [node-1]

TASK [ceph-dashboard : slurp self-signed generated certificate for dashboard] ***
Saturday 05 June 2021  01:02:49 +0000 (0:00:00.255)       0:05:20.928 ********* 
ok: [node-1] => (item=ceph-dashboard.key)
ok: [node-1] => (item=ceph-dashboard.crt)

TASK [ceph-dashboard : copy self-signed generated certificate on mons] *********
Saturday 05 June 2021  01:02:49 +0000 (0:00:00.467)       0:05:21.395 ********* 
changed: [node-1 -> node-2] => (item=[{'content': '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', 'source': '/etc/ceph/ceph-dashboard.key', 'encoding': 'base64', 'invocation': {'module_args': {'src': '/etc/ceph/ceph-dashboard.key'}}, 'failed': False, 'changed': False, 'item': 'ceph-dashboard.key', 'ansible_loop_var': 'item'}, 'node-2'])
changed: [node-1 -> node-2] => (item=[{'content': 'LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURMVENDQWhXZ0F3SUJBZ0lVSXk3K1hOU1Y4bU5ubmlJNmN4dFpXeVhVTyt3d0RRWUpLb1pJaHZjTkFRRUwKQlFBd0pqRUxNQWtHQTFVRUNnd0NTVlF4RnpBVkJnTlZCQU1NRG1ObGNHZ3RaR0Z6YUdKdllYSmtNQjRYRFRJeApNRFl3TlRBeE1ESTBPVm9YRFRNeE1EWXdNekF4TURJME9Wb3dKakVMTUFrR0ExVUVDZ3dDU1ZReEZ6QVZCZ05WCkJBTU1EbU5sY0dndFpHRnphR0p2WVhKa01JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0MKQVFFQTNEMkpCVE14Qy9wK2hMSEtVWllhRjZTRms3NDRzeStGSkpUTzdnZW9tazVpUStqQnNMOWhZZUpBRmswdgo2M2FMR3ptVEhlME9hbXZ5YitrMWQrM3FNSk5EUmpMbEIwMjRjSEYzcTMxYm9JWWFCcVhwRjl5cHRrWTNJU3dCCm1yVG9UTU1nbGIxOVBKNGxmRWpYVlFoL0VmbnpEMUxOdDZEUGZhRVQ1NllENGhxbjd3MURMMmdoVXlUbDRlMmYKRmh5R2luR01WM2pEM2pOYmp5MWxicGIxOHNLbE9aMXlBT1FPdml2cE5haGRvRk9HQnk4Vks4QzhickZiOHVRTQpHcEdST2NRWlo1a25SRm91SWNvek1CVUIyVEgyZVZuTVJUM0ZWT3p3V2JtQmhRVkVnTXhiZUpYTHJlcnpSZnhzCnY3dEVVb2pEa3N3bkZENWp6OEZDekkvM2ZRSURBUUFCbzFNd1VUQWRCZ05WSFE0RUZnUVVlb3JBSVpkT3dGOTMKZURTUDJHeFJsT0VuRjQ4d0h3WURWUjBqQkJnd0ZvQVVlb3JBSVpkT3dGOTNlRFNQMkd4UmxPRW5GNDh3RHdZRApWUjBUQVFIL0JBVXdBd0VCL3pBTkJna3Foa2lHOXcwQkFRc0ZBQU9DQVFFQTArcjhwUnplVms4MFFTcUQxN0FtClJ1ajh3bzVFNTRSK2xIcHdQSnBSOEVNTHpZcXVTazM4SzgvY2JLOFdRWTJWV3hBYUVpbXNOc2dPNVJzQUl3TWIKc3dCbzNTa3pjbk1RaGdpOXlMNFUvTnhLSmYrdjBnSWVHNjFneUJjTnh6U3VUaURYTE9IZmlvSUNmY0l4aTQ5WApVVVloVEV2bVJiVkJveTBUWGd5aGJoMkRvWjZSU04rRnhIUTBtN01NOFk3Z3g0U3N6ZXowUld4eW44NzhlY1krCms3WUVwNUl3MTI2Uzc5RzFrUElOS3dld0NNWWRieWFybnNQaEs5WXJPQlZLNUUyM3lHNlpBRHRIUWtPS2wxeGkKWVpFYVoyTXR6bmY5TG9CSmdPMXhVemZPTzN5R05KZG5CNVlPNUliMVF4WUlkNGFremtsRHF1VXpQRmtyUDJTcwppUT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K', 'source': '/etc/ceph/ceph-dashboard.crt', 'encoding': 'base64', 'invocation': {'module_args': {'src': '/etc/ceph/ceph-dashboard.crt'}}, 'failed': False, 'changed': False, 'item': 'ceph-dashboard.crt', 'ansible_loop_var': 'item'}, 'node-2'])

TASK [ceph-dashboard : import dashboard certificate file] **********************
Saturday 05 June 2021  01:02:50 +0000 (0:00:00.735)       0:05:22.131 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : import dashboard certificate key] ***********************
Saturday 05 June 2021  01:02:52 +0000 (0:00:01.929)       0:05:24.060 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the dashboard port (8443)] **************************
Saturday 05 June 2021  01:02:53 +0000 (0:00:01.337)       0:05:25.397 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the dashboard SSL port (8443)] **********************
Saturday 05 June 2021  01:02:54 +0000 (0:00:01.008)       0:05:26.406 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : include_tasks] ******************************************
Saturday 05 June 2021  01:02:55 +0000 (0:00:00.924)       0:05:27.330 ********* 
included: /opt/ceph-ansible/roles/ceph-dashboard/tasks/configure_dashboard_backends.yml for node-1

TASK [ceph-dashboard : get current mgr backend - ipv4] *************************
Saturday 05 June 2021  01:02:55 +0000 (0:00:00.047)       0:05:27.378 ********* 
ok: [node-1]

TASK [ceph-dashboard : get current mgr backend - ipv6] *************************
Saturday 05 June 2021  01:02:55 +0000 (0:00:00.037)       0:05:27.415 ********* 
skipping: [node-1]

TASK [ceph-dashboard : config the current dashboard backend] *******************
Saturday 05 June 2021  01:02:55 +0000 (0:00:00.035)       0:05:27.451 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : disable mgr dashboard module (restart)] *****************
Saturday 05 June 2021  01:02:57 +0000 (0:00:01.198)       0:05:28.649 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : enable mgr dashboard module (restart)] ******************
Saturday 05 June 2021  01:02:58 +0000 (0:00:01.425)       0:05:30.075 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : check dashboard password in file option command] ********
Saturday 05 June 2021  01:03:00 +0000 (0:00:01.771)       0:05:31.846 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set_fact dashboard_password_from_stdin] *****************
Saturday 05 June 2021  01:03:00 +0000 (0:00:00.387)       0:05:32.234 ********* 
ok: [node-1]

TASK [ceph-dashboard : create dashboard admin user] ****************************
Saturday 05 June 2021  01:03:00 +0000 (0:00:00.033)       0:05:32.267 ********* 
changed: [node-1 -> node-2]

TASK [ceph-dashboard : set grafana api user] ***********************************
Saturday 05 June 2021  01:03:13 +0000 (0:00:12.515)       0:05:44.783 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set grafana api password] *******************************
Saturday 05 June 2021  01:03:14 +0000 (0:00:01.009)       0:05:45.793 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set grafana api password (legacy)] **********************
Saturday 05 June 2021  01:03:15 +0000 (0:00:01.492)       0:05:47.286 ********* 
skipping: [node-1]

TASK [ceph-dashboard : disable ssl verification for grafana] *******************
Saturday 05 June 2021  01:03:15 +0000 (0:00:00.032)       0:05:47.318 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set alertmanager host] **********************************
Saturday 05 June 2021  01:03:16 +0000 (0:00:01.044)       0:05:48.362 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set prometheus host] ************************************
Saturday 05 June 2021  01:03:17 +0000 (0:00:01.145)       0:05:49.508 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : include_tasks] ******************************************
Saturday 05 June 2021  01:03:19 +0000 (0:00:01.244)       0:05:50.752 ********* 
included: /opt/ceph-ansible/roles/ceph-dashboard/tasks/configure_grafana_layouts.yml for node-1

TASK [ceph-dashboard : set grafana url] ****************************************
Saturday 05 June 2021  01:03:19 +0000 (0:00:00.053)       0:05:50.805 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : inject grafana dashboard layouts] ***********************
Saturday 05 June 2021  01:03:20 +0000 (0:00:01.430)       0:05:52.235 ********* 
skipping: [node-1]

TASK [ceph-dashboard : config grafana api url vip] *****************************
Saturday 05 June 2021  01:03:20 +0000 (0:00:00.035)       0:05:52.271 ********* 
skipping: [node-1]

TASK [ceph-dashboard : config alertmanager api url] ****************************
Saturday 05 June 2021  01:03:20 +0000 (0:00:00.030)       0:05:52.301 ********* 
skipping: [node-1]

TASK [ceph-dashboard : config prometheus api url] ******************************
Saturday 05 June 2021  01:03:20 +0000 (0:00:00.030)       0:05:52.332 ********* 
skipping: [node-1]

TASK [ceph-dashboard : create radosgw system user] *****************************
Saturday 05 June 2021  01:03:20 +0000 (0:00:00.030)       0:05:52.362 ********* 
changed: [node-1 -> node-2]

TASK [ceph-dashboard : get the rgw access and secret keys] *********************
Saturday 05 June 2021  01:04:20 +0000 (0:00:59.837)       0:06:52.200 ********* 
ok: [node-1]

TASK [ceph-dashboard : set the rgw user] ***************************************
Saturday 05 June 2021  01:04:20 +0000 (0:00:00.035)       0:06:52.235 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw access key] *********************************
Saturday 05 June 2021  01:04:21 +0000 (0:00:01.146)       0:06:53.381 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw access key (legacy)] ************************
Saturday 05 June 2021  01:04:23 +0000 (0:00:01.405)       0:06:54.787 ********* 
skipping: [node-1]

TASK [ceph-dashboard : set the rgw secret key] *********************************
Saturday 05 June 2021  01:04:23 +0000 (0:00:00.033)       0:06:54.821 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw secret key (legacy)] ************************
Saturday 05 June 2021  01:04:24 +0000 (0:00:01.077)       0:06:55.898 ********* 
skipping: [node-1]

TASK [ceph-dashboard : set the rgw host] ***************************************
Saturday 05 June 2021  01:04:24 +0000 (0:00:00.033)       0:06:55.931 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw port] ***************************************
Saturday 05 June 2021  01:04:25 +0000 (0:00:01.101)       0:06:57.033 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw scheme] *************************************
Saturday 05 June 2021  01:04:26 +0000 (0:00:00.789)       0:06:57.822 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : set the rgw admin resource] *****************************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.954)       0:06:58.777 ********* 
skipping: [node-1]

TASK [ceph-dashboard : disable ssl verification for rgw] ***********************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.033)       0:06:58.810 ********* 
skipping: [node-1]

TASK [ceph-dashboard : disable iscsi api ssl verification] *********************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.033)       0:06:58.844 ********* 
skipping: [node-1]

TASK [ceph-dashboard : add iscsi gateways - ipv4] ******************************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.032)       0:06:58.876 ********* 
skipping: [node-1] => (item=None) 

TASK [ceph-dashboard : add iscsi gateways - ipv4 (legacy)] *********************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.033)       0:06:58.910 ********* 
skipping: [node-1] => (item=None) 

TASK [ceph-dashboard : add iscsi gateways - ipv6] ******************************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.033)       0:06:58.944 ********* 
skipping: [node-1] => (item=None) 

TASK [ceph-dashboard : add iscsi gateways - ipv6 (legacy)] *********************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.033)       0:06:58.977 ********* 
skipping: [node-1] => (item=None) 

TASK [ceph-dashboard : disable mgr dashboard module (restart)] *****************
Saturday 05 June 2021  01:04:27 +0000 (0:00:00.038)       0:06:59.015 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : enable mgr dashboard module (restart)] ******************
Saturday 05 June 2021  01:04:29 +0000 (0:00:02.081)       0:07:01.096 ********* 
ok: [node-1 -> node-2]

TASK [ceph-dashboard : print dashboard URL] ************************************
Saturday 05 June 2021  01:04:31 +0000 (0:00:01.807)       0:07:02.904 ********* 
ok: [node-1] => 
  msg: The dashboard has been deployed! You can access your dashboard web UI at https://host-100-200-23-227.openstacklocal:8443/ as an 'admin' user with '2wsx1qaz' password.

TASK [set ceph dashboard install 'Complete'] ***********************************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.025)       0:07:02.930 ********* 
ok: [node-1]

PLAY [mons,osds,mdss,rgws,rbdmirrors,mgrs] *************************************

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.032)       0:07:02.962 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.167)       0:07:03.129 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]

TASK [ceph-handler : include check_running_containers.yml] *********************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.058)       0:07:03.188 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : include check_socket_non_container.yml] *******************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.047)       0:07:03.235 ********* 
included: /opt/ceph-ansible/roles/ceph-handler/tasks/check_socket_non_container.yml for node-2, node-3, node-4, node-6, node-1

TASK [ceph-handler : find ceph mon socket] *************************************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.070)       0:07:03.306 ********* 
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]
ok: [node-2]

TASK [ceph-handler : check if the ceph mon socket is in-use] *******************
Saturday 05 June 2021  01:04:31 +0000 (0:00:00.142)       0:07:03.449 ********* 
skipping: [node-4]
skipping: [node-3]
skipping: [node-6]
skipping: [node-1]
ok: [node-2] => (item={'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph mon socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.151)       0:07:03.600 ********* 
skipping: [node-3]
skipping: [node-2] => (item=[{'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-mon.ceph-research-3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:32.010232', 'end': '2021-06-05 01:04:32.012469', 'delta': '0:00:00.002237', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mon.ceph-research-3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-mon.ceph-research-3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 681, 'dev': 26, 'nlink': 1, 'atime': 1622852725.1092238, 'mtime': 1622852724.8772151, 'ctime': 1622852724.8772151, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.051)       0:07:03.652 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
ok: [node-3]
ok: [node-4]

TASK [ceph-handler : check if the ceph osd socket is in-use] *******************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.158)       0:07:03.810 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-1]
ok: [node-3] => (item={'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-4] => (item={'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-3] => (item={'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})
ok: [node-4] => (item={'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph osd socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.300)       0:07:04.110 ********* 
skipping: [node-2]
skipping: [node-6]
skipping: [node-3] => (item=[{'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.3.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:32.378244', 'end': '2021-06-05 01:04:32.380861', 'delta': '0:00:00.002617', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.3.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.3.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 724, 'dev': 26, 'nlink': 1, 'atime': 1622852848.4759579, 'mtime': 1622852848.4759579, 'ctime': 1622852848.4759579, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-3] => (item=[{'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.1.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:32.509876', 'end': '2021-06-05 01:04:32.511907', 'delta': '0:00:00.002031', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.1.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.1.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 702, 'dev': 26, 'nlink': 1, 'atime': 1622852827.559137, 'mtime': 1622852827.559137, 'ctime': 1622852827.559137, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-4] => (item=[{'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.2.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:32.386950', 'end': '2021-06-05 01:04:32.389171', 'delta': '0:00:00.002221', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.2.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 727, 'dev': 26, 'nlink': 1, 'atime': 1622852849.4091084, 'mtime': 1622852849.4091084, 'ctime': 1622852849.4091084, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-1]
skipping: [node-4] => (item=[{'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-osd.0.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:32.521530', 'end': '2021-06-05 01:04:32.524217', 'delta': '0:00:00.002687', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-osd.0.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-osd.0.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 699, 'dev': 26, 'nlink': 1, 'atime': 1622852827.5602508, 'mtime': 1622852827.5602508, 'ctime': 1622852827.5602508, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph osd socket] *************************************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.052)       0:07:04.163 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check if the ceph mds socket is in-use] *******************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.046)       0:07:04.209 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : remove ceph mds socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.050)       0:07:04.260 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : find ceph rgw socket] *************************************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.048)       0:07:04.308 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
ok: [node-6]

TASK [ceph-handler : check if the ceph rgw socket is in-use] *******************
Saturday 05 June 2021  01:04:32 +0000 (0:00:00.164)       0:07:04.472 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
ok: [node-6] => (item={'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.91452.93896065841656.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622855064.0974765, 'mtime': 1622855064.0974765, 'ctime': 1622855064.0974765, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph rgw socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.171)       0:07:04.643 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6] => (item=[{'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.91452.93896065841656.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622855064.0974765, 'mtime': 1622855064.0974765, 'ctime': 1622855064.0974765, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.91452.93896065841656.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:33.055902', 'end': '2021-06-05 01:04:33.058515', 'delta': '0:00:00.002613', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.91452.93896065841656.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-client.rgw.ceph-extension-2.rgw0.91452.93896065841656.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 629, 'dev': 26, 'nlink': 1, 'atime': 1622855064.0974765, 'mtime': 1622855064.0974765, 'ctime': 1622855064.0974765, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 
skipping: [node-1]

TASK [ceph-handler : find ceph mgr socket] *************************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.048)       0:07:04.692 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
ok: [node-1]

TASK [ceph-handler : check if the ceph mgr socket is in-use] *******************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.178)       0:07:04.870 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
ok: [node-1] => (item={'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 673, 'dev': 26, 'nlink': 1, 'atime': 1622855072.5708737, 'mtime': 1622855072.5708737, 'ctime': 1622855072.5708737, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False})

TASK [ceph-handler : remove ceph mgr socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.179)       0:07:05.050 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1] => (item=[{'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 673, 'dev': 26, 'nlink': 1, 'atime': 1622855072.5708737, 'mtime': 1622855072.5708737, 'ctime': 1622855072.5708737, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, {'cmd': ['grep', '-q', '/var/run/ceph/ceph-mgr.ceph-research-2.asok', '/proc/net/unix'], 'stdout': '', 'stderr': '', 'rc': 0, 'start': '2021-06-05 01:04:33.460949', 'end': '2021-06-05 01:04:33.463190', 'delta': '0:00:00.002241', 'changed': False, 'invocation': {'module_args': {'_raw_params': 'grep -q /var/run/ceph/ceph-mgr.ceph-research-2.asok /proc/net/unix', 'warn': True, '_uses_shell': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': [], 'failed': False, 'failed_when_result': False, 'item': {'path': '/var/run/ceph/ceph-mgr.ceph-research-2.asok', 'mode': '0755', 'isdir': False, 'ischr': False, 'isblk': False, 'isreg': False, 'isfifo': False, 'islnk': False, 'issock': True, 'uid': 64045, 'gid': 64045, 'size': 0, 'inode': 673, 'dev': 26, 'nlink': 1, 'atime': 1622855072.5708737, 'mtime': 1622855072.5708737, 'ctime': 1622855072.5708737, 'gr_name': 'ceph', 'pw_name': 'ceph', 'wusr': True, 'rusr': True, 'xusr': True, 'wgrp': False, 'rgrp': True, 'xgrp': True, 'woth': False, 'roth': True, 'xoth': True, 'isuid': False, 'isgid': False}, 'ansible_loop_var': 'item'}]) 

TASK [ceph-handler : find ceph rbd mirror socket] ******************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.056)       0:07:05.107 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check if the ceph rbd mirror socket is in-use] ************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.048)       0:07:05.155 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : remove ceph rbd mirror socket if exists and not used by a process] ***
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.050)       0:07:05.206 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check for a nfs ganesha pid] ******************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.049)       0:07:05.255 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check for a tcmu-runner] **********************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.049)       0:07:05.305 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-api] *******************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.049)       0:07:05.355 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check for a rbd-target-gw] ********************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.046)       0:07:05.401 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : check for a ceph-crash process] ***************************
Saturday 05 June 2021  01:04:33 +0000 (0:00:00.047)       0:07:05.449 ********* 
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-1]
ok: [node-6]

TASK [ceph-handler : set_fact handler_mon_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:01.235)       0:07:06.684 ********* 
ok: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_osd_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.044)       0:07:06.728 ********* 
skipping: [node-2]
ok: [node-3]
ok: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mds_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.049)       0:07:06.778 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rgw_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.046)       0:07:06.824 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
ok: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_nfs_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.044)       0:07:06.869 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_rbd_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.046)       0:07:06.916 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact handler_mgr_status] ******************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.049)       0:07:06.966 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
ok: [node-1]

TASK [ceph-handler : set_fact handler_crash_status] ****************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.056)       0:07:07.022 ********* 
ok: [node-2]
ok: [node-3]
ok: [node-4]
ok: [node-6]
ok: [node-1]

TASK [ceph-config : include create_ceph_initial_dirs.yml] **********************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.055)       0:07:07.077 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : include_tasks rgw_systemd_environment_file.yml] ************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.050)       0:07:07.128 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
included: /opt/ceph-ansible/roles/ceph-config/tasks/rgw_systemd_environment_file.yml for node-6

TASK [ceph-config : create rados gateway instance directories] *****************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.056)       0:07:07.185 ********* 
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-config : generate environment file] *********************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.159)       0:07:07.344 ********* 
skipping: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080}) 

TASK [ceph-config : reset num_osds] ********************************************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.107)       0:07:07.451 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : count number of osds for lvm scenario] *********************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.050)       0:07:07.502 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : look up for ceph-volume rejected devices] ******************
Saturday 05 June 2021  01:04:35 +0000 (0:00:00.046)       0:07:07.549 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : set_fact rejected_devices] *********************************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.048)       0:07:07.597 ********* 

TASK [ceph-config : set_fact _devices] *****************************************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.044)       0:07:07.642 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm batch --report' to see how many osds are to be created] ***
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.049)       0:07:07.691 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (legacy report)] ***
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.050)       0:07:07.742 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : set_fact num_osds from the output of 'ceph-volume lvm batch --report' (new report)] ***
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.051)       0:07:07.794 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : run 'ceph-volume lvm list' to see how many osds have already been created] ***
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.050)       0:07:07.845 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : set_fact num_osds (add existing osds)] *********************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.050)       0:07:07.895 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-config : create ceph conf directory] ********************************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.050)       0:07:07.945 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
ok: [node-6]

TASK [ceph-config : generate ceph.conf configuration file] *********************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.238)       0:07:08.184 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
[DEPRECATION WARNING]: set_available_variables is being deprecated. Use 
"@available_variables.setter" instead.. This feature will be removed in version
 2.13. Deprecation warnings can be disabled by setting 
deprecation_warnings=False in ansible.cfg.
ok: [node-6]

TASK [ceph-rgw : create rgw keyrings] ******************************************
Saturday 05 June 2021  01:04:36 +0000 (0:00:00.271)       0:07:08.455 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
ok: [node-6] => (item={'instance_name': 'rgw0', 'radosgw_address': '100.200.23.0/24', 'radosgw_frontend_port': 8080})

TASK [ceph-rgw : include_tasks multisite] **************************************
Saturday 05 June 2021  01:04:37 +0000 (0:00:00.189)       0:07:08.645 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-handler : set_fact multisite_called_from_handler_role] **************
Saturday 05 June 2021  01:04:37 +0000 (0:00:00.051)       0:07:08.696 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
ok: [node-6]
skipping: [node-1]

TASK [ceph-crash : create client.crash keyring] ********************************
Saturday 05 June 2021  01:04:37 +0000 (0:00:00.053)       0:07:08.749 ********* 
changed: [node-2 -> node-2]

TASK [ceph-facts : check if podman binary is present] **************************
Saturday 05 June 2021  01:04:41 +0000 (0:00:04.731)       0:07:13.480 ********* 
ok: [node-2 -> node-2]
ok: [node-3 -> node-2]
ok: [node-4 -> node-2]
ok: [node-6 -> node-2]
ok: [node-1 -> node-2]

TASK [ceph-facts : set_fact container_binary] **********************************
Saturday 05 June 2021  01:04:42 +0000 (0:00:00.243)       0:07:13.724 ********* 
ok: [node-2 -> node-2]
ok: [node-3 -> node-2]
ok: [node-4 -> node-2]
ok: [node-6 -> node-2]
ok: [node-1 -> node-2]

TASK [ceph-crash : set_fact container_exec_cmd] ********************************
Saturday 05 June 2021  01:04:42 +0000 (0:00:00.079)       0:07:13.803 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-crash : get keys from monitors] *************************************
Saturday 05 June 2021  01:04:42 +0000 (0:00:00.074)       0:07:13.877 ********* 
ok: [node-2 -> node-2]

TASK [ceph-crash : copy ceph key(s) if needed] *********************************
Saturday 05 June 2021  01:04:43 +0000 (0:00:00.790)       0:07:14.668 ********* 
changed: [node-3]
ok: [node-2]
changed: [node-4]
changed: [node-6]
changed: [node-1]

TASK [ceph-crash : create /var/lib/ceph/crash/posted] **************************
Saturday 05 June 2021  01:04:43 +0000 (0:00:00.360)       0:07:15.028 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-1]
skipping: [node-6]

TASK [ceph-crash : include_tasks systemd.yml] **********************************
Saturday 05 June 2021  01:04:43 +0000 (0:00:00.045)       0:07:15.074 ********* 
skipping: [node-2]
skipping: [node-3]
skipping: [node-4]
skipping: [node-6]
skipping: [node-1]

TASK [ceph-crash : start the ceph-crash service] *******************************
Saturday 05 June 2021  01:04:43 +0000 (0:00:00.045)       0:07:15.119 ********* 
ok: [node-1]
ok: [node-3]
ok: [node-4]
ok: [node-2]
ok: [node-6]

PLAY [mons] ********************************************************************

TASK [get ceph status from the first monitor] **********************************
Saturday 05 June 2021  01:04:44 +0000 (0:00:00.496)       0:07:15.616 ********* 
ok: [node-2 -> node-2]

TASK [show ceph status for cluster ceph] ***************************************
Saturday 05 June 2021  01:04:44 +0000 (0:00:00.425)       0:07:16.041 ********* 
ok: [node-2 -> node-2] => 
  msg:
  - '  cluster:'
  - '    id:     83b8c8b9-24e1-4842-a961-e82a6cdd24a3'
  - '    health: HEALTH_WARN'
  - '            mon is allowing insecure global_id reclaim'
  - '            Degraded data redundancy: 52/156 objects degraded (33.333%), 35 pgs degraded, 129 pgs undersized'
  - ' '
  - '  services:'
  - '    mon: 1 daemons, quorum ceph-research-3 (age 39m)'
  - '    mgr: ceph-research-2(active, since 4s)'
  - '    osd: 4 osds: 4 up (since 37m), 4 in (since 37m)'
  - ' '
  - '  data:'
  - '    pools:   5 pools, 129 pgs'
  - '    objects: 52 objects, 5.1 KiB'
  - '    usage:   4.0 GiB used, 76 GiB / 80 GiB avail'
  - '    pgs:     52/156 objects degraded (33.333%)'
  - '             94 active+undersized'
  - '             35 active+undersized+degraded'
  - ' '
  - '  io:'
  - '    client:   767 B/s rd, 0 op/s rd, 0 op/s wr'
  - ' '

PLAY RECAP *********************************************************************
node-1                     : ok=173  changed=10   unreachable=0    failed=0    skipped=390  rescued=0    ignored=0   
node-2                     : ok=142  changed=5    unreachable=0    failed=0    skipped=336  rescued=0    ignored=0   
node-3                     : ok=146  changed=9    unreachable=0    failed=0    skipped=312  rescued=0    ignored=0   
node-4                     : ok=136  changed=8    unreachable=0    failed=0    skipped=304  rescued=0    ignored=0   
node-5                     : ok=109  changed=7    unreachable=0    failed=0    skipped=212  rescued=0    ignored=0   
node-6                     : ok=129  changed=7    unreachable=0    failed=0    skipped=304  rescued=0    ignored=0   


INSTALLER STATUS ***************************************************************
Install Ceph Monitor           : Complete (0:00:09)
Install Ceph Manager           : Complete (0:00:10)
Install Ceph OSD               : Complete (0:01:02)
Install Ceph RGW               : Complete (0:00:08)
Install Ceph Dashboard         : Complete (0:01:48)
Install Ceph Grafana           : Complete (0:01:04)
Install Ceph Node Exporter     : Complete (0:00:28)

Saturday 05 June 2021  01:04:44 +0000 (0:00:00.023)       0:07:16.065 ********* 
=============================================================================== 
ceph-dashboard : create radosgw system user ---------------------------- 59.84s
check for python ------------------------------------------------------- 47.42s
ceph-grafana : wait for grafana to start ------------------------------- 27.19s
ceph-infra : enable chronyd -------------------------------------------- 16.41s
ceph-infra : install chrony -------------------------------------------- 15.74s
ceph-grafana : make sure grafana is down ------------------------------- 15.60s
ceph-dashboard : create dashboard admin user --------------------------- 12.52s
ceph-osd : get keys from monitors -------------------------------------- 11.84s
ceph-container-engine : allow apt to use a repository over https (debian) -- 11.17s
ceph-osd : use ceph-volume lvm batch to create bluestore osds ---------- 11.04s
ceph-infra : update cache for Debian based OSs ------------------------- 10.27s
ceph-config : look up for ceph-volume rejected devices ------------------ 6.68s
ceph-grafana : download ceph grafana dashboards ------------------------- 6.52s
ceph-osd : copy ceph key(s) if needed ----------------------------------- 6.33s
ceph-common : install dependencies for apt modules ---------------------- 5.83s
gather and delegate facts ----------------------------------------------- 5.75s
ceph-osd : set noup flag ------------------------------------------------ 5.50s
ceph-common : configure debian ceph community repository stable key ----- 5.27s
ceph-container-engine : install container packages ---------------------- 5.04s
ceph-crash : create client.crash keyring -------------------------------- 4.73s
